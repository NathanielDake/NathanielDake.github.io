{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduction to Probability Theory\n",
    "This is a post that I have been excited to get to for over a year now. Probability theory plays an incredibly interesting and unique role in the studying of machine learning and articiail intelligence techniques. It gives us a wonderful way of dealing with **uncertainty**, and shows up in everything from **Hidden Markov Models**, **Bayesian Networks**, **Causal Path Analysis**, **Bayesian A/B** testing, and many other areas.\n",
    "\n",
    "There are several things that make probability so interesting, and I am going to try and cover all of them in this post and several others. The main points are as follows:\n",
    "* Probability is rather intertwined with statistics; we will dissect the differences and also how they fit together.\n",
    "* Many paradox's arise from probability, which makes it rather unintuitive to understand. **Simpson's Paradox** and **the Monty Hall** problem are two hallmark probability paradox problems. We will go through each in detail to discuss why they are paradoxical, and how to remedy it.\n",
    "* There are many different ways to visualize and conceptualize probability.\n",
    "* **Discrete** vs. **Continuous** probability distributions cause certain visualizations to break down, causing a gap in understanding.\n",
    "\n",
    "This post will not cover all of the above, but it will certainly help us build a base from which we can climb to higher levels of understanding. To begin, I want to start from a historical perspective, digging into how probability first came to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Historical Background and Definitions\n",
    "At is core, probability theory was defined incredibly well by **Pierre-Simon Laplace** in 1814:\n",
    "\n",
    "> Probability theory is nothing but common sense reduced to calculation. ... [Probability] is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible ... when nothing leads us to expect that any one of these cases should occur more than any other.\n",
    "\n",
    "This simple summary should always be kept in mind when working with probability. The following terms must also be defined:\n",
    "\n",
    "* **Trial**: A single occurrence with an outcome that is uncertain until we observe it. \n",
    "    * For example, rolling a single die.\n",
    "* **Outcome**: A possible result of a trial; one particular state of the world. What Laplace calls a case. \n",
    "    * For example: 4.\n",
    "* **Sample Space**: The set of all possible outcomes for the trial. \n",
    "    * For example, {1, 2, 3, 4, 5, 6}.\n",
    "* **Event**: A subset of outcomes that together have some property we are interested in. \n",
    "    * For example, the event \"even die roll\" is the set of outcomes {2, 4, 6}.\n",
    "* **Probability**: As Laplace said, the probability of an event with respect to a sample space is the \"number of favorable cases\" (outcomes from the sample space that are in the event) divided by the \"number of all the cases\" in the sample space (assuming \"nothing leads us to expect that any one of these cases should occur more than any other\"). Since this is a proper fraction, probability will always be a number between 0 (representing an impossible event) and 1 (representing a certain event). \n",
    "    * For example, the probability of an even die roll is 3/6 = 1/2.\n",
    "   \n",
    "There is one more term that I would like to discuss before moving onto the general rules of probability; that term is **probabilistic**. The term probabilistic is thrown around frequently without many people having a sound definition for what it really entails. Probabilistic can be defined as:\n",
    "\n",
    "> **Probabilistic:** Subject to or involving chance variation\n",
    "\n",
    "Another way of looking at it is that it deals with **uncertainty**. Now, uncertainty can come about in the real world in a variety of ways (no, I am not going to talk about rounds of cards):\n",
    "1. We have a partial knowledge of the state of the world.\n",
    "2. Noisy observations.\n",
    "3. *Phenomena* not covered by our model.\n",
    "4. Inherent **Stochasticity** \n",
    "\n",
    "The entire goal of probability theory is to allow us to allow us to deal with uncertainty in ways that are principled and proven. Now, these definitions must be understood and internalized before moving on. One of the troubles with probability is the new vocabularly that it introduces, so be sure to look back on these definitions if anything is unclear as we move forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Probability Rules\n",
    "Now, in general there are three \n",
    "\n",
    "### 3.2.1 Maringal Probabilities\n",
    "\n",
    "\n",
    "### 3.2.3 Joint Distribution \n",
    "\n",
    "\n",
    "### 3.2.4 Conditional Probabilities\n",
    "\n",
    "\n",
    "### 3.2.2 Marginalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Discrete vs. Continuous Distributions\n",
    "\n",
    "\n",
    "### 3.3.1 Discrete Distributions\n",
    "\n",
    "\n",
    "### 3.3.2 Continuous Distributions\n",
    "\n",
    "\n",
    "### Probability Density Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
