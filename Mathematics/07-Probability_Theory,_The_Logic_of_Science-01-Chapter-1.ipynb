{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chapter 1 - Plausible Reasoning\n",
    "As we tread further into the twenty first century, almost everyone is expected to memorize the mantra \"we must make data driven decisions\" (well, at least most people in the technology space, and certainly data scientists). However, I want us to pause for a moment and think about what that really means?\n",
    "\n",
    "In an idealized, rigid, platonic world this may simply mean to cast our first intuitions aside, instead using metrics related to the problem at hand, past decision's outcomes, and so on. Now, in a simplistic system this is a satisfactory approach. Consider the following (overly simple) scenario; you want to figure out the fastest way to drive from your house to the grocery store at rush hour (note how _constrained_ this situation already is). You intuitively feel as though route $A$ will be faster, so in general you take that route. You know that on average it takes about 9 minutes. Then, one day your roommate decides to join you and recommends route $B$, stating that they always drive that way and that on average it takes 6 minutes. You give route $B$ a shot an sure enough it takes 6 and half minutes. \n",
    "\n",
    "This is an example of a very simplistic scenario that is conducive to basic data driven decision making. You have (essentially) all the data/variables you need in order to represent the scenario at hand. In other words, the decision is based on a univariate function:\n",
    "\n",
    "$$\\text{Time spent driving to grocery story} = f(\\text{route})$$\n",
    "\n",
    "We have data regarding driving time of both routes, and in this toy example there is really nothing else we need to consider in order to make an optimal decision that reduces driving time. \n",
    "\n",
    "It should be no surprise that this is _not_ how things work in the real world. The real world is messy, contains an abundance of variables, and these variables manifest into **uncertainty**. The question that I have become obsessed with is as follows:\n",
    "\n",
    "> How can we reason optimally in complex and uncertain situations? \n",
    "\n",
    "For instance, let's now say that your company sells widgets. You, as a person in marketing, are in charge of coming up with sales offerings around the holiday season. Your initial intuition is that if you give out 20 dollar coupons to anyone who makes a purchase, you will get more purchases. However, there is a competing hypothesis from your colleague that suggests offering a discount to customers who make over 1000 dollar in purchases would actually be more effective at generating revenue. You have some historical time series data, but _neither_ have specifically been conditioned upon the exact hypotheses you are both proposing (i.e., you have no data that was collected during a 20 dollar coupon period, or during a 1000 dollar purchase discount period). The data that you have is necessarily incomplete, and even if it wasn't we still must confront the following logical problem:\n",
    "\n",
    "> How do we use data (frequencies of events) to estimate plausibilities of beliefs?\n",
    "\n",
    "In other words, how can we use the data present to estimate how plausible on hypothesis is compared to the other? This question is the central focus of _Probability Theory: The Logic of Science_, by E.T. Jaynes. Often viewed as the first text to make probability theory a \"hard\" branch of mathematics (compared to a group of ad hoc methods), it is an incredibly ambitious and thought provoking book that should be on any data scientist's or statistician's bookshelf. With that said, at times it is rather dense, and I wanted to take the time to create a set of articles that serve as chapter summaries. Note, these do not mean to replace the original text; rather, they can be read in tandem to clear up any sources of confusion and ensure clear understanding. \n",
    "\n",
    "With that said, let's begin digging into the book, starting with the preface and chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Preface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
