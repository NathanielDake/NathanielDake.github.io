{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chapter 1 - Plausible Reasoning\n",
    "As we tread further into the twenty first century, almost everyone is expected to memorize the mantra \"we must make data driven decisions\" (well, at least most people in the technology space, and certainly data scientists). However, I want us to pause for a moment and think about what that really means?\n",
    "\n",
    "In an idealized, rigid, platonic world this may simply mean to cast our first intuitions aside, instead using metrics related to the problem at hand, past decision's outcomes, and so on. Now, in a simplistic system this is a satisfactory approach. Consider the following (overly simple) scenario; you want to figure out the fastest way to drive from your house to the grocery store at rush hour (note how _constrained_ this situation already is). You intuitively feel as though route $A$ will be faster, so in general you take that route. You know that on average it takes about 9 minutes. Then, one day your roommate decides to join you and recommends route $B$, stating that they always drive that way and that on average it takes 6 minutes. You give route $B$ a shot an sure enough it takes 6 and half minutes. \n",
    "\n",
    "This is an example of a very simplistic scenario that is conducive to basic data driven decision making. You have (essentially) all the data/variables you need in order to represent the scenario at hand. In other words, the decision is based on a univariate function:\n",
    "\n",
    "$$\\text{Time spent driving to grocery story} = f(\\text{route})$$\n",
    "\n",
    "We have data regarding driving time of both routes, and in this toy example there is really nothing else we need to consider in order to make an optimal decision that reduces driving time. \n",
    "\n",
    "It should be no surprise that this is _not_ how things work in the real world. The real world is messy, contains an abundance of variables, and these variables manifest into **uncertainty**. The question that I have become obsessed with is as follows:\n",
    "\n",
    "> How can we reason optimally in complex and uncertain situations? \n",
    "\n",
    "For instance, let's now say that your company sells widgets. You, as a person in marketing, are in charge of coming up with sales offerings around the holiday season. Your initial intuition is that if you give out 20 dollar coupons to anyone who makes a purchase, you will get more purchases. However, there is a competing hypothesis from your colleague that suggests offering a discount to customers who make over 1000 dollar in purchases would actually be more effective at generating revenue. You have some historical time series data, but _neither_ have specifically been conditioned upon the exact hypotheses you are both proposing (i.e., you have no data that was collected during a 20 dollar coupon period, or during a 1000 dollar purchase discount period). The data that you have is necessarily incomplete, and even if it wasn't we still must confront the following logical problem:\n",
    "\n",
    "> How do we use data (frequencies of events) to estimate plausibilities of beliefs?\n",
    "\n",
    "In other words, how can we use the data present to estimate how plausible on hypothesis is compared to the other? This question is the central focus of _Probability Theory: The Logic of Science_, by E.T. Jaynes. Often viewed as the first text to make probability theory a \"hard\" branch of mathematics (compared to a group of ad hoc methods), it is an incredibly ambitious and thought provoking book that should be on any data scientist's or statistician's bookshelf. With that said, at times it is rather dense, and I wanted to take the time to create a set of articles that serve as chapter summaries. Note, these do not mean to replace the original text; rather, they can be read in tandem to clear up any sources of confusion and ensure clear understanding. \n",
    "\n",
    "With that said, let's begin digging into the book, starting with the preface and chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.1 Preface\n",
    "Jaynes starts the book by stating who the intended audience is; while this is generally not very informative, here it actually has a good deal of merit! He states that the material is meant to help those who are in fields such as physics, chemistry, biology, geology, medicine, economics, sociology, engineering, operations research, etc.; any field where **inference** is needed. He adds a footnote stating that by inference he means:\n",
    "\n",
    "> **Inference:** **Deductive** reasoning whenever enough information is at hand to permit it; **inductive** or _plausible_ reasoning when the necessary information is not available (as it almost never is in real problems). If a problem can be solved with deductive reasoning, probability theory is not needed for it. Thus, the topic of this book is **the optimal processing of incomplete information**.\n",
    "\n",
    "As a data scientist this type of footnote should send tingles down your spine. In nearly every situation you will encounter you are often treading the line of making inferences based off of a combination of deductive and inductive reasoning; who wouldn't want to be making those inferences in an optimal way? \n",
    "\n",
    "Now, for those unfamiliar with the concepts of deduction and induction I recommend checking out <a href=\"../Machine_Learning/08-Bayesian_Machine_Learning-01-Bayesian-Inference.html#2.2.1-Induction-vs-Deduction\">this section of my article</a> on Bayesian Inference. But, for a quick recap we can think of **deduction** as forming some hypothesis about the state of the world or it's workings, gathering data about that state, and then seeing if our data confirms or denies our hypothesis. It can be visualized below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1kRqKhLAi4rDCKpwiSy70NvD0RdDE5a39\">\n",
    "\n",
    "That is deduction. We perform deduction every day with relative ease. On the other hand we have **induction**, which works in the opposite direction:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1P0ojSU-S8g_TjI9ZFc2csZj6oeIYZQ1u\">\n",
    "\n",
    "Here, we gather data about the world around us, and after picking up on a pattern we induce a hypothesis, and then work towards concluding it's validity. Now, it should be noted that this idea of reasoning from evidence to hypothesis can be thought of as a parallel of reasoning from effect to cause. \n",
    "\n",
    "As a small example (that I came across when reading _The Book of Why_ by Judea Pearl, consider Sherlock Holmes. Imagine that Sherlock Holmes was called to investigate a broken window. Sure he could go about it deductively and, while driving to the location of the window (i.e. before gathering any data), generate the hypothesis that the it was a group of children who broke the window. This is what humans do with ease, daily. However, what truly made Sherlock Holmes so powerful was his ability to perform induction. In this case, he arrives at the scene of the broken window, sees glass scattered about, and surely a host of other interesting things; he takes all of this _data_ and then generates a hypothesis about how the window was broken. \n",
    "\n",
    "If that example leads you to feel slightly intimidated by the process of induction, do not worry! Without digging into it too deeply, there is actually an _asymmetry_ going on here that can lead you into _chaos theory_. For this I will borrow another example, this time from Nassim Taleb. Imagine that you want to understand what happens when you leave a block of ice out at room temperature. You hypothesize that it will melt (form your hypothesis). You then take 10 blocks of ice, leave them all out at room temperature, and see that they do indeed all melt (collect data). You just performed a very straight forward deduction, making use of what is referred to as the **forward process**. \n",
    "\n",
    "Consider the inverse case now. Imagine that you walk into your kitchen and see that there is a puddle of water on the ground. You try and gather data, seeing if there is a leak in a pipe anywhere, if a cup spilled, etc. There is an incredibly long list of ways that this water could have gotten there. Trying to determine (from what is most likely incomplete data) that the water is actually there due to a block of ice that melted, is incredibly challenging. This is known as the **backward process**. If you are interested in this type of problem I recommend looking at the [Butterfly Effect](https://en.wikipedia.org/wiki/Butterfly_effect); for now I will leave it here to prevent going down a rabbit hole. \n",
    "\n",
    "\n",
    "### 1.1 The Theme of the Book\n",
    "To give us a north star to focus on, I want to take a moment to highlight what is essentially the theme of the book:\n",
    "\n",
    "> **Probability theory as extended logic**.\n",
    "\n",
    "We will dig into this much further in subsequent posts, but what this book does is create a framework in which the rules of probability theory can be viewed as uniquely valid principles of _logic_ in general, leaving out reference to _chance_ or _random variables_. This allows for the imaginary distinction between probability theory and statistical inference to disappear, allowing logical unity as well as greater technical power. \n",
    "\n",
    "This theme amounts to recognition that the mathematical rules of probability theory are not merely rules for calculating frequencies of \"random variables\"; they are also the unique and consistent rules for conducting inference (i.e. plausible reasoning) of any kind. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
