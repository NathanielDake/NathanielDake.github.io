{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Probabilistic Graphical Models\n",
    "PGM's are a framework to deal with certain types of problems and applications. We can begin by breaking down the name and determining what it actually means.\n",
    "\n",
    "## 1.1 Model\n",
    "A model is a **declarative** representation of our understanding of the world. It is a representation within the computer that captures our understanding of what these variables are and how they interact with eachother. The fact that it is *declarative* means that it stands on its own. This means that we can look into it and make sense of it, aside from any algorithm that we might chose to apply on it. \n",
    "\n",
    "Why is this important? Because it means that this same model can be used in the context of one algorithm that may answer a specific kind of question, or other algorithms that may answer different kinds of questions, or the same question in more efficient ways. \n",
    "\n",
    "The other advantage of having a stand alone model is that we can separate out the construction of the model from the algorithms that are used to reason over it. So we can construct methodologies that ellicit these models from a human expert, or ones that learn it from historical data using statistical machine learning techniques, or a combination of the two. \n",
    "\n",
    "## 1.2 Probabilistic\n",
    "The word probabilistic is in the name because these models are designed to help us deal with a large amount of uncertainty. Uncertainty can come in many different forms and for many different reasons:\n",
    "> 1. We have a partial knowledge of the state of the world.\n",
    "2. Noisy observations.\n",
    "3. *Phenomena* not covered by our model.\n",
    "4. Inherent Stochasticity \n",
    "\n",
    "**Probability theory** is a theory that allows us to deal with uncertainty in ways that are principled and bring to bear important and valuable tools. Probabilistic models again provide a *declarative* representation with clear semantics. The word declarative is a again meaning that it is stand alone, where you could look at a probability distribution and it has clear semantics that represent our uncertainty about different states that the world might be in. \n",
    "\n",
    "It also provides us with a toolbox consisting of **powerful reasoning patterns**, that include, for example, conditioning on new forms of evidence or decision making under uncertainty. \n",
    "\n",
    "And because of the intricate connection between probability theory and **statistics**, you can bring to bear a range of powerful learning methodologies from **statistical learning** to allow us to learn these models effectively from historical data. Avoiding the need for a human to specify every single aspect of the model by hand. \n",
    "\n",
    "## 1.3 Graphical \n",
    "Finally, the word **graphical**. The word graphical is here from the perspective of computer science, because probabilistic graphical models are a synthesis between ideas from probability theory and statistics and ideas from computer science. And the idea here is to use the connections computer science, specifically that of graphs, to allow us to represent systems that are very complicated that involved large numbers of variables.   \n",
    "\n",
    "So in order to capture **probability distributions** over spaces involving such a large number of factors, we need to have probability distributions over what are called **random variables**. And so the focus of this class and what we'll do for most of it is to think about the world as represented by a set of random variables:\n",
    "\n",
    "$$X_1,...,X_n$$\n",
    "\n",
    "each of which captures some facet of the world. So, one symptom that may be present or absent, or a test result that might have a continuous set of possible values or a pixel that might have one of several labels. So each of these is a random variable and our goal is *to capture our uncertainty about the possible states of the world in terms of their probability distribution or what's called a joint distribution over the possible assignments to the set of random variables*.\n",
    "\n",
    "$$P\\big(X_1,...,X_n\\big)$$\n",
    "\n",
    "Now, the important thing to realize when looking at this, is that even in the simplest case where each of these is a random variable is binary valued, and there are $n$ of them, then this is a distribution with:\n",
    "\n",
    "$$2^n \\; possible \\; states \\; of \\; the \\; world$$\n",
    "\n",
    "And so we have to deal with objects that are intrinsically, **exponentially large**. And our only way to do that is by exploiting **data structures** that encode, that use ideas from computer science in this case to exploit the structure and distribution and represent and manipulate it in an effective way. \n",
    "\n",
    "### 1.3.1 What are Graphical Models?\n",
    "So what are graphical models? Let's look at a couple of very simple examples, so here's a toy **Bayesian network**: \n",
    "\n",
    "<img src=\"images/simple-bn.png\">\n",
    "\n",
    "A Bayesian network is one of the two main classes of probabilistic graphical models, and it uses a **directed graph** as the intrinsic representation. In this case, remember we had a set of random variables $X_1$ up to $X_n$. The random variables are represented by *nodes* in the graph. So, to take a look at this very simple example, we have a situation where we have a student who takes a course and gets a grade in the course, and so that's one of our random variables. We have other random variables that are also related to that. For example, the intelligence of the student's in the course, the difficulty of the course. And others that might also be of interest, for example the quality of the recommendation letter that the student gets in the course which is dependent on things, perhaps the students' grade, and these score that the students might receive on the SAT. \n",
    "\n",
    "So, this is a representation of a probability distribution, in this case over these five random variables. And the edges in this graph represent the **probabilistic connections** between those random variables in a way that is very formal as we'll define later on. \n",
    "\n",
    "The other main class of probabilistic graphical model is what's called the **Markov network** and that uses an undirected graph. \n",
    "\n",
    "<img src=\"images/simple-markov.png\">\n",
    "\n",
    "And in this case, we have an undirected graph over 4 random variables A, B, C, D and will give an example of this type of network maybe a little bit later on.\n",
    "\n",
    "### 1.3.2 Graphical Representation Summary\n",
    "So to summarize, the graphical representation gives us:\n",
    "> 1. An *intuitive* and *compact* data structure for capturing these high dimensional probability distributions. \n",
    "2. It provides us at the same time, a suite of methods for *efficient reasoning*, using general purpose algorithms that exploit the graphical structure. \n",
    "3. And because of the way in which the graph structure encodes the parameterization of the probability distribution, we can represent these high-dimensional probability distribution efficiently using a **very small number of parameters**. Which allows us both feasible elicitation by hand from an expert as well as automatically learning from data. And in both cases a reduction in the number of parameters is very valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 2. Factors \n",
    "So a **factor** really is a **function**, or a table.  It takes a bunch of arguments-in this case, a set of random variables:\n",
    "\n",
    "$$\\phi\\big(X_1,...,X_k\\big)$$\n",
    "\n",
    "And just like any function it gives us a value for every assignment  to those random variables. \n",
    "\n",
    "$$\\phi : Val\\big(X_1,...,X_k\\big) \\rightarrow R$$\n",
    "\n",
    "So it takes all possible assignments in  the cross products space of $X_1$ up to $X_K$.  That is all possible combinations of assignments and in this case it gives me  a real value for each such combination. And the set of variables $X_1$ up to $X_K$ is  called the **scope** of the factor. That is it's the set of arguments, that a  factor takes. \n",
    "\n",
    "## 2.1 Examples of Factors\n",
    "Let's look at some examples of factors. A **joint distribution** is a factor.  For every combination for example here of the variables I, D, and G, it gives me a  number. As it happens this number is a  probability. And it happens that it sums to one but  that doesn't matter. What's important is that for every value of I, D, and G, a combination of values, we get a number. That's why it's a factor. \n",
    "\n",
    "Note, the scope of a factor only includes variables that are specifically utilized and needed in determining the outcome. \n",
    "\n",
    "We can also deal with **factor products**, **factor marginalization**, and **factor reduction**. \n",
    "\n",
    "## 2.2 Why Factors? \n",
    "It turns out that factors are the fundamental building block in defining  these distributions and high dimensional spaces.  That is, the way in which we're going to define an exponentially large probability  distribution over $N$ random variables is by taking a bunch of little pieces and  putting them together by multiplying factors in order to define these high  dimensional probability distributions. It turns out also that the same set of  basic operations that we use to define the probability distributions in these  high dimensional spaces are also what we use for manipulating them in order to  give us a set of basic inference algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
