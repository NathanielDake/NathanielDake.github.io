{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Spinning Intro\n",
    "* Changing certain words of an article so it does not match the original, so a search engine can't mark it as duplicate content\n",
    "* How is this done:\n",
    "    * take an article and slightly modify it, different terms, same meaning\n",
    "    * \"Udemy is a **platform** or **marketplace** for online **learning**\"\n",
    "    * \"Udemy is a **podium** or **forum** for online **research**\"\n",
    "* Clearly context is very important!\n",
    "* the idea is that you need to use the surrounding words to influence the replacement of the current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Trigram Model and Markov Models\n",
    "* how can we model the probability of a word given the surrounding words? \n",
    "* Lets start by taking an entire document and labeling all of the words: **w(1), w(2),...,w(n)**\n",
    "* we can then model the probability of **w(i)** using the surrounding words:\n",
    "    * those that came before w(i): w(1)...w(i-1)\n",
    "    * and those that came after w(i): w(i+1)...w(n)\n",
    "* Probabilistically this would look like:\n",
    "### $$P\\Big(w(i)\\;\\Big|\\;w(1)...w(i-1), w(i+1)...w(n)\\Big)$$\n",
    "* Why wouldn't this work?\n",
    "* well, using this approach we are considering every word in the document, which means that only that model itself would match it exactly\n",
    "* We need to do something similar to what we do with markov models and only consider the closest words\n",
    "\n",
    "## Trigram\n",
    "* we are going to use something called a trigram to accomplish this! \n",
    "* we are going to create triples, where we store combinations of 3 consecutive words\n",
    "* A few pieces of vocav worth knowing:\n",
    "    * **corpus**: collection of text\n",
    "    * **tokens**: words and punctuation that make up the corpus\n",
    "    * **Type**: distinct token\n",
    "    * **vocabulary**: set of all types\n",
    "    * **unigram**: 1 token sequence\n",
    "    * **bigram**: 2 token sequence\n",
    "    * **trigram**: 3 token sequence\n",
    "    * **n-gram**: n token sequence \n",
    "* in the case of a trigram we are going to use the previous words and next word to predict the current word:\n",
    "### $$P\\Big(w(i)\\;\\Big|\\;w(i-1), w(i+1)\\Big)$$\n",
    "* How will we implement this? \n",
    "* We are going to create a dictionary with the previous word and next word as the key, and then randomly sample the middle word **w(i)**! \n",
    "    * for example we could have the key ('I ', 'sports'), which would have an array of values, ['hate','love', 'enjoy', etc.]\n",
    "    * we would randomly sample from that array\n",
    "* this is sort of like a markov model, expect a markov model is only concerned with P(w(i)|w(i-1))\n",
    "* We won't replace every single word in the document, because that wouldn't give us anything useful\n",
    "* so we will make the decision to replace the word based on some small probability \n",
    "* Both this and latent semantic analysis are what we call unsupervised learning algorithms, because they have no labels and we just want to learn the structure of the data\n",
    "* Note: spam detector and sentiment analyzer were supervised because we had labels to match to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Markov Chains and Monte Carlo Methods\n",
    "* Great tutorial: https://deeplearning4j.org/markovchainmontecarlo\n",
    "* Markov Chain Monte Carlo (MCMC) is a mathematical method that draws samples randomly from a black-box to approximate the probability distribution of attributes over a range of objects (the height of men, the names of babies, the outcomes of events like coin tosses, the reading levels of school children, the rewards resulting from certain actions) or the futures of states.\n",
    "* MCMC methods help gauge the distribution of an outcome or statistic you’re trying to predict, by randomly sampling from a complex probabilistic space.\n",
    "* As with all statistical techniques, we sample from a distribution when we don’t know the function to succinctly describe the relation to two variables (actions and rewards). MCMC helps us approximate a black-box probability distribution.\n",
    "\n",
    "## Concrete Example\n",
    "Let’s say you’re a gambler in the saloon of a Gold Rush town and you roll a suspicious die without knowing if it is fair or loaded. You roll a six-sided die a hundred times, count the number of times you roll a four, and divide by a hundred. That gives you the probability of four in the total distribution. If it’s close to 16.7 (1/6 * 100), the die is probably fair.\n",
    "\n",
    "Monte Carlo looks at the results of rolling the die many times and tallies the results to determine the probabilities of different states. It is an inductive method, drawing from experience. The die has a state space of six, one for each side.\n",
    "\n",
    "## Systems and States\n",
    "At a more abstract level, where words mean almost anything at all, a system is a set of things connected together (you might even call it a graph, where each state is a vertex, and each transition is an edge). It’s a set of states, where each state is a condition of the system. But what are states?\n",
    "\n",
    "* Cities on a map are “states”. A road trip strings them together in transitions. The map represents the system.\n",
    "* Words in a language are states. A sentence is just a series of transitions from word to word.\n",
    "* Genes on a chromosome are states. To read them (and create amino acids) is to go through their transitions.\n",
    "* Web pages on the Internet are states. Links are the transitions.\n",
    "* Bank accounts in a financial system are states. Transactions are the transitions.\n",
    "* Emotions are states in a psychological system. Mood swings are the transitions.\n",
    "* Social media profiles are states in the network. Follows, likes, messages and friending are the transitions.\n",
    "* Rooms in a house are states. People walking through doorways are the transitions.\n",
    "\n",
    "So states are an abstraction used to describe these discrete, separable, things. A group of those states bound together by transitions is a system. And those systems have structure, in that some states are more likely to occur than others (ocean, land), or that some states are more likely to follow others.\n",
    "\n",
    "We are more like to read the sequence Paris -> France than Paris -> Texas, although both series exist, just as we are more likely to drive from Los Angeles to Las Vegas than from L.A. to Slab City, although both places are nearby.\n",
    "\n",
    "A list of all possible states is known as the “state space.” The more states you have, the larger the state space gets, and the more complex your combinatorial problem becomes.\n",
    "\n",
    "## Markov Chains\n",
    "Since states can occur one after another, it may make sense to traverse the state space, moving from one to the next. A Markov chain is a probabilistic way to traverse a system of states. It traces a series of transitions from one state to another. It’s a random walk across a graph.\n",
    "\n",
    "Each current state may have a set of possible future states that differs from any other. For example, you can’t drive straight from Atlanta to Seattle - you’ll need to hit other states in between. We are all, always, in such corridors of probabilities; from each state, we face an array of possible future states, which in turn offer an array of future states two degrees away from the start, changing with each step as the state tree unfolds. New possibilites open up, others close behind us. Since we generally don’t have enough compute to explore every possible state of a game tree for complex games like go, one trick that organizations like DeepMind use is Monte Carlo Tree Search to narrow the beam of possibilities to only those states that promise the most likely reward.\n",
    "\n",
    "Traversing a Markov chain, you’re not sampling with a God’s-eye view any more like a conquering alien. You are in the middle of things, groping your way toward one of several possible future states step by probabilistic step, through a Markov Chain.\n",
    "\n",
    "While our journeys across a state space may seem unique, like road trips across America, an infinite number of road trips would slowly give us a picture of the country as a whole, and the network that links its cities together. This is known as an equilibrium distribution. That is, given infinite random walks through a state space, you can come to know how much total time would be spent in any given state. If this condition holds, you can use Monte Carlo methods to initiate randoms “draws”, or walks through the state space, in order to sample it.\n",
    "\n",
    "## Markov Time\n",
    "Markov chains have a particular property: oblivion, or forgetting.\n",
    "\n",
    "That is, they have no long-term memory. They know nothing beyond the present, which means that the only factor determining the transition to a future state is a Markov chain’s current state. You could say the “m” in Markov stands for “memoryless”: A woman with amnesia pacing through the rooms of a house without knowing why.\n",
    "\n",
    "Or you might say that Markov Chains assume the entirety of the past is encoded in the present, so we don’t need to know anything more than where we are to infer where we will be next. Check out a visual demo here: http://setosa.io/ev/markov-chains/\n",
    "\n",
    "So imagine the current state as the input data, and the distribution of attributes related to those states (perhaps that attribute is reward, or perhaps it is simply the most likely future states), as the output. From each state in the system, by sampling you can determine the probability of what will happen next, doing so recursively at each step of the walk through the system’s states.\n",
    "\n",
    "## Probability as a Spaced\n",
    "When they call it a state space, they’re not joking. You can picture it, just like you can picture land and water, each one of them a probability as much as they are a physical thing. Unfold a six-sided die and you have a flattened state space in six equal pieces, shapes on a plane. Line up the letters by their frequency for 11 different languages, and you get 11 different state spaces.\n",
    "\n",
    "Another tutorial: https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Article Spinner Code\n",
    "A great resource for this article spinner is found here: http://norvig.com/ngrams/ch14.pdf\n",
    "Lets now write the code for our article spinner. Start with our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random         # needed for probabilities and sampling\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_reviews = BeautifulSoup(open('data/electronics/positive.review').read(), \"lxml\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all of the Trigrams\n",
    "Recall, for each trigram the key is the previous and next word, and the value is going to be the possible middle words (so an array, may only contain a single value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = {}\n",
    "for review in positive_reviews:                          # loop through every review \n",
    "    s = review.text.lower()                              # don't want two versions of same word\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    for i in range(len(tokens) - 2):\n",
    "        k = (tokens[i], tokens[i+2])                     # the key is a tuple, tuples are immutable and can be key\n",
    "        if k not in trigrams: \n",
    "            trigrams[k] = []\n",
    "        trigrams[k].append(tokens[i+1])                  # now we have all of the possible middle words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform into a probability vector\n",
    "Now that we have all of the possible middle words, we need to transform this into a probability vector. We need to convert these trigrams into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_probabilities = {}                 # dictionary to hold trigram probabilities, the loop through trigrams\n",
    "for k, words in trigrams.items():           # k will be the key, and words is a list of words for that key\n",
    "    if len(set(words)) > 1:                 # set gets rid of duplicates, then we need to make sure > 1 word\n",
    "        d = {}                              # another dictionary d, keyed by the middle word\n",
    "        n = 0\n",
    "        for w in words:                     # loop through each word, d count how many times the middle word occur\n",
    "            if w not in d:                  \n",
    "                d[w] = 0\n",
    "            d[w] += 1 \n",
    "            n += 1                          # n is going to track the total number of words\n",
    "        for w, c in d.items():\n",
    "            d[w] = float(c)/n               # # of times each word occurs, divided by total number of words\n",
    "        trigrams_probabilities[k] = d       # setting trigram prob for specific key to be that of d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Randomly Sample Trigram Probabilities\n",
    "Now we need to create a function that will randomly sample from these trigram probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(d):                 # function, takes dictionary (key is word, value is probability of that word)\n",
    "    r = random.random()               # generate random number\n",
    "    cumulative = 0 \n",
    "    for w, p in d.items():\n",
    "        cumulative += p\n",
    "        if r < cumulative:\n",
    "            return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Function to test spinner\n",
    "It needs to randomly choose a review, then try to spin it and print both out so we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_spinner():\n",
    "    review = random.choice(positive_reviews)            # grab a random positive review\n",
    "    s = review.text.lower()\n",
    "    print('Original:', s)\n",
    "    tokens = nltk.tokenize.word_tokenize(s)             # tokenize the positive review\n",
    "    for i in range(len(tokens) - 2):                    # loop through each token\n",
    "        if random.random() < 0.2:                       # choose with a small probability to replace (20% chance)\n",
    "            k = (tokens[i], tokens[i+2])                # get the word before and after our word\n",
    "            if k in trigrams_probabilities: \n",
    "                w = random_sample(trigrams_probabilities[k])\n",
    "                tokens[i+1] = w \n",
    "    print ('Spun:')\n",
    "    print(\" \".join(tokens).replace(\" .\", \".\").replace(\" '\", \"'\").replace(\" ,\", \",\").replace(\"$ \", \"$\").replace(\" !\", \"!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "very reasonable price and a must have as a spare for those that leave affixed to windshield year-round. fast shipping. a great shopping experience\n",
      "\n",
      "Spun:\n",
      "very reasonable price was a must have as a spare for those that leave affixed to windshield year-round. fast enough. a great shopping experience\n"
     ]
    }
   ],
   "source": [
    "test_spinner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
