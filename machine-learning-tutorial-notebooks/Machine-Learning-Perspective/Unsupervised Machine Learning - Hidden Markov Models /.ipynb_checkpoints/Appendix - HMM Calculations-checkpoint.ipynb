{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Hidden Markov Model Calculations\n",
    "This appendix serves as an accompaniment to hidden markov models, discrete observations. We will go over calculations concerning:\n",
    "> 1. **Probability of a Sequence**\n",
    "2. **Forward-Backward Algorithm**\n",
    "3. **Viterbi Algorithm**\n",
    "4. **Baum-Welch Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0. General Definitions\n",
    "We will start by restating common variables that will be used throughout our calculations. \n",
    "\n",
    "### 0.1 Hidden States and Observations\n",
    "We will let the number of hidden states be $M$:\n",
    "\n",
    "#### $$\\text{Number of hidden states} = M$$\n",
    "\n",
    "And the length of our sequence of observations be $T$:\n",
    "\n",
    "#### $$\\text{Length of sequence of observations} = T$$\n",
    "\n",
    "### 0.2 Joint Distribution\n",
    "We know that the joint distribution containing both our observed symbols and hidden states is:\n",
    "\n",
    "#### $$p(x,z)$$\n",
    "\n",
    "Where both $x$ and $z$ are vectors:\n",
    "\n",
    "#### $$x = \\big[x(1), x(2), ..., x(T)\\big]$$\n",
    "\n",
    "#### $$z = \\big[z(1), z(2), ..., z(T)\\big]$$\n",
    "\n",
    "### 0.3 Marginalized Distribution\n",
    "And we want to find the distribution for the sequence of observed symbols:\n",
    "\n",
    "#### $$p\\big(x(1), x(2),...,x(T)\\big)$$\n",
    "\n",
    "This is done by _marginalizing_ out $z$. \n",
    "\n",
    "### 0.4 Initial State Distribution $\\pi$\n",
    "We have our _initial state distribution_, $\\pi$, the probability of being in a hidden state when the sequence begins:\n",
    "\n",
    "#### $$\\pi$$\n",
    "\n",
    "### 0.5 State Transition Matrix $A$\n",
    "Then there is our _state transition matrix_, $A$, which represents the probability of going from state $i$ to state $j$:\n",
    "\n",
    "#### $$A(i, j) = \\text{probability of going from state i to state j}$$\n",
    "\n",
    "### 0.6 Observation Matrix $B$\n",
    "Finally, we have our _observation matrix_, $B$, which represents the probability of observing symbol $k$ while in state $j$:\n",
    "\n",
    "#### $$B(j,k) = \\text{probability of observing symbol k while you are in state j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Probability of a Sequence\n",
    "We have gone over the equations utilized in determining the probability of a sequence, but we will now solidify that with an actual example. Here is the problem statement:\n",
    "\n",
    "> There is a magician who has two biased coins, _coin 1_ and _coin 2_ that he is flipping. We are trying to determine the probability of observing the following sequence of coin flips:<br>\n",
    "<br>\n",
    "$$p\\big(HHT \\big)$$<br>\n",
    "$$p\\big( x(1)=H, x(2)=H, x(3)=T \\big)$$<br>\n",
    "To start, we know that since the magician has two coins, the number of hidden states, $M$, is 2. We also know that we can either observe a flipped coin being heads or tails, so the number of possible observed symbols is also 2. We are told that the magician really likes coin 1, so the initial state distribution is:<br>\n",
    "<img src=\"images/initial-1.png\" width=\"150\">\n",
    "<br>\n",
    "He is also very figity, and tends to switch between coins very often, so his state transition matrix is:<br>\n",
    "<br>\n",
    "<img src=\"images/state-transition-1.png\" width=\"200\">\n",
    "<br>\n",
    "Finally, the coin 1 is biased towards heads and has a 0.7 probability of being heads, and a 0.3 probability of being tails. Coin 2 is biased towards tails, and has a 0.6 probability of being tails, and a 0.4 probability of being heads. Hence, the observation matrix looks like:<br>\n",
    "<img src=\"images/observation-1.png\" width=\"200\">\n",
    "<br>\n",
    "\n",
    "We now have all of the information needed to find the probability of the sequence $H,H,T$. Intuitively, we can think about it as follows: We must first take into account the probability that we start in specific state, and from that state we observed heads. We then must account for the probability that from that state we transition to another hidden state and observe heads. And finally, we must take into the account that from the second hidden state we transition to _another_ hidden state and observe tails. Let's look at each part separately.\n",
    "\n",
    "### 1.1 The probability of the Initial State\n",
    "We can write the probability of the initial state and observing heads as:\n",
    "\n",
    "#### $$\\pi\\big(z(1)\\big)p\\big(x(1)=1|z(1)\\big)$$\n",
    "\n",
    "### 1.2 The probability of transitioning from state 1 to state 2\n",
    "\n",
    "#### $$p\\big(z(2)|z(1)\\big) = A\\big(z(1),z(2)\\big)$$\n",
    "\n",
    "### 1.3 The probability of observing heads from state 2\n",
    "\n",
    "#### $$p\\big(x(2)=1|z(2)\\big) = B\\big(z(2),x(2)=1\\big)$$\n",
    "\n",
    "Now, if we repeated this process for transitioning from state 2 to 3, we would end up with the following equation:\n",
    "\n",
    "#### $$p\\big(x,z \\big) = \\pi\\big(z(1)\\big)p\\big(x(1)=1|z(1)\\big) * p\\big(z(2)|z(1)\\big) * p\\big(x(2)=1|z(2)\\big) * p\\big(z(3)|z(2)\\big) * p\\big(x(3)=0|z(3)\\big)$$\n",
    "\n",
    "Which we can then update by utilizing our matrices $A$ and $B$:\n",
    "\n",
    "#### $$p\\big(x,z \\big) = \\pi\\big(z(1)\\big)B\\big(z(2),x(2)=1\\big) * A\\big(z(1),z(2)\\big) * B\\big(z(2),x(2)=1\\big) * A\\big(z(2),z(3)\\big) * B\\big(z(3),x(3)=0\\big)$$\n",
    "\n",
    "Now, at this point we can see that there are multiple values that $z$ can take on, and that in order to find $p(x)$ we must marginalize out $z$, like so:\n",
    "\n",
    "#### $$\\sum_{z_1 = 1..M,..,z_3=1..M}\\pi\\big(z(1)\\big)B\\big(z(2),x(2)=1\\big) * A\\big(z(1),z(2)\\big) * B\\big(z(2),x(2)=1\\big) * A\\big(z(2),z(3)\\big) * B\\big(z(3),x(3)=0\\big)$$\n",
    "\n",
    "We can see that since $M$ is 2, and we have $T =3$ observations, there are going to be $2^3$ different operations that need to be performed in order to marginalize out $z$. That would be very messy to write out, be we will calculate it in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Probability:  0.11169000000000001\n",
      "Iterations:  8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial probability distribution and transition matrices\n",
    "pi = np.array([0.8, 0.2])\n",
    "A = np.array([[0.1, 0.9],[0.9, 0.1]])\n",
    "B = np.array([[0.7, 0.3],[0.4, 0.6]])\n",
    "x = np.array([0,0,1]) # 0 for heads, 1 for tails, based on numpy indexing\n",
    "\n",
    "# Function to calculate the probability of the observed sequence for a given z1, z2, z3\n",
    "def sequence_probability_with_z(z1, z2, z3):\n",
    "  return pi[z1]*B[z2, x[0]] * A[z1, z2]*B[z2, x[1]] * A[z2, z3]*B[z3, x[2]]\n",
    "\n",
    "# Initial marginalized sequence probability and number of iterations performed\n",
    "marginalized_sequence_prob = 0\n",
    "iterations = 0\n",
    "\n",
    "# Calculate marginalized sequence probability: p(x1, x2, x3) -> p(H, H, T)\n",
    "Z = [0,1]\n",
    "for z1 in Z:\n",
    "  for z2 in Z:\n",
    "    for z3 in Z:\n",
    "      iterations += 1 \n",
    "      marginalized_sequence_prob += sequence_probability_with_z(z1, z2, z3)\n",
    "\n",
    "print('Sequence Probability: ', marginalized_sequence_prob)\n",
    "print('Iterations: ', iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally arrive at our sequence probability:\n",
    "\n",
    "$$p(H,H,T) = 0.11$$\n",
    "\n",
    "Note that in order to achieve this we needed to perform 8 operations. Specifically, we know that the time complexity of this algorithm is $O(M^T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 2. Forward-Backward Algorithm\n",
    "At this point we are ready to move on to an algorithm that will help reduce the exponential complexity that we are dealing with above. This algorithm, _**the Forward-Backward**_ algorithm, works by defining a variable called $\\alpha$ that represents the joint probability of seeing the sequence you have observed up until now and being in a specific state at that time:\n",
    "\n",
    "$$\\alpha(t,i) = p \\big(x(1),...,x(t),z(t)=i\\big)$$\n",
    "\n",
    "We can see that $\\alpha$ is indexed by both time and $i$, the state.\n",
    "\n",
    "### 2.1.1 Step 1 $\\rightarrow$ Initial Value of $\\alpha$\n",
    "The first step of the forward backward algorithm is to calculate the initial value of $\\alpha$ at $t=1$:\n",
    "\n",
    "$$\\alpha(1, i) = p \\big(x(1), z(1)=i\\big)$$\n",
    "\n",
    "$$\\alpha(1, i) =  p\\big(z(1) = i \\big) p\\big(x(1) \\mid z(1)= i\\big)$$\n",
    "\n",
    "Where, we know that:\n",
    "\n",
    "$$\\pi_i = p\\big(z(1) = i \\big)$$\n",
    "\n",
    "And also that:\n",
    "\n",
    "$$p\\big(x(1) \\mid z(1)= i\\big) = B\\big(i, x(1)\\big)$$\n",
    "\n",
    "So, we can rewrite our equation for $\\alpha(1,i)$ as:\n",
    "\n",
    "$$\\alpha(1,i) = \\pi_iB\\big(i, x(1)\\big)$$\n",
    "\n",
    "Now, in terms of our example, we know that we have $M=2$ states. Hence, we need to find $\\alpha$ for each starting state (coin 1 and coin 2):\n",
    "\n",
    "$$\\alpha(1,1) = \\alpha(1, \\text{coin 1}) = \\pi_1B\\big(z(1)=1, x(1)\\big)$$\n",
    "\n",
    "$$\\alpha(1,2) = \\alpha(1, \\text{coin 2})= \\pi_2B\\big(z(1)=2, x(1)\\big)$$\n",
    "\n",
    "### 2.1.2 Key Point\n",
    "The first thing that I want you to take note of, is that that variable, $\\alpha$, will only be updated for _specific values_. Currently it only has _two values_; that for $t=1$ and coin 1, and $t=1$ with coin 2. From an implementation standpoint, $\\alpha$ will be a 2 dimensional matrix:\n",
    "\n",
    "```\n",
    "alpha = np.zeros((T, self.M))\n",
    "```\n",
    "\n",
    "The second key point is that the entire goal of this process is to find $p\\big(x(1)\\big)$. That may not be clear, or you may wonder why we have introduced this new variable $\\alpha$. To answer that, let's first write the equation for $p\\big(x(1)\\big)$:\n",
    "\n",
    "$$p\\big(x(1)\\big) = p\\big(z(1)=1\\big) p\\big(x(1) \\mid z(1) =1 \\big) +...+p\\big(z(1)=M\\big) p\\big(x(1) \\mid z(1) =M \\big)$$\n",
    "\n",
    "$$p\\big(x(1)\\big) = \\pi_1 B \\big(1, x(1) \\big) + \\pi_2 B \\big(2, x(1) \\big)+ ... +\\pi_M B \\big(M, x(1) \\big)$$\n",
    "\n",
    "But wait! We can see that this is our definition for $\\alpha(1,i)$! If we sum $\\alpha$ over the state $i$ when $t=1$, we end up with $p\\big(x(1)\\big)$. This leads us to the critical intuition:\n",
    "\n",
    "> $\\alpha$ is a way to successively keep track of the probability of a sequence.\n",
    "\n",
    "### 2.1.3 Step 2 $\\rightarrow$ The Induction Step\n",
    "At that point, we come to the _induction step_. We are trying to find $p\\big(x(1), x(2)\\big)$ now. We know that the induction step is an updating of $\\alpha$ defined as:\n",
    "\n",
    "$$\\alpha(t+1, j) = \\sum_{i=1}^M \\alpha(t,i) A(i,j)B(j, x(t+1))$$\n",
    "\n",
    "But inuitively we can think of it as follows (in relation to our magician example): At $t=1$ the magician in holding one of the coins-we are not sure which-and we observe him flip a heads. We then go to $t=2$, he shuffles the coins behind his back, and then flips one of the coins where we again observe a heads. What we want to know is, what is the probability of that happening?\n",
    "\n",
    "In order to determine that we must consider that the magician could transition from coin 1 or 2 at $t=1$, to coin 1 or 2 at $t=2$, where we then must determine the probability of observing $x(2)=heads$. This can be written as:\n",
    "\n",
    "$$\\pi_1A(1,1)B(1, x(2)=heads)+ \\pi_2A(2,1)B(1, x(2)=heads)+\\pi_1A(1,2)B(2, x(2)=heads)+ \\pi_2A(2,2)B(2, x(2)=heads)$$\n",
    "\n",
    "We can include $B$ for $t=1$ as well:\n",
    "\n",
    "$$\\pi_1B(1, x(1)=heads)A(1,1)B(1, x(2)=heads)+ \\pi_2B(2, x(1)=heads)A(2,1)B(1, x(2)=heads)+...$$\n",
    "\n",
    "Ahah! This is just $\\alpha$ at time $t=2$! \n",
    "\n",
    "$$\\alpha(t=2, i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  [[0.56     0.08    ]\n",
      " [0.0896   0.2048  ]\n",
      " [0.057984 0.060672]]\n",
      "Sequence Probability:  0.11865600000000001\n"
     ]
    }
   ],
   "source": [
    "M = 2\n",
    "T = 3 \n",
    "alpha = np.zeros((T, M))\n",
    "\n",
    "# Initial value step\n",
    "for i in range(M):\n",
    "  alpha[0,i] = pi[i]*B[i, x[0]]\n",
    "\n",
    "# Induction Step\n",
    "for t in range(1, T):\n",
    "  for j in range(M):\n",
    "    for i in range(M):\n",
    "      alpha[t, j] += alpha[t-1, i]*A[i,j]*B[j,x[t]]\n",
    "\n",
    "sequence_probability = alpha[-1].sum()\n",
    "print('alpha: ', alpha)\n",
    "print('Sequence Probability: ', sequence_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then vectorize the above process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  [[0.56     0.08    ]\n",
      " [0.0896   0.2048  ]\n",
      " [0.057984 0.060672]]\n",
      "Sequence Probability:  0.118656\n"
     ]
    }
   ],
   "source": [
    "alpha = np.zeros((T, M))\n",
    "alpha[0] = pi * B[:, x[0]]\n",
    "for t in range(1, T):\n",
    "  alpha[t] = alpha[t-1].dot(A) * B[:, x[t]]\n",
    "sequence_probability = alpha[-1].sum()\n",
    "print('alpha: ', alpha)\n",
    "print('Sequence Probability: ', sequence_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
