{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Dynamic Programming and Iterative Policy Evaluation\n",
    "We are now going to start looking at solutions to MDP's. As we saw in the last section, the center piece of the discussion is the **Bellman Equation**:\n",
    "\n",
    "#### <span style=\"color:#0000cc\">$$\\text{Bellman Equation} \\rightarrow V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$</span>\n",
    "\n",
    "In fact, the bellman equation can be used directly to solve for the value function. If you look carefully, you will see that this is actually a set of $S$ equations with $S$ unknowns. In fact, it is a linear equation, meaning it is not too difficult to solve. In addtion, a lot of the matrix entries will be zero, since the state transitions will most likely be sparse. \n",
    "\n",
    "However, this is _not_ the approach we will take. Instead, we will do what is called **iterative policy evaluation**. \n",
    "\n",
    "## 1.1 Iterative Policy Evaluation\n",
    "What exactly is iterative policy evaluation? Well, essentially it means that we apply the bellman equation again and again, and eventually it will just converge. We can write down the algorithm in pseudocode as follows:\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{def iterative_policy_evaluation}(\\pi)\\text{:} \\\\\n",
    "\\hspace{1cm} \\text{initialize V(s) = 0 for all s} \\in \\text{S} \\\\\n",
    "\\hspace{1cm} \\text{while True:} \\\\\n",
    "\\hspace{2cm} \\Delta = 0 \\\\\n",
    "\\hspace{2cm} \\text{for each s} \\in \\text{S:} \\\\\n",
    "\\hspace{3cm} \\text{old_v = V(s)} \\\\\n",
    "\\hspace{3cm} V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}\\\\\n",
    "\\hspace{3cm} \\Delta = \\text{max(} \\Delta \\text{, |V(s) - old_v|)} \\\\\n",
    "\\hspace{2cm} \\text{if} \\Delta \\text{< threshold: break} \\\\\n",
    "\\hspace{1cm} \\text{return V(s)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "We can see above that the input to iterative policy evaluation is a policy, $\\pi$, and the output is the value function for that particular policy. It works as follows:\n",
    "> * We start by initializing $V(s)$ to 0 for all states. \n",
    "* Then, in an infinite loop, we initialize a variable called $\\Delta$, which represents the maximum change during the current iteration. $\\Delta$ is used to determine when to quit. When it is small enough, that is when we will break out of the loop. \n",
    "* Then, for every state in the state space, we keep a copy of the old $V(s)$, and then we use bellmans equation to update V(s). \n",
    "* We set $\\Delta$ to be the maximum change for $V(s)$ in that iteration. \n",
    "* Once this converges, we return $V(s)$\n",
    "\n",
    "The main point of interest in this algorithm, is of course the part that contains the bellman equation. Notice how strictly speaking, the value at iteration $k+1$ depend only on the values at iteration $k$:\n",
    "\n",
    "#### $$ V_{k+1}(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_{k}(s') \\Big \\}$$\n",
    "\n",
    "However, this need not be the case. In fact, we can always just use our most up to date versions of the value function for any state. This actually ends up converging faster. \n",
    "\n",
    "## 1.2 Definitions\n",
    "A final note; we generally call the act of finding the value function for a given policy the _**prediction problem**_. Soon, we will learn an algorithm for finding the optimal policy, which is known as the _**control problem**_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Gridworld in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Environment\"\"\"\n",
    "class Grid: \n",
    "  def __init__(self, width, height, start):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0] # start is a tuple of 2 integers\n",
    "    self.j = start[1]\n",
    "    \n",
    "  def set(self, rewards, actions):\n",
    "    \"\"\"actions enumerate all possible actions that can take you to new state.\n",
    "       actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "       rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    \"\"\"\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "    \n",
    "  def set_state(self, s):\n",
    "    \"\"\"Useful for various algorithms we will use. For example, iterative policy evaluation\n",
    "    requires looping through all the states, and then doing an action to get to the next\n",
    "    state. In order to know what the next state is, we have to put the agent into that \n",
    "    state, do the action, and then determine the next state. We do not automatically have\n",
    "    a master list of state transitions, we must figure them out by playing the game.\"\"\"\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "    \n",
    "  def current_state(self):\n",
    "    \"Returns current (i,j) position of agent.\"\n",
    "    return (self.i, self.j)\n",
    "  \n",
    "  def is_terminal(self, s):\n",
    "    \"\"\"Returns true if s is terminal state, false if not. Easy way to check this is to see\n",
    "    if the state is in the action dictionary. If you can do an action from the state, then\n",
    "    you can transition to a different state, and hence your state is not terminal.\"\"\"\n",
    "    return s not in self.actions\n",
    "  \n",
    "  def move(self, action):\n",
    "    \"\"\"Checks if action is in actions dictionary. If not, we are not able to do this move,\n",
    "    so we simply stay in same position.\"\"\"\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      if action == 'U':\n",
    "        self.i -= 1\n",
    "      elif action == 'D':\n",
    "        self.i += 1\n",
    "      elif action == 'R':\n",
    "        self.j += 1\n",
    "      elif action == 'L':\n",
    "        self.j -= 1\n",
    "    # Return reward (if any, default is 0)\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "  \n",
    "  def undo_move(self, action):\n",
    "    \"\"\"Pass in the action you just took, and the environment will undo it. \n",
    "    Ex -> Just went up, it will move you back down.\"\"\"\n",
    "    if action == 'U':\n",
    "      self.i += 1\n",
    "    elif action == 'D':\n",
    "      self.i -= 1\n",
    "    elif action == 'R':\n",
    "      self.j -= 1\n",
    "    elif action == 'L':\n",
    "      self.j += 1\n",
    "    # Raise an exception if we arrive somewhere we shouldn't be -> Should never happen\n",
    "    assert(self.current_state() in self.all_states())\n",
    "    \n",
    "  def game_over(self):\n",
    "    \"Returns true if game over, false otherwise. Only need to check if in terminal state.\"\n",
    "    return (self.i, self.j) not in self.actions\n",
    "  \n",
    "  def all_states(self):\n",
    "    \"\"\"We can calculate all of the states simply by enumerating all of the states from \n",
    "    which we can take an action (which don't include terminal states), and all of the \n",
    "    states that return a reward (which do include terminal states). Since there may be\n",
    "    some of the same states in both actions and rewards, we cast it to a set. This also\n",
    "    makes the data structure O(1) for search.\"\"\"\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "  \n",
    "def standard_grid():\n",
    "  \"\"\"Returns standard grid. This is a grid that has the structure shown in section 4.\n",
    "  All of the actions are defined such that we can move within the grid, but never off\n",
    "  of it. We also cannot walk into the wall, nor out of the terminal state. Upper left\n",
    "  is defined to be (0,0). We define rewards for arriving at each state. The grid looks\n",
    "  like:\n",
    "      .  .  .  1\n",
    "      .  x  . -1\n",
    "      s  .  .  .\n",
    "  * x means you can't go there\n",
    "  * s means start position\n",
    "  * number means reward at that state\n",
    "  \"\"\"\n",
    "  g = Grid(3, 4, (2, 0))\n",
    "  rewards = {(0, 3): 1, (1, 3): -1}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "  }\n",
    "  g.set(rewards, actions)\n",
    "  return g\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "  \"\"\"Here we want to penalize each move. This is done to prevent a robot from moving \n",
    "  randomly to solve the maze. If you only gave it a reward for solving the maze, then it\n",
    "  would never learn anything beyond a random strategy. We know that we can incentivize\n",
    "  the robot to solve the maze more efficiently by giving it a negative reward for each\n",
    "  step taken. That is what we are doing here-incentivizing the robot to solve the maze \n",
    "  efficiently, rather than moving randomly until it reaches the goal.\"\"\"\n",
    "  g = standard_grid()\n",
    "  g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "  })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Iterative Policy Evaluation In Code\n",
    "We are now going to implement iterative policy evaluation in code. To demonstrate how we can find the value function for different policies, we are going to do iterative policy evaluation on two different policies. \n",
    "\n",
    "> The first policy we will look at is a completely random (_uniform_) policy. \n",
    "\n",
    "Remember, there are two probability distributions involved in bellmans equation:\n",
    "\n",
    "#### $$\\text{Policy probability: }\\hspace{1cm}\\pi(a \\mid s)$$\n",
    "\n",
    "\n",
    "#### $$\\text{Markov State Transition Probability: }\\hspace{1cm}p(s',r \\mid s,a)$$\n",
    "\n",
    "The probability that is relevant for implementing bellmans equation is $\\pi(a \\mid s)$. This is the probability that we take an action $a$, given that we are in state $s$. For a _uniform random_ policy, this probability will be:\n",
    "\n",
    "#### $$\\frac{1}{\\mid A(s) \\mid}$$\n",
    "\n",
    "Where $A(s)$ is the set of all possible actions to take from state $s$. In other words, our probability will be 1 divided by the total number of possible actions from state $s$. \n",
    "\n",
    "The other probability, $p(s', r \\mid s, a)$ is only relevant when state transitions themselves are random. That is a scenario when you try to move left, but instead you end up going right. \n",
    "\n",
    "> The second policy we will look at is a completely deterministic policy. \n",
    "\n",
    "From the start position you go directly to the goal state (up, up, right, right, right). However, if you are starting from any other state, the policy is to go directly to the losing state. So, we should expect the values on the upper left path to be positive, and the other values to be negative. \n",
    "\n",
    "Also, as a note/clarification-when performing iterative policy evaluation with random actions, our final value function will differ from the bellman equation as follows; the original bellman equation starts off as:\n",
    "\n",
    "#### <span style=\"color:#0000cc\">$$\\text{Bellman Equation} \\rightarrow V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$</span>\n",
    "\n",
    "We can drop the $p(s', r \\mid s, a)$, since as we stated earlier, our state transitions are not currently random. That means our value function looks like:\n",
    "\n",
    "#### $$V_\\pi(s) = \\sum_a \\pi(a \\mid s) *  \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$\n",
    "\n",
    "Because $\\pi(a \\mid s)$ is a uniform random distribution, it is _not_ dependent on the state and is actually equal to: \n",
    "\n",
    "#### $$\\pi(a) = \\frac{1}{\\mid A(s) \\mid}$$\n",
    "\n",
    "Hence, we can update our value equation to be:\n",
    "\n",
    "#### $$V_\\pi(s) = \\sum_a \\frac{1}{\\mid A(s) \\mid} *  \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$\n",
    "\n",
    "In pseudocode, that will look like:\n",
    "\n",
    "---\n",
    "```\n",
    "new_v += p_a * (r + gamma * V[grid.current_state()]) \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway**:<br>\n",
    "> Another key thing to keep in mind, is how our value function is actually represented. Generally, when we think of a _function_ we envision an equation of some sort, that maps one domain to another, such as: <br>\n",
    "<br>\n",
    "$$f(x) = x^2 + 4x + 8 $$\n",
    "<br>\n",
    "However, a function be more generally defined as: _**A rule that relates inputs to outputs in a dataset or system.**_ This allows us to have an alternative way of viewing functions: _As the actual set of input/output pairs it defines, or in other words, as a dataset._ This is the way in which our value function is defined. As we converge on a final value function, we are not determining some final equation, but rather a final mapping of our input domain (our 11 states) to our output domain, the value associated with each state. \n",
    "\n",
    "So remember:\n",
    "> _We can view an equation equivalently in two ways: either via its mathematical expression or as a dataset consisting of a complete listing of the function's input/output pairs._\n",
    "\n",
    "We can now get to the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 10e-4 # Threshold for convergence\n",
    "\n",
    "\"\"\"Functions to help us visualize policies and values.\"\"\"\n",
    "def print_values(V, g): \n",
    "  \"\"\"Takes in values dictionary and grid, draws grid, and in each position it prints\n",
    "  the value. \"\"\"\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "    \n",
    "def print_policy(P, g):\n",
    "  \"\"\"Takes in policy and grid, draws grid, and in each position it prints\n",
    "  the action from the policy. Note, this will only work for deterministic policies, \n",
    "  since we can't print more than 1 thing per location.\"\"\"\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i, j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \"\"\"Iterative Policy Evaluation:\n",
    "      * Given a Policy -> find its value function V(s)\n",
    "      * We will do this for both uniform random policy and fixed deterministic policy\n",
    "      * NOTE: There are 2 sources of randomness\n",
    "      * p(a|s)      -> deciding what action to take given the state\n",
    "      * p(s',r|s,a) -> the next state and reward given your action-state pair\n",
    "      * we are only modeling p(a|s) = uniform\"\"\"\n",
    "  grid = standard_grid()\n",
    "  \n",
    "  # States will be positions (i, j). Simpler than tic-tac-toe, because we only have \n",
    "  # 1 game piece that can only be at one position at a time. We get the set of all states\n",
    "  # from the grid, since these will be the keys to the value function dictionary.\n",
    "  states = grid.all_states()\n",
    "  \n",
    "  # ----- Perform Iterative Policy Evaluation for Uniform Random Actions -----\n",
    "  \n",
    "  # Initialize all V(s) to be 0\n",
    "  V = {v: 0 for v in states}\n",
    "  gamma = 1.0 # Discount factor\n",
    "  \n",
    "  # Enter infinite loop, repeat until convergence\n",
    "  while True:\n",
    "    biggest_change = 0\n",
    "    # Loop through all of the states\n",
    "    for s in states: \n",
    "      old_v = V[s] # Keep copy of old V(s), so we can track magnitude of each change\n",
    "      \n",
    "      # Loop through all possible actions from this state. V(s) only has value if it is \n",
    "      # not a terminal state.\n",
    "      if s in grid.actions:\n",
    "        new_v = 0 # we will accumulate the answer\n",
    "        p_a = 1.0 / len(grid.actions[s]) # Each action has equal probability\n",
    "        for a in grid.actions[s]:\n",
    "          # Look ahead action comes into play. Must first set state to s, and then do the\n",
    "          # action, so that we can determine the next state s', since that is required\n",
    "          # to use the bellman equation\n",
    "          grid.set_state(s) \n",
    "          r = grid.move(a)\n",
    "          new_v += p_a * (r + gamma * V[grid.current_state()]) # This is RL portion!\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "      \n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for uniformly random actions:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"\\n\\n\")\n",
    "    \n",
    "  # ----- Perform Iterative Policy Evaluation for Fixed Policy -----\n",
    "  # Print policy so we know what it looks like\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "  print_policy(policy, grid)\n",
    "  \n",
    "  # Reinitialize all V(s) to be 0\n",
    "  V = {v: 0 for v in states}\n",
    "  \n",
    "  # Let's see how V(s) changes as we get further away from the reward\n",
    "  gamma = 0.9 # discount factor\n",
    "  \n",
    "  # Repeat until convergence. Simpler loop, we don't need to loop through any actions, \n",
    "  # because there is only one action per state. Because there is only 1 action per state,\n",
    "  # the probability of that action in that state is 1. \n",
    "  while True:\n",
    "    biggest_change = 0 \n",
    "    for s in states: \n",
    "      old_v = V[s]\n",
    "      \n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        a = policy[s]\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        V[s] = r + gamma * V[grid.current_state()]\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for fixed policy:\")\n",
    "  print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look at the results, we can see that most of the value for the uniform random policy are negative. That is because if you are moving randomly, there is a good chance you end up in the losing state. Remember, there are two ways you can end up in the losing state, but only one way you can enter the goal state. The most negative value is for the state right underneath the losing state, which makes sense. \n",
    "\n",
    "Now, for the fixed policy and discount factor 0.9, we see exactly what we expect to see: the further away we get from the terminal state, the more the value decreases, and each time it decreases by exactly 10%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Policy Improvement\n",
    "We can now start discussing the _control problem_, the problem of how to find better policies, eventually leading to the optimal policy. What we know so far is how to find the value function given a fixed policy. Let's look at our recursive definition of $Q_\\pi(s,a)$ again:\n",
    "\n",
    "#### $$Q_{\\pi}(s,a) = \\sum_{s',r}p(s',r \\mid s,a)\\big[r + \\gamma V_\\pi (s')\\big]$$\n",
    "\n",
    "This tells us the value of doing action $a$, while in state $s$, using the policy $\\pi$. Using the current policy, we simply get the current state value function. \n",
    "\n",
    "#### $$V_\\pi(s) = Q_\\pi(s, \\pi(s)) = \\sum_{s',r}p(s',r \\mid s,\\pi(s))\\big[r + \\gamma V_\\pi (s')\\big] $$\n",
    "\n",
    "Now let's say that we want to change just one of the actions in the policy; Can we do this? Of course we can! We have a finite set of actions, so all we need to do is just go through each one until we get a better Q, than $\\pi(s)$ does:\n",
    "\n",
    "#### $$find \\; a \\in A \\; s.t. Q_\\pi(s,a) > Q_\\pi(s, \\pi(s)) $$\n",
    "\n",
    "This is where doing all of the programming exercises we go through becomes _very useful_. This looks like a really abstract equation, but all it's saying is: \n",
    "> _If the policy is currently to go up, let's look at left, right, and down to see if we can get a bigger Q. If it does, let's change our policy for that state to this new action._\n",
    "\n",
    "Formally speaking, what we are doing when we choose a new action for the state is finding a new policy $\\pi'$, that has a bigger value for the state than $\\pi$:\n",
    "\n",
    "#### $$V_\\pi(s) \\leq V_{\\pi'}(s)$$\n",
    "\n",
    "All we need to do this is to pick an action that gives us maximum $Q$. We can write this in the form where we are using $Q$:\n",
    "\n",
    "#### $$\\pi'(s) = argmax_a\\big(Q_\\pi(s,a)\\big)$$\n",
    "\n",
    "Or in the form where we are doing a lookahead search on $V$:\n",
    "\n",
    "#### $$\\pi'(s) = argmax_a \\big(\\sum_{s', r} p(s', r \\mid s,a) \\big[r + \\gamma V_\\pi(s')\\big]\\big)$$\n",
    "\n",
    "## 4.1 Things to notice\n",
    "There are a few things about policy improvement that you should notice:\n",
    "\n",
    "1. First, notice that it is _greedy_. We are never considering globally the value function at all states. We are only looking at the current state, and picking the best action based on the value function at that state. \n",
    "\n",
    "2. Second, notice how it uses an imperfect version of $V_\\pi(s)$. Once we change $\\pi$, $V_\\pi(s)$ also changes. We will see soon why this is not a problem. \n",
    "\n",
    "One question that may arise is: How do we know when we are finished trying to change the policy? When we have found the optimal policy, the policy will not change with respect to the value function. In addition, the value function will no longer improve-it will stay constant. Notice how the inequality we had earlier was less than or equal to:\n",
    "\n",
    "#### $$V_\\pi(s) \\leq V_{\\pi'}(s)$$\n",
    "\n",
    "In the case where it is less than, we are still improving. In the case where it is equal to, then we have found the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Policy Iteration\n",
    "We are now going to discuss the algorithm we are going to use to find the optimal policy. It will solve the problem we encountered in the previous lecture, specifically, when we change the policy the value function also changes and becomes out of date. We will now see how we can rectify that problem.\n",
    "\n",
    "So, what do we do when we change the policy and the value funcion becomes out of date? Well, we can simply recalculate the value function. Luckily we already know how to do find $V$ given $\\pi$! We have written the code already, and the algorithm is called _iterative policy evalutation_. At a high level this algorithm is very simple: We just alternate between _policy evaluation_ and _policy improvement_. We keep doing this until the policy doesn't change. Note that what we are _not_ checking for is the value function converging (although it will anyway because once the policy stops changing we only need to do one more iteration of policy evaluation). \n",
    "\n",
    "In pseudcode **policy iteration** looks like:\n",
    "\n",
    "```\n",
    "Step 1. Randomly initialize V(s) and policy pi(s)\n",
    "\n",
    "Step 2. V(s) = iterative_policy_evaluation(pi)\n",
    "\n",
    "Step 3. Policy Improvement\n",
    "policy_changed = False\n",
    "for s in all_states:      # Loop through all states\n",
    "  old_a = policy(s)\n",
    "  policy(s) = argmax[a] { sum[s', r] { p(s',r | s,a) [r + gamma*V(s')] } }\n",
    "  if policy(s) != old_a:\n",
    "    policy_changed = True\n",
    "if policy_changed:\n",
    "  go back to step 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Policy Iteration in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
