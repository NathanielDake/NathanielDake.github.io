{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Approximation Methods\n",
    "We are now going to look at approximation methods. Recall that in the last section, we discussed a major disadvantage to all of the methods we have studied so far. That is, they all require us to estimate the value function for each state, and in the case of the action-value function, we have to estimate it for each state and action pair. We learned early on that the state space can grow very large, very quickly. This makes _all_ of the methods we have studied impractical. \n",
    "\n",
    "> * $V$ - Need to estimate |S| values\n",
    "* $Q$ - Need to estimate |S|x|A| values\n",
    "* |S| and |A| can be very large\n",
    "\n",
    "The solution to this is _**approximation**_. \n",
    "\n",
    "## 1.1 Approximation Theory\n",
    "Recall from our earlier work concerning deep learning, that neural networks are universal function approximators. That means that given the right architecture, a neural network can approximate any function to an arbitrary degree of accuracy. In practice, they do not perform perfectly, but they do perform very well.\n",
    "\n",
    "Mathematically, what we are trying to do is first do a feature extraction: so from the state $s$ we can extract a feature vector $x$:\n",
    "\n",
    "$$x = \\varphi (s)$$\n",
    "\n",
    "Our goal is to then find a _function_ that takes in a feature vector $x$, and a set of parameters $\\theta$, that faithfully approximates the value function $V(s)$:\n",
    "\n",
    "$$\\hat{f}(x, \\theta) \\approx V(s)$$\n",
    "\n",
    "## 1.2 Linear Approximation\n",
    "In this section, we are going to focus specifically on linear methods. We will see that function approximation methods require us to use models that are _differentiable_, hence we wouldn't be able to use something like a decision-tree or k-nearest neighbor. In the next set of notebooks (RL with deep learning) we will look at using deep learning methods, which are also differentiable. Unlike linear models, we won't need to do feature engineering before hand, although we could. Models like convolutional neural networks will allow us to use raw pixel data as the state, and the neural network will do its own automatic feature extraction and selection. However, those are harder to implement and take away from the fundamentals of RL, so we will hold off on them for now. For now, all we will need to know are linear regression and gradient descent. \n",
    "\n",
    "## 1.3 Section 8 Outline\n",
    "We are going to proceed with the following outline for this section:\n",
    "\n",
    "> * We are first going to apply approximation methods to Monte Carlo Prediction. That means we will be estimating the value function given a fixed policy. But instead of representing the value function as a dictionary indexed by state, we will use a linear function approximator. Recall that MC methods require us to play the entire episodes and calculate the returns before doing any updates. So next we will...\n",
    "* Apply approximation methods to `TD(0)` prediction. Remember, `TD(0)` takes aspects of both MC sampling and the bootstrap method of DP. \n",
    "* After working on the prediction problem, we will move to the control problem, and we will use SARSA for this. But we will of course be replacing $Q$ with a linear function approximator. \n",
    "\n",
    "## 1.4 Sanity Checking\n",
    "One thing to keep in mind in this section, is that we can always sanity check our results by comparing to the non-approximated version. We expect our approximation to be close, but not perfect. One obstacle that we may encounter is that our algorithm may be implemented perfectly, but your model is bad. Remember, linear models are _not_ very expressive. So, if we extract a poor set of features, the model won't be able to learn the value function well. In other words, the model will have a large error. To avoid this, we need to proactively think about what features are good for mapping states to values. We will need to put in manual work for feature engineering in order to improve our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 2. Linear Models for RL\n",
    "What we are going to do now is apply supervised learning to reinforcement learning. Recall that supervised learning basically amounts to function approximation. We are trying to find a parameterized function that closely approximates the true function. In this case, that true function is the value function that we use to solve MDPs. \n",
    "\n",
    "Earlier in the course, we talked about the fact that rewards have to be real numbers. Since returns are sums of rewards, they also have to be real numbers. And since values are expected values of returns, they are also real numbers. So, thinking about the supervised learning techniques we have at our disposal-classification and regression-it should be clear that what we want to do here is regression. \n",
    "\n",
    "## 2.2 Error\n",
    "In particular, we want our estimate, $\\hat{V}$, to be close to the true $V$. As we know, for all supervised learning methods we need a cost function, and the appropriate cost function for linear regression is _squared error_. We can represent it as follows:\n",
    "\n",
    "$$Error = \\big[V(s) - \\hat{V}(s)\\big]^2$$\n",
    "\n",
    "Now that we have our basic error function, we can replace $V$ with is definition:\n",
    "\n",
    "$$Error = \\big[E[G(t) \\mid S_t = s] - \\hat{V}(s)\\big]^2$$\n",
    "\n",
    "However, since we do not know this expected value, we need to replace it with something else. In particular, we can take what we learned from Monte Carlo, and we can replace it with the _sample mean_ of the actual returns:\n",
    "\n",
    "$$Error = \\big[\\frac{1}{N}\\sum_{i=1}^N G_{i,s}- \\hat{V}(s)\\big]^2$$\n",
    "\n",
    "An alternative way of looking at this is that we treat each state and return pair as a training sample. In this way, we will try to minimize the individual squared differences between $G$ and $\\hat{V}$ simultaneously. This will look just like linear regression, as expected:\n",
    "\n",
    "$$Error = \\sum_{i=1}^N \\big[G_{i,s} - \\hat{V}(s) \\big]^2$$\n",
    "\n",
    "$$Error = \\sum_{i=1}^N \\big(y_i - \\hat{y}_i \\big)^2$$\n",
    "\n",
    "## 2.3 Stochastic Gradient Descent\n",
    "The advantage of representing the error in this way, is that it allows us to do _stochastic gradient descent_. This is where we take a small step in the direction of the gradient, with respect to the cost of only one sample at a time. This is perfect for our needs, because at every iteration of the game, we only have one sample (state and return pair) to look at. \n",
    "\n",
    "## 2.4 Gradient Descent\n",
    "We can recall, that in linear regression our function approximator is parameterized by a set of weights (generally either refered to as $w$ of $\\theta$). Here, we can let $\\hat{V}$ be parameterized by $\\theta$. In other words, what we are trying to find is the $\\theta$ that allows $\\hat{V}$ to be the best approximation. To achieve this, we want to do gradient descent with respect to $\\theta$, and minimize the error we derived earlier. \n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\partial E}{\\partial \\theta}$$\n",
    "\n",
    "For clarity, recall that $\\frac{\\partial E}{\\partial \\theta}$ is just representing how the squared error changes with respect to a change in $\\theta$. And once again, $\\alpha$ represents the learning rate. We can take this a step further, and replace the error with the squared difference we just derived earlier:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\theta} = \\frac{\\big[G_{i,s}- \\hat{V}(s, \\theta)\\big]^2}{\\partial \\theta}$$\n",
    "\n",
    "$$\\theta = \\theta + \\alpha \\Big( G - \\hat{V}(s, \\theta)\\Big) \\frac{\\partial \\hat{V}(s,\\theta)}{\\partial \\theta}$$\n",
    "\n",
    "Note: above the 2 from the exponent is drop after the derivative is taken. Remember, this is stochastic gradient descent, so we are only looking at one sample of $G$ at a time. \n",
    "\n",
    "## 2.5 Gradient Descent for Linear Models \n",
    "Recall that we are only looking at linear models in this class, so $\\hat{V}$ is the dot product of the feature vector $x$ and $\\theta$. \n",
    "\n",
    "$$\\hat{V}(s, \\theta) = \\theta^T \\varphi(s) = \\theta^T x$$\n",
    "\n",
    "This dot product is just a linear combination, which can be expanded into the more familiar form: \n",
    "\n",
    "$$\\hat{V}(s, \\theta) = \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $$\n",
    "\n",
    "In other words, the derivative of $V$ with respect to $\\theta$ is $x$:\n",
    "\n",
    "$$\\frac{\\partial \\hat{V} (s, \\theta)}{\\partial \\theta} = x$$\n",
    "\n",
    "So we can formulate our new update rule as follows:\n",
    "\n",
    "$$\\theta = \\theta + \\alpha \\Big( G - \\hat{V}(s, \\theta)\\Big) x$$\n",
    "\n",
    "## 2.6 Relationship to Monte Carlo\n",
    "Something interesting happens when we think back to our Monte Carlo methods; in other words, when we were not parameterizing $V$. Instead, $\\hat{V}$ itself was the parameter we were trying to find, and we were trying to find it for all states $s$. If $\\hat{V}$ itself is the parameter, then we get this update equation:\n",
    "\n",
    "$$\\hat{V}(s) = \\hat{V}(s) + \\alpha \\big(G_s - \\hat{V}(s)\\big) \\frac{\\partial \\hat{V}(s)}{\\partial \\hat{V}(s)}$$\n",
    "\n",
    "$$\\hat{V}(s) = \\hat{V}(s) + \\alpha \\big(G_s - \\hat{V}(s)\\big)$$\n",
    "\n",
    "But we can recall that this is the exact same equation that we had before for updating the mean! So, we can see what we were doing before to find $V$ was actually an instance of gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Feature Engineering\n",
    "We are now going to look at how feature engineering can be applied in the case of RL. Recall, Neural Networks can in some sense automatically find good nonlinear transformations/features of the raw data. But, we are only looking at linear methods for now, which means we will need to come up with features on our own. \n",
    "\n",
    "## 3.1 Mapping s $\\rightarrow$ x\n",
    "One way to think of states is that they are categorical variables. For instace:\n",
    "\n",
    "> * **(0,0)** $\\rightarrow$ category 1 \n",
    "* **(0,1)** $\\rightarrow$ category 2\n",
    "* and so on...\n",
    "\n",
    "How do we treat categorical variables? We do so via the technique _**one-hot encoding**_. So, what is do one-hot encoding? Well, we have $S$ states, so the dimensionality of $x$, which we will call $D$, is:\n",
    "\n",
    "$$D = \\mid S \\mid$$ \n",
    "\n",
    "This means that we have a long feature vector of size $\\mid S \\mid$. Then for each of the states, we set one of the values in $x$ to 1:\n",
    "\n",
    "$$s = (0,0) \\rightarrow x = [1,0,0,...]$$\n",
    "\n",
    "$$s = (0,1) \\rightarrow x = [0,1,0,...]$$\n",
    "\n",
    "The problem with this is that it doesn't allow us to compress the amount of space it takes to store the value function! It requires the same number of parameters as measuring $V(s)$ directly. Remember, compressing the amount of space is the whole reason we are doing this in the first place. If we store $V$ as a dictionary, it will have $\\mid S \\mid $ keys, and $\\mid S \\mid$ values. Hence, we make no improvement if we do one hot encoding. In fact, it is equivalent to what we were doing before, since each $\\theta$ would represent the value for the corresponding state:\n",
    "\n",
    "$$V(s=0) = \\theta^T [1,0,0,...] = \\theta_0$$\n",
    "\n",
    "## 3.2 One-Hot Encoding\n",
    "There is, however, one positive aspect to using one-hot encoding for your feature transformation. Let's suppose your algorithm isn't working, and $\\hat{V}$ isn't representing the true $V$ very well. One reason your $\\hat{V}$ may not be good, is because your features may be bad! So, your code may be fine and free of bugs, but still yield poor results because the features are bad. In that case, you could change your feature transformer to use one-hot encoding, where you could predict each $V(s)$ individually. If you do this and your code works, that tells your that your features are bad (since it's the same as a non-approximation method). \n",
    "\n",
    "## 3.3 Alternative to One-Hot Encoding\n",
    "So, if we know that one-hot encoding is bad, then what is good? Well, in the case of grid world, consider that each (i,j) represents a position in 2-d space. Therefore, it is more like a real number than a category. So, we can represent the $x$ vector as simply (i,j) itself. You may want to scale it so that its mean is 0 and variance is 1. We would consider this feature vector to simply be the raw data, without any feature engineering:\n",
    "\n",
    "$$(x_1, x_2) = (i,j)$$\n",
    "\n",
    "So, what is the problem with just setting $x$ to be the location of the agent? Well, remember that our model is linear. Let's say $j$ is fixed-that means $V$ can only change linearly with respect to $i$. That means $V$ can only be a line with respect to this x coordinate. So, it is always increasing, or always decreasing-this is _not_ very expressive. \n",
    "\n",
    "## 3.4 Polynomials \n",
    "Recall, that one easy way to make new features is by creating polynomials. In calculus, it is shown that an infinite Taylor Expansion can approximate any function. So, if we start with $x_1$ and $x_2$ we can create the terms:\n",
    "\n",
    "$$x_2^2$$\n",
    "\n",
    "$$x_2^2$$\n",
    "\n",
    "$$x_1 x_2$$\n",
    "\n",
    "We can also create higher order polynomials, but we must be careful of overfitting. This is the method we will use from here on out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Monte Carlo Prediction with Approximation\n",
    "We can now start applying approximation methods to the prediction problem and control problem. We are going to start with the prediction problem! We are going to use Monte Carlo estimation but replace the value function dictionary with our linear model of the value function. \n",
    "\n",
    "## 4.1 Two Main Steps\n",
    "Recall that for MC estimation, we have two main steps which we want to do repeatedly. \n",
    "\n",
    "> 1. Play the game and calculate a list of states and returns. \n",
    "2. Update $\\hat{V}(S)$ using the return as the target and $V(S)$ as the prediction:<br>\n",
    "<br>\n",
    "$$\\hat{V}(s) = \\hat{V}(s) + \\alpha \\big(G_s - \\hat{V}(s) \\big)$$\n",
    "<br>\n",
    "Remember, this is gradient descent, but it is also equivalent to calculating the mean of all the returns for this state.\n",
    "\n",
    "## 4.2 With Approximation\n",
    "Since in this lecture we are approximation $V(s)$ with the parameter $\\theta$, what we want to update instead is the parameter $\\theta$:\n",
    "\n",
    "> 1. Play the game and calculate a list of states and returns. \n",
    "2. Update $\\hat{V}(S)$ as average of returns:<br>\n",
    "<br>\n",
    "$$\\theta = \\theta + \\alpha \\big(G - \\hat{V}(s, \\theta) \\big)x$$\n",
    "<br>\n",
    "Here, we continue to do stochastic gradient descent, but with respect to $\\theta$ instead of $\\hat{V}$. \n",
    "\n",
    "## 4.3 Pseudocode\n",
    "Remember that this is for a fixed policy, since we are focusing on the prediction problem. Let's look at some pseudocode to solidify this idea:\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{def mc_approx_prediction}(\\pi)\\text{:} \\\\\n",
    "\\hspace{1cm} \\theta =\\text{random} \\\\\n",
    "\\hspace{1cm} \\text{for i=1..N:} \\\\\n",
    "\\hspace{2cm} \\text{states_and_returns = play_game} \\\\\n",
    "\\hspace{2cm} \\text{for s, g in states_and_returns:} \\\\\n",
    "\\hspace{3cm} x = \\Phi(s)\\\\\n",
    "\\hspace{3cm} \\theta \\leftarrow \\theta + \\alpha (g - \\theta^T x)x \\\\\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "> * We start by taking in an input policy, $\\pi$, and randomly initializing the $\\theta$ vector\n",
    "* Next, we enter a loop for some number of iterations\n",
    "* On each iteration, we play the game, which returns a sequence of states and returns\n",
    "* Loop through these states and returns, and apply our update equation to $\\theta$, treating our return as the target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. MC Prediction with Approximation in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common import standard_grid, negative_grid, print_policy, print_values\n",
    "from common import random_action, play_game, SMALL_ENOUGH, GAMMA, ALL_POSSIBLE_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107663e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:\n",
      "---------------------------\n",
      " 0.36| 0.53| 0.70| 0.00|\n",
      "---------------------------\n",
      " 0.32| 0.00| 0.33| 0.00|\n",
      "---------------------------\n",
      " 0.27| 0.12|-0.04|-0.19|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# Note: This is policy evaluation, not optimization \n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # found by policy_iteration_random on standard_grid\n",
    "  # MC method won't get exactly this, but should be close\n",
    "  # values:\n",
    "  # ---------------------------\n",
    "  #  0.43|  0.56|  0.72|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.33|  0.00|  0.21|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.25|  0.18|  0.11| -0.17|\n",
    "  # policy:\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   L  |   U  |   L  |\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "  \n",
    "  # Randomly initialize theta vector. Our model is V_hat = theta.dot(x), where\n",
    "  # x = [row, column, row*column, 1] , where the 1 is for the bias term\n",
    "  theta = np.random.randn(4) / 2 \n",
    "  \n",
    "  # Define a function that turns the state into a feature vector x. The only nonlinear \n",
    "  # feature is the interaction effect between the i and j coordinate (s[0] and s[1])\n",
    "  def s2x(s):\n",
    "    return np.array([s[0] - 1, s[1] - 1.5, s[0]*s[1] - 3, 1])\n",
    "  \n",
    "  # Repeat until converge - Main Loop\n",
    "  deltas = []\n",
    "  t = 1.0\n",
    "  for it in range(2000):\n",
    "    if it % 100 == 0:\n",
    "      t += 0.01\n",
    "    alpha = LEARNING_RATE/t # Using decaying learning rate\n",
    "    \n",
    "    # Generate an episode using pi. Pattern is the same as before. We play the game, \n",
    "    # and get a sequence of states and returns. We then loop through the states and \n",
    "    # returns, but instead of updating V, we now update the parameter theta, using the \n",
    "    # equation we derived earlier for stochastic gradient descent. \n",
    "    biggest_change = 0\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # Check if we have already seen s. This is 'first-visit' MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        old_theta = theta.copy()\n",
    "        x = s2x(s)\n",
    "        V_hat = theta.dot(x)\n",
    "        theta += alpha*(G - V_hat)*x\n",
    "        biggest_change = max(biggest_change, np.abs(old_theta - theta).sum())\n",
    "        seen_states.add(s)\n",
    "    deltas.append(biggest_change)\n",
    "  \n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "  \n",
    "  # Obtain predicted values\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      V[s] = theta.dot(s2x(s))\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
