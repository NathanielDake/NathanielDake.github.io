{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Components of an RL System\n",
    "\n",
    "<img src=\"images/sar-flow.png\" width=\"500\">\n",
    "\n",
    "Let's quickly recall what we had discussed earlier concerning the components of an RL system. We talked about the following:\n",
    "\n",
    "> * **Agent**: The thing that is playing the game, that we want to program the RL algorithm into.\n",
    "<br>\n",
    "<br>\n",
    "* **Environment**: The thing that the agent interacts with; the agents world.\n",
    "<br>\n",
    "<br> \n",
    "* **State**: Specific configuration of the environment that the agent is sensing. Note, the state only represents that which the agent can sense. \n",
    "<br>\n",
    "<br> \n",
    "* **Actions**: Things that an agent can do that will affect its state. In Tic-Tac-Toe, that's placing a piece on the board. Performing an actions always brings us to the next state, which also comes with a possible reward. \n",
    "<br>\n",
    "<br> \n",
    "* **Rewards**: Tells you how good your action was, not whether it was a correct or incorrect action. It does not tell you whether it was the best or worst action, it is just a number. The rewards you have received over the course of your existence doesn't necessarily represent possible rewards you could get in the future. For example, you could search a bad part of state space, hit a local max of 10pts, while the global max was actually 1000 pts. The agent does not know that (but in our case we will, since we designed the game). \n",
    "\n",
    "## 1.1 Notation\n",
    "We know that being in a state, $S(t)$, and taking an action $A(t)$, will lead us to the reward $R(t+1)$ and the state $S(t+1)$:\n",
    "\n",
    "#### $$S(t), A(t) \\rightarrow R(t+1), S(t+1)$$\n",
    "\n",
    "However, when we drop the time index's, we represent this as the 4-tuple: \n",
    "\n",
    "#### $$(s,a,r,s')$$\n",
    "\n",
    "The $r$ is not given a prime as you would expect, but it is standard notation. So, the prime symbol doesn't strictly mean \"at time t + 1\"\n",
    "\n",
    "## 1.2 New Terms \n",
    "The first new term we want to discuss is **Episode**. This represents one run of the game. For example, we will start a game of tic tac toe with an empty board, and as soon as one player gets 3 pieces in a row, that's the end of the episode. As you may imagine, our RL agent will learn across many episodes. For example, after playin 1000, 10000, or 100000 episodes, we can possibly have trained an intelligent agent. The number of episodes we will use is a hyper parameter and will depend on the game being played, the number of states, how random the game is, etc. \n",
    "\n",
    "Playing the game tic-tac-toe is an **episodic task** because you can play it again and again. This is different from a **continuous task** which never ends. We will not be looking at continuous tasks in this course. \n",
    "\n",
    "Now, the next question we ask is: when is the end of an episode? Well, there are certain states in the state space that tell us when the episode is over. These are states from which no more action can be taken. They are referred to as **terminal states**. For tic-tac-toe these are when one player gets 3 in a row, or when the board is full (a draw). \n",
    "\n",
    "## 1.3 Cart-Pole / Inverted Pendulum\n",
    "This problem comes up all the time in RL and control systems. If you search google for inverted pendulum, you will see research papers concerning control systems, and if you search cart-pole you will see all kinds of RL research papers. At the beginning of an episode, the cart is stationary and the pole is perpendicular to the ground. Because the system is unstable, the pole will then begin to fall, and the job of the cart is to move so that the pole does not fall down. \n",
    "\n",
    "When the pole falls so far that it is impossible to get back up, any angle past a threshold where it is impossible to get the pole back up is a terminal state. Note, because the angle in this example is a real number, is a continuous/infinite space. We will not deal with these. \n",
    "\n",
    "## 1.4 Assigning Rewards\n",
    "One difficult problem in reinforcement learning that we will come across is: defining rewards. We, the programmers, can be thought of as coaches to the AI. The reward is something that we give to the agent. So, we can define how we are going to reward the agent, which will drive how it learns. \n",
    "\n",
    "For example, if we just give it the same reward no matter what it does, then the agent will probably just end up acting randomly, since any action will lead to the same value. You don't want to do that, because it will encourage bad behavior. \n",
    "\n",
    "A real situation could be seen with a robot trying to solve a maze. If it manages to exit the maze it would receive a reward of 1, else it would receive 0. Most people would think that this seems reasonable, and it is _semi-reasonable_. However, with this reward structure, the robot may never actually solve the maze. So, if it has only ever received a reward of 0, it may think that it is the best it can do. A better solution would be to give the robot a -1 for every step it takes, and then it would be encouraged to solve the maze as quickly as possible. \n",
    "\n",
    "### 1.4.1 Caution\n",
    "One point of caution is to not build your own prior knowledge into the AI. For example, in a game such as chess, an agent should be rewarded only for winning, not taking opponent's pieces, or implementing some strategy that you read about in a chess book. You want to leave the agent free to come up with its own solution. The danger of rewarding the agent for achieving sub goals, is that they may find a novel way to maximize the reward for the subgoals, without actually winning the game. For example, taking all of the opponents chess pieces and then still losing the game. \n",
    "\n",
    "So to summarize, we can say that:\n",
    "\n",
    "> \"The reward signal is your way of telling the agent what you want it to achieve, now how you want it to be achieved.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 3. The Value Function \n",
    "Take a moment to consider the following scenario. You have an exam tomorow. You would like to hang out with your friends. You know if you hangout with you friends you will most likely have a dopamine hit and feel happy. On the other hand if you study for your exam you will feel tired and potentially bored. So why study? Well, this is the idea of **planning**.\n",
    "\n",
    "In particular, we can describe a *value function*:\n",
    "\n",
    "> **Value Function**: We don't just think about immediate rewards, but future rewards too. We want to assign some value to the current state that reflects the future too.\n",
    "\n",
    "Now, we can think of this in the reverse direction as well. Let's say you receive a reward; getting hired for your dream job. Now, if you look back to you career and things you did in school, what would you attribute your success to? What state of being in your past lead you to get your dream job today? This is refered to as the *credit assignment problem*:\n",
    "\n",
    "> **Credit Assignment Problem**: What did you do in the past that led to the reward you are receiving now? In other words, what action gets the credit. \n",
    "\n",
    "The credit assignment problem shows up in online advertising as well, but the concept is referred to as **attribution**. The idea is that if a user is shown the same ad 10 different times before they buy, which ad gets the credit? \n",
    "\n",
    "Now, closely related to the credit assignment problem is the idea of **delayed rewards**. Note that these are all kind of just different ways of saying the same thing, and the solution is also the same. Delayed rewards is just looking at the problem from the other direction. With credit assignment, we are looking into the past and asking \"what action lead to the reward we are getting now?\". With delayed rewards, we are asking \"How is the action I am taking now related to rewards I may potentially receive in the future?\" \n",
    "\n",
    "The idea of delayed rewards tells us that an AI must have the ability of foresight, or planning. \n",
    "\n",
    "## 3.1 Example Scenario\n",
    "\n",
    "<img src=\"images/scenario-1.png\">\n",
    "\n",
    "Imagine the following: you are in state A, which is the second last state in a game. There are only two possible next states, both of which are terminal states. B gives you a reward of 1, and C gives you a reward of 0. You have a 50% probability of going to either state; perhaps your agent doesn't know which one is best. We can think of the value of state B as 1, and the value of state C as 0. So, what is the value of state A? We can think of it as 0.5, since it is the expected value of your final reward, given that you have a 50% chance of ending up in either state:\n",
    "\n",
    "#### $$Value(A) = 0.5*1 + 0.5*0 = 0.5$$\n",
    "\n",
    "Now, lets say that you are in state A, and state A can only lead to state B, and there is no other possible next state:\n",
    "\n",
    "<img src=\"images/scenario-2.png\">\n",
    "\n",
    "If B gives you a reward of 1, then perhaps A's value should also be 1, since the only possible final scenario is to have a final reward of 1, once you reach A:\n",
    "\n",
    "#### $$Value(A) = 1 * 1 = 1$$\n",
    "\n",
    "Thus, the value tells us about the \"future goodness\" of a state. We can make this a little more formal; we actually call this value, the value function. \n",
    "\n",
    "## 3.2 Value Function\n",
    "The value function is a measure of the future rewards that we may get:\n",
    "\n",
    "> **V(s)** = the value (taking into account the probability of all possible future rewards) of a state\n",
    "\n",
    "The name value, is not quite ideal, since it is very ambiguous. However, it is part of the RL nomenclature, so we will deal with it. \n",
    "\n",
    "### 3.2.1 Rewards vs. Values\n",
    "It is easy to get rewards and values mixed up. The difference is that the value of state, is a measure of the possible future rewards we may get from being in that state. Rewards on the other hand are immediate. \n",
    "\n",
    "We, therefore, chose actions to take based on **values of states**, and not on the reward we would get by going to a state! The reward is the main goal, but we can't use the reward to tell us how good a state is, since it doesn't tell us anything about future rewards. \n",
    "\n",
    "### 3.2.2 Efficiency\n",
    "One way to think about the value function, is that it is a fast and efficient way to determine how good it is to be in a state, without needing to search the rest of the game tree. You could try to enumerate every possible state transition, and their probabilities of occuring; however, you can guess that this would be a computationally inefficient task. In fact, tic tac toe is easy since it is only a 3x3 board, so the number of states is approximately 3^(3x3) = 19683. However, that will grow exponentially with the size of the board! For example, if you have a connect 4 board, then you get 3^(4x4) = 43 million! As we know, exponential growth is never good, and hence searching the state space is only possible in the simplest of cases. Hence, a value function that can tell you how good you will do in the future, given only your current state, is *extremely* helpful. This means that $V(s)$ gives an answer instantly, in only $O(1)$ time! The only question is, is it even possible to find a value function...\n",
    "\n",
    "### 3.2.3 Value Functions in RL\n",
    "Estimating the value function is a central task in RL, but it should be noted that not all RL algorithms require it. For instance, a genetic algorithm simply spawns offspring that have random genetic mutations, and the ones who survive the longest will go on to spawn offspring in the next generation. So, by pure evolution and natural selection, we can breed better and better agents that get iteratively better at the game! However, this is not the type of algorithm that we are interested in for RL, most of the time. \n",
    "\n",
    "### 3.2.4 Value Function: Math\n",
    "So, we can dig into the math and notation surrounding the value function now. The value function takes in one parameter, the state, so we can denote it $V(s)$. It is the expected value of all future rewards, given that the current state is $s$:\n",
    "\n",
    "#### $$V(s) = E[all \\; future \\; rewards \\; | \\; S(t) = s]$$\n",
    "\n",
    "In other words, this is the average value of all possible future rewards, given that the current state is $s$. \n",
    "\n",
    "### 3.2.5 Finding $V(s)$\n",
    "Let's now look at a generic algorithm that we can use to find the value function. For now, we are going to introduce some unrealistic constraints, to show that the problem can actually be solved. However, later on it will be clear that we don't need to do this. The algorithm we will use is an iterative one, and each time we run the loop we will get closer and closer to the true value function. \n",
    "\n",
    "So, the first thing that we do is **initialize the value function**:\n",
    "\n",
    "> * For all states where the agent wins, we say the value is 1. \n",
    "    $$V(s) = 1 \\text{if s = winning state}$$ \n",
    "* For all states where the agent loses or it is a draw, we say the value is 0. \n",
    "    $$V(s) = 0 \\text{if s = winning state}$$ \n",
    "* For all other states, we say the value is 0.5. \n",
    "    $$V(s) = 0.5 \\text{if s = winning state}$$ \n",
    "    \n",
    "When we study the \"real\" algorithms soon, we will not need to focus on such careful initialization. For this game, $V(s)$ can be interpreted as probability of winning after arriving in s (for this game only). \n",
    "\n",
    "**Update Function**<br>\n",
    "The update function looks simple, but there are some hidden details. It kind of looks like gradient descent:\n",
    "\n",
    "#### $$V(s) \\leftarrow V(S) + \\alpha * (V(s') - V(s))$$\n",
    "\n",
    "We take the value function at a state, $V(s)$, and we update it by adding the learning rate times $V(s')$ minus $V(s)$; here, $s$ is the current state, and $s'$ is the next state. \n",
    "\n",
    "### 3.2.6 Detail 1\n",
    "The first detail here is that $s$ represents every state that we encounter in an episode. That means that for each iteration of the loop, we actually need to play the game and remember all of the states that we were in. We then loop through each of the states in the state history, and then update the value using the above equation. Notice that a terminal state value will never be updated, because to update a state you need the next state value, which does not exist if we are in a terminal state. \n",
    "\n",
    "In Pseudocode it may look like:\n",
    "\n",
    "```\n",
    "for t in range(max_iterations):\n",
    "  state_history = play_game\n",
    "  for (s, s') in state_history from end to start:\n",
    "    V(s) = V(s) + learning_rate * (V(s') - V(s))\n",
    "```\n",
    "\n",
    "### 3.2.7 Detail 2 \n",
    "The next detail we need to think about is how do we actually play the game. Are we just going to take random actions? No! First, think about what taken random actions would lead to; it would result in a game tree having different probabilities than a game tree that was created by taking best actions. Second, remember that we have the value function! The value function tells us how good a state is, based on how good the future states will be. So, all we need to do is perform the action that leads to the next best state. \n",
    "\n",
    "In pseudocode that may look like:\n",
    "\n",
    "```\n",
    "maxV = 0\n",
    "maxA = None\n",
    "for a, s' in possible_next_states:\n",
    "  if V(s') > maxV:\n",
    "    maxV = V(s')  // max value \n",
    "    maxA = a      // max action that lead to max value\n",
    "perform action maxA\n",
    "```\n",
    "\n",
    "### 3.2.8 Problem with this\n",
    "What is the main problem with this strategy? The problem is that the value function isn't accurate. If had the true value function, we wouldn't need to do any of this work in the first place. This problem is one that we have touched on before; the explore-exploit dilemma. In particular, by taking random actions we can learn about new states that we otherwise may never have gone to. By doing so, we can improve our estimate of the value function for those states. \n",
    "\n",
    "However, in order to win, we need to take the action that yields the maximum value. So, we will be using the epsilon-greedy strategy to be making a tradeoff between exploration and exploitation. \n",
    "\n",
    "### 3.2.9 Intuition\n",
    "Let's take a moment to gain some intuition about why this iterative update works. First, you should recognize this iterative update equation from before. It is reminiscent of the low-pass filter and average-value-finding equation we saw earlier, as well as gradient descent. V(s') is like the target value, and we want to move $V(s)$ towards that value. However, with RL there are multiple possible next states, so there are multiple s'. So, by doing this update equation multiple times, we are pulling V(s) in multiple different directions all at once. The idea is that, by playing infinitely many episodes, the proportion of time we spend in s' will approach the true next state probabilities.\n",
    "\n",
    "### 3.2.10 Extremely Important Detail\n",
    "One incredibly important detail that is somewhat difficult to discern given only the update formula, is the order in which you have to update the values for any given state. We know that for any particular episode, we are only going to be updating the values for the states that were in that episode. But, the key of this equation is that we are moving $V(s)$ closer to $V(s')$. That mean that we are doing this update under the assumption that $V(s')$ is more accurate than $V(s)$. For the terminal state this is true, since that is always going to be 0 or 1, and will never change. But, if $V(s)$ and $V(s')$ are of the same accuracy, aka they are both not accurate at all, then making one closer to the other won't make anything better. So, the idea is we want to **move backwards** through the state history, because the current state value is always updated using the next state value. And we want the next state value to be more accurate, so we should update the state values in reverse order. This is also consistent with the terminal state being precisely accurate with a value of 0 or 1 that never changes. \n",
    "\n",
    "#### $$V(terminal) = 0 \\; or \\; 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Tic-Tac-Toe Implementation \n",
    "Typically in ML we're implementing *procedural* algorithms. However, in RL we have multiple objects interacting, and we take an OOP approach. \n",
    "\n",
    "At a high level, we will have two main objects: the **agent** and the **environment**. During any game, there will be two instances of the **agent**, and they will both interact with the same instance of environment. \n",
    "\n",
    "<img src=\"images/tic-tac-toe.png\" width=\"500\">\n",
    "\n",
    "In order to make these things interact, we can create a function called `play_game()`, which accepts the agent for player 1, the agent for player 2, and the environment object. Inside of this function, it will essentially be a single loop. We will alternate between the two players, and at each iteration of the loop we need to do a few things. \n",
    "\n",
    "1. We need to make the current player perform an action which updates the environment. \n",
    "2. We need to switch who the current player is, so that it alternates. \n",
    "3. We need to check if the game is over, since that is when the loop will terminate. \n",
    "\n",
    "We can write this function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(p1, p2, env, draw=False):\n",
    "  # Loop until the game is over\n",
    "  current_player = None\n",
    "  while not env.game_over():\n",
    "    # Alternate between players\n",
    "    current_player = p2 if current_player == p1 else p1\n",
    "    \n",
    "    # Draw the board before the user who wants to see it makes a move\n",
    "    condition1 = draw == 1 and current_player == p1\n",
    "    condition2 = draw == 2 and current_player == p2\n",
    "    if draw and (condition1 or condition2):\n",
    "      env.draw_board()\n",
    "      \n",
    "    # Current player makes move\n",
    "    current_player.take_action(env)\n",
    "    \n",
    "    # Update State histories\n",
    "    state = env.get_state()\n",
    "    p1.update_state_histories(state)\n",
    "    p2.update_state_histories(state)\n",
    "    \n",
    "    if draw:\n",
    "      env.draw_board()\n",
    "      \n",
    "    # Do the value function update\n",
    "    p1.update(env)\n",
    "    p2.update(env)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that above we have created a partial API for some instance methods that our objects will need. \n",
    "\n",
    "* The environment will need a `game_over()` function, that returns a boolean. True if the game is over, false otherwise. You can guess that this function will need to scan the board to check if there is a winner, or if the board is full and it is a draw. \n",
    "* The agent will need a take action function, which accepts as a parameter the environment. This will update the environment, and hence the state. \n",
    "* Another function we need on the agent is an update function, that updates the agents internal estimate of the value function. This is where the update equation that we have talked about is going to go. The update function will have to accept the environment as well, since it will have to query the most current reward from the environment. \n",
    "* Another function that will be very useful for us, but not entirely necessary, is a draw board function. This will display what positions are currently occupied, and by what pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
