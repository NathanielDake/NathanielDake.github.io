{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Markov Decision Processes\n",
    "We are now going to formalize some of the concepts that we have learned about in reinforcement learning. We have learning about the terms:\n",
    "> * **Agent**\n",
    "* **Environment**\n",
    "* **Action**\n",
    "* **State**\n",
    "* **Reward**\n",
    "* **Episode**\n",
    "\n",
    "This section is about putting these concepts into a formal framework called **Markov Decision Processes**.\n",
    "\n",
    "## 1.1 Gridworld\n",
    "In this section we are going to describe the game that we are going to use for the rest of this course. It is in some ways simpler than tic-tac-toe, but it has some properties that allow us to explore some of the more interesting properties of RL.\n",
    "\n",
    "<img src=\"images/gridworld.png\">\n",
    "\n",
    "In this game our agent is a robot, and the environment is a grid. The agent is allowed to move in 4 directions: up, down, left, and right. Grid world is generally built in the following way: \n",
    "> * at position (1, 1) there is a wall, so if the robot tries to go there it will bump into the wall. \n",
    "* (0,3) is a winning state (terminal state with a +1 reward)\n",
    "* (1, 3) is a losing state (terminal state with a -1 reward)\n",
    "\n",
    "One thing we will notice about gridworld is that it has a much smaller number of states than tic-tac-toe; there are only 12 positions, 11 states (where the robot is), and 4 actions-that is a small game! However, there are many concepts to be learned! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 2. The Markov Property\n",
    "Let us first review the **Markov Property** in the strict mathematical sense. Suppose we have a sequence:\n",
    "\n",
    "#### $$\\{x_1, x_2,...,x_t\\}$$\n",
    "\n",
    "We can define a conditional probability on $x_t$, given all the previous $x$'s:\n",
    "\n",
    "#### $$p\\{x_t \\mid x_{t-1}, x_{t-2}, ..., x_1\\}$$\n",
    "\n",
    "Generally speaking, this can't be simplified. However, if we assume that the markov property is true, than it can be simplified. The markov property specifies how many previous $x$'s the current $x$ depends on. So, **First-Order Markov** means that $x_t$ depends only on $x_{t-1}$:\n",
    "\n",
    "#### $$p\\{x_t \\mid x_{t-1}, x_{t-2}, ..., x_1\\} = p \\{x_t \\mid x_{t-1} \\}$$\n",
    "\n",
    "A **Second Order Markov** means that $x_t$ only depends on $x_{t-1}$ and $x_{t-2}$:\n",
    "\n",
    "#### $$p\\{x_t \\mid x_{t-1}, x_{t-2}, ..., x_1\\} = p \\{x_t \\mid x_{t-1}, x_{t-2} \\}$$\n",
    "\n",
    "For now we will be working with the first order markove only, and we typically refer to this as *the markov property*. \n",
    "\n",
    "## 2.1 Simple Example\n",
    "Consider the sentence: **\"Let's do a simple example\"**. Let's say that you are given:\n",
    "\n",
    "> \"Let's do a simple\"\n",
    "\n",
    "In this case it is relatively easy to guess that the next word is \"example\". Now, all we are given is:\n",
    "\n",
    "> \"simple\"\n",
    "\n",
    "It is not longer easy to predict the next word. This can be even hard! For instance, what if we were just given:\n",
    "\n",
    "> \"a\"\n",
    "\n",
    "Now it is _very_ difficult to predic the next word, \"simple\". Well, that is what the markov assumption is; we can clearly see that it can be quite limiting. However, we can also define the problem so that it is not. \n",
    "\n",
    "## 2.2 Markov Property in RL\n",
    "So, what exactly does the markov property look like in RL? Recall, that taking an action $A(t)$ while in state $S(t)$ produces two things: the next state $S(t+1)$ and a reward $R(t+1)$:\n",
    "\n",
    "#### $$\\{S(t), A(t) \\} \\rightarrow \\{ S(t+1), R(t+1)\\}$$\n",
    "\n",
    "What the markov property in this case says, is that $S(t+1)$ and $R(t+1)$ depend only on $A(t)$ and $S(t)$, but not any $A$ or $S$ before that:\n",
    "\n",
    "#### $$p\\big(S_{t+1}, R_{t+1} \\mid S_t, A_t, S_{t-1}, A_{t-1},...,S_0, A_0\\big) = p \\big( S_{t+1}, R_{t+1} \\mid S_t, A_t\\big) $$\n",
    "\n",
    "For convenience, we can also use the shorthand symbols we have mentioned earlier: $s, a,  r, s'$:\n",
    "\n",
    "#### $$p(s', r \\mid s, a) = p(S_{t+1} = s', R_{t+1} = r \\mid S_t = s, A_t = a)$$ \n",
    "\n",
    "So, how is the different from the normal way that we usually write the markov property? Well, notice that this is a _joint distribution_ on $s'$ and $r$. So, it is telling us the joint distribution of two variables, conditioned on two other variables. This is different from the usual markov form, where we have one variable on the left, and one variable on the right. \n",
    "\n",
    "## 2.3 Other Conditional Distributions\n",
    "Given the above joint conditional distribution, it is of course just a matter of using the rules of probability to find the marginal and conditional distributions. For example, if we just want to know $s'$ given $s$ and $a$ we can use:\n",
    "\n",
    "#### $$p(s' \\mid s, a) = \\sum_{r \\in R}p(s', r \\mid s, a )$$\n",
    "\n",
    "And if we just wanted to know $r$ given $s$ and $a$:\n",
    "\n",
    "#### $$p(r \\mid s,a) = \\sum_{s' \\in S} p(s', r \\mid s, a)$$\n",
    "\n",
    "Also, note that for essentially all cases that we will consider, these probabilities will be deterministic. That means that the reward you get for going to the state will always be the same reward, and taking an action in a state will always take you to the same next state. \n",
    "\n",
    "## 2.4 Is the Markov Assumption Limiting?\n",
    "Let's look at a recent application of RL to demonstrate that the markov assumption is not necessarily limiting. DeepMind used the concatenation of the 4 most recent frames in order to represent the current state when playing Atari games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Markov Decision Processes (MDPs)\n",
    "We have essentially been looking at MDPs this entire time, but just not referring to them by name. Any RL task with a set of states, actions, and rewards, that follows the markov property is a markov decision process. \n",
    "\n",
    "Formally speaking, the MDP is a 5 tuple, made up of:\n",
    "\n",
    "> * **Set of states**\n",
    "* **Set of actions**\n",
    "* **Set of rewards**\n",
    "* **State-transition probabilities, reward probabilities (as defined jointly earlier**\n",
    "* **Discount factor**\n",
    "\n",
    "## 3.1 Policy\n",
    "There is one final piece needed to complete our puzzle. The other key term in markov decission process is **decision**. The way that we make decisions, and chose what actions to take in what states, is called a policy. We generally denote the policy with the symbole $\\pi$. Technically, $\\pi$ is not part of the MDP itself, but it is part of the solution, along with the value function. \n",
    "\n",
    "The reason we are just talking about the policy now is because it is somewhat of a weird symbol. We write down $\\pi$ as a mathematical symbol, but there is no equation for $\\pi$. For example, if $\\pi$ is epsilon-greedy, how do we write that as an equation? It is more like an algorithm. The only exception to this is when you want to write down the **optimal policy**, which can be defined mathematically, in terms of the *value function*; we will discuss this later. So, for now we can just think of $\\pi$ as the shorthand notation for the algorithm that the agent is using to navigate the environment. \n",
    "\n",
    "## 3.2 State-transition Probability\n",
    "Let's look at the state transition probability again:\n",
    "\n",
    "#### $$p(s' \\mid s, a)$$\n",
    "\n",
    "Recall that we said that this is typically deterministic, but that is not always the case. Why might that be so? Recall, that the state is only what is derived from what the agent senses from the environment; it is not the environment itself. The state can be an imperfect representation of the environment, in which case you would expect the state transition to be probabilistic. For example, the state you measure could represent multiple configurations of the environment. As an example of an imperfect representation of the environment, think about blackjack; you may think of the dealers next card as part of the state. But, as the agent, you can't see the next card so it is not part of your state. It _is_ part of the environment. \n",
    "\n",
    "## 3.3 Actions\n",
    "When we think of actions, we typically think of joystick inputs (up/down/left/right/jump) or blackjack moves (hit/stand). However, actions can be very broad as well, such as how to distribute government funding. So, RL can be applied to making political decisions as well. \n",
    "\n",
    "## 3.4 Agent vs Environment\n",
    "Sometimes there is a bit of confusion surrounding what constitutes the agent vs. the environment. You are navigating your environment, but what constitutes you? Are you your body? Your body is, more correctly, part of the environment! Your body isn't making decisions or learning; your body has sensors which pass on signals to your brain, but it is your brain and mind that make all decisions and do all learning! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Future Rewards\n",
    "## 4.1 Total Reward\n",
    "We are now going to formalize the idea of **total future reward**. This refers to everything from $t+1$ and onward. We call this the **return**, $G(t)$:\n",
    "\n",
    "#### $$G(t) = \\sum_{\\tau = 1}^\\infty R(t + \\tau)$$\n",
    "\n",
    "Notice how it does not depend on the current reward, $R(t)$. This is because when we arrive a state, we receive the reward for that state-there is nothing to predict about it, because it has already happened. \n",
    "\n",
    "Now, think of a very long task, potentially containing thousands of steps. Your goal is to maximize your total reward. However, is there a difference between getting a reward now, and getting that same reward 10 years from now? Think about finance; we know that \\$1000 today is worth less than $1000 10 years ago. Would you rather get \\$1000 now, or 10 year from now? Choose today! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Discount Factor\n",
    "This causes us to introduce a discount factor on future rewards. We call the discount factor $\\gamma$, and we use a number between 0 and 1 to represent it:\n",
    "\n",
    "#### $$G(t) = \\sum_{\\tau = 0}^ \\infty \\gamma ^{\\tau} R(t + \\tau, 1)$$\n",
    "\n",
    "A $\\gamma = 1$ means that we don't care how far in the future a reward is, all rewards should be weighted equally. A $\\gamma = 0$ means that we don't care about the future rewards at all, and is a truly greedy algorithm since the agent would only try to maximize its immediate reward. Usually we choose something close to 1, such as 0.9. If we have a very short episodic task, it may not be worth discounting at all. An intuitive reason for discounting future rewards is that the further you look into the future, the harder it is to predict. Hence, there is not a lot of sense getting something 10 years from now, unless you are sure you can make it happen, and that your circumstances won't change. \n",
    "\n",
    "## 4.3 Merging Continuous and Episodic Tasks\n",
    "You may notice that the sum for the return goes from 0 to $\\infty$; this implies that we are looking at a continuous task, when in reality the tasks we have looked at so far (tic-tac-toe) are episodic. This is a mathematical subtlety, but we actually want to write all of our equations in continuous form; simply put, it makes the math a little easier to work with. \n",
    "\n",
    "There is a way to merge episodic and continuous tasks so that they are equivalent. The way you do it is this: The episodic task has a terminal state. Pretend that there is a state transition from the terminal state to itself, that always happens with probability of 1, and always yields a reward of 0. In this way, the episodic task remains the same, but since it goes on forward, it is technically a continuous task. \n",
    "\n",
    "<img src=\"images\\continuous-episodic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
