{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Dynamic Programming and Iterative Policy Evaluation\n",
    "We are now going to start looking at solutions to MDP's. As we saw in the last section, the center piece of the discussion is the **Bellman Equation**:\n",
    "\n",
    "#### <span style=\"color:#0000cc\">$$\\text{Bellman Equation} \\rightarrow V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$</span>\n",
    "\n",
    "In fact, the bellman equation can be used directly to solve for the value function. If you look carefully, you will see that this is actually a set of $S$ equations with $S$ unknowns. In fact, it is a linear equation, meaning it is not too difficult to solve. In addtion, a lot of the matrix entries will be zero, since the state transitions will most likely be sparse. \n",
    "\n",
    "However, this is _not_ the approach we will take. Instead, we will do what is called **iterative policy evaluation**. \n",
    "\n",
    "## 1.1 Iterative Policy Evaluation\n",
    "What exactly is iterative policy evaluation? Well, essentially it means that we apply the bellman equation again and again, and eventually it will just converge. We can write down the algorithm in pseudocode as follows:\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{def iterative_policy_evaluation}(\\pi)\\text{:} \\\\\n",
    "\\hspace{1cm} \\text{initialize V(s) = 0 for all s} \\in \\text{S} \\\\\n",
    "\\hspace{1cm} \\text{while True:} \\\\\n",
    "\\hspace{2cm} \\Delta = 0 \\\\\n",
    "\\hspace{2cm} \\text{for each s} \\in \\text{S:} \\\\\n",
    "\\hspace{3cm} \\text{old_v = V(s)} \\\\\n",
    "\\hspace{3cm} V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}\\\\\n",
    "\\hspace{3cm} \\Delta = \\text{max(} \\Delta \\text{, |V(s) - old_v|)} \\\\\n",
    "\\hspace{2cm} \\text{if} \\Delta \\text{< threshold: break} \\\\\n",
    "\\hspace{1cm} \\text{return V(s)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "We can see above that the input to iterative policy evaluation is a policy, $\\pi$, and the output is the value function for that particular policy. It works as follows:\n",
    "> * We start by initializing $V(s)$ to 0 for all states. \n",
    "* Then, in an infinite loop, we initialize a variable called $\\Delta$, which represents the maximum change during the current iteration. $\\Delta$ is used to determine when to quit. When it is small enough, that is when we will break out of the loop. \n",
    "* Then, for every state in the state space, we keep a copy of the old $V(s)$, and then we use bellmans equation to update V(s). \n",
    "* We set $\\Delta$ to be the maximum change for $V(s)$ in that iteration. \n",
    "* Once this converges, we return $V(s)$\n",
    "\n",
    "The main point of interest in this algorithm, is of course the part that contains the bellman equation. Notice how strictly speaking, the value at iteration $k+1$ depend only on the values at iteration $k$:\n",
    "\n",
    "#### $$ V_{k+1}(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_{k}(s') \\Big \\}$$\n",
    "\n",
    "However, this need not be the case. In fact, we can always just use our most up to date versions of the value function for any state. This actually ends up converging faster. \n",
    "\n",
    "## 1.2 Definitions\n",
    "A final note; we generally call the act of finding the value function for a given policy the _**prediction problem**_. Soon, we will learn an algorithm for finding the optimal policy, which is known as the _**control problem**_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Gridworld in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Environment\"\"\"\n",
    "class Grid: \n",
    "  def __init__(self, width, height, start):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0] # start is a tuple of 2 integers\n",
    "    self.j = start[1]\n",
    "    \n",
    "  def set(self, rewards, actions):\n",
    "    \"\"\"actions enumerate all possible actions that can take you to new state.\n",
    "       actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "       rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    \"\"\"\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "    \n",
    "  def set_state(self, s):\n",
    "    \"\"\"Useful for various algorithms we will use. For example, iterative policy evaluation\n",
    "    requires looping through all the states, and then doing an action to get to the next\n",
    "    state. In order to know what the next state is, we have to put the agent into that \n",
    "    state, do the action, and then determine the next state. We do not automatically have\n",
    "    a master list of state transitions, we must figure them out by playing the game.\"\"\"\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "    \n",
    "  def current_state(self, s):\n",
    "    \"Returns current (i,j) position of agent.\"\n",
    "    return (self.i, self.j)\n",
    "  \n",
    "  def is_terminal(self, s):\n",
    "    \"\"\"Returns true if s is terminal state, false if not. Easy way to check this is to see\n",
    "    if the state is in the action dictionary. If you can do an action from the state, then\n",
    "    you can transition to a different state, and hence your state is not terminal.\"\"\"\n",
    "    return s not in self.actions\n",
    "  \n",
    "  def move(self, action):\n",
    "    \"\"\"Checks if action is in actions dictionary. If not, we are not able to do this move,\n",
    "    so we simply stay in same position.\"\"\"\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      if action == 'U':\n",
    "        self.i -= 1\n",
    "      elif action == 'D':\n",
    "        self.i += 1\n",
    "      elif action == 'R':\n",
    "        self.j += 1\n",
    "      elif action == 'L':\n",
    "        self.j -= 1\n",
    "    # Return reward (if any, default is 0)\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "  \n",
    "  def undo_move(self, action):\n",
    "    \"\"\"Pass in the action you just took, and the environment will undo it. \n",
    "    Ex -> Just went up, it will move you back down.\"\"\"\n",
    "    if action == 'U':\n",
    "      self.i += 1\n",
    "    elif action == 'D':\n",
    "      self.i -= 1\n",
    "    elif action == 'R':\n",
    "      self.j -= 1\n",
    "    elif action == 'L':\n",
    "      self.j += 1\n",
    "    # Raise an exception if we arrive somewhere we shouldn't be -> Should never happen\n",
    "    assert(self.current_state() in self.all_states())\n",
    "    \n",
    "  def game_over(self):\n",
    "    \"Returns true if game over, false otherwise. Only need to check if in terminal state.\"\n",
    "    return (self.i, self.j) not in self.actions\n",
    "  \n",
    "  def all_states(self):\n",
    "    \"\"\"We can calculate all of the states simply by enumerating all of the states from \n",
    "    which we can take an action (which don't include terminal states), and all of the \n",
    "    states that return a reward (which do include terminal states). Since there may be\n",
    "    some of the same states in both actions and rewards, we cast it to a set. This also\n",
    "    makes the data structure O(1) for search.\"\"\"\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "  \n",
    "def standard_grid():\n",
    "  \"\"\"Returns standard grid. This is a grid that has the structure shown in section 4.\n",
    "  All of the actions are defined such that we can move within the grid, but never off\n",
    "  of it. We also cannot walk into the wall, nor out of the terminal state. Upper left\n",
    "  is defined to be (0,0). We define rewards for arriving at each state. The grid looks\n",
    "  like:\n",
    "      .  .  .  1\n",
    "      .  x  . -1\n",
    "      s  .  .  .\n",
    "  * x means you can't go there\n",
    "  * s means start position\n",
    "  * number means reward at that state\n",
    "  \"\"\"\n",
    "  g = Grid(3, 4, (2, 0))\n",
    "  rewards = {(0, 3): 1, (1, 3): -1}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "  }\n",
    "  g.set(rewards, actions)\n",
    "  return g\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "  \"\"\"Here we want to penalize each move. This is done to prevent a robot from moving \n",
    "  randomly to solve the maze. If you only gave it a reward for solving the maze, then it\n",
    "  would never learn anything beyond a random strategy. We know that we can incentivize\n",
    "  the robot to solve the maze more efficiently by giving it a negative reward for each\n",
    "  step taken. That is what we are doing here-incentivizing the robot to solve the maze \n",
    "  efficiently, rather than moving randomly until it reaches the goal.\"\"\"\n",
    "  g = standard_grid()\n",
    "  g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "  })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
