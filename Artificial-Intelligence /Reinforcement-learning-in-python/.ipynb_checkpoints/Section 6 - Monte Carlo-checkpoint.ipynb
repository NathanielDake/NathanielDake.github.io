{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Monte Carlo Intro\n",
    "In this section we are going to be discussing another technique for solving MDP's, known as **Monte Carlo**. In the last section, you may have noticed something a bit odd; we have talked about how RL is all about learning from experience and playing games. Yet, in none of our dynamic programming algorithms did we actually play the game. We had a full model of the environment, which included all of the state transition probabilities. You may wonder: is it reasonable to assume that we would have that type of information in a real life environment? For board games, perhaps. But, what about self driving cars? \n",
    "\n",
    "The way that we manipulated our dynamic programming algorithms required us to put an agent into a state. That may not always be possible, especially when talking about self driving cars, or even video games. A video game starts in the state that it decides-you can't choose any state you want. This is another instance of having god mode capabilities, so it is not always realistic to assume that that is always possible. In this section, we will be playing the game and learning purely from experience. \n",
    "\n",
    "## 1.2 Monte Carlo Methods\n",
    "Monte Carlo is a rather poorly defined term. Usually, it refers to any algorithm that involves a significantly random component. With Monte Carlo Methods in RL, the random component is the _**return**_. Recall that what we are always looking for is the expected return given that you are in state $s$. With MC, instead of calculating the true expected value of G (which requires probability distributions), we instead calculate its sample mean. \n",
    "\n",
    "In order for this to work, we need to assume that we are doing episodic tasks only. The reason is because an episode has to terminate before we can calculate any of the returns. This also means that MC methods are _not_ fully online algorithms. We don't do an update after every action, but rather after every episode. \n",
    "\n",
    "The methods that we use in the Monte Carlo section should be somewhat reminiscent of the multi armed bandit problem. With the multi armed bandit problem, we were always averaging the reward after every action. With MDP's we are always averaging the return. One way to think of Monte Carlo, is that _every state_ is a _separate multi-armed bandit problem_. What we are trying to do is learn to behave optimally for all of the multi armed bandit problems, all at once. In this section, we are again going to follow the same pattern that we did in the DP section. We will start by looking at the _**prediction problem**_ (_find the value given the policy_), and then look at the _**control problem**_ (_finding the optimal policy_). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.0 Monte Carlo Policy Evaluation\n",
    "We are now going to solve the prediction problem using Monte Carlo Estimation. Recall that the definition of the value function is that it is the expected value of the future return, given that the current state is $s$:\n",
    "\n",
    "#### $$V_\\pi(s) = E \\big[G(t) \\mid S_t = s\\big]$$\n",
    "\n",
    "We know that we can estimate any expected value simply by adding up samples and dividing by the total number of samples:\n",
    "\n",
    "#### $$\\bar{V}_\\pi(s) = \\frac{1}{N} \\sum_{i =1}^N G_{i,s}$$\n",
    "\n",
    "Where above, $i$ is indexing the episode, and $s$ is indexing the state. The question now, is how do we get these sample returns. \n",
    "\n",
    "## 2.1 How do we generate $G$?\n",
    "In order to get these sample returns, we need to play many episodes to generate them! For every episode that we play, we will have a sequence of states and rewards. And from the rewards, we can calculate the returns by definition, which is just the sum of all future rewards:\n",
    "\n",
    "#### $$G(t) = r(t+1) + \\gamma * G(t+1)$$\n",
    "\n",
    "Notice how, to actually implement this in code, it would be very useful to loop through the states in reverse order, since $G$ depends only on future values. Once we have done this for many episodes, we will have multiple lists of $s$'s and $G$'s. We can then take the sample mean. \n",
    "\n",
    "## 2.2 Mutliple Visits to $s$\n",
    "One interesting question that comes up is, what if you see the same state more than once in an episode? For instance if you see state $s$ at $t=1$ and $t=3$? What is the return for state $s$? Should we use $G(1)$ or $G(3)$? There are two answers to this question, and surprisingly they both lead to the same answer. \n",
    "\n",
    "**First Visit Monte Carlo**<br>\n",
    "The first method is called _first visit monte carlo_. That means that you would only count the return for time $t=1$. \n",
    "\n",
    "**Every Visit Monte Carlo**<br>\n",
    "The second method is called _every visit monte carlo_. That means that you would calculate the return for every time you visited the state $s$, and all of them would contribute to the sample mean; i.e. use both $t=1$ and $t=3$ as samples. \n",
    "\n",
    "Surprisingly, it has been proven that both lead to the same answer. \n",
    "\n",
    "## 2.3 First-Visit MC Pseudocode\n",
    "Let's now look at some pseudocode for first visit monte carlo prediction. \n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "def first_visit_monte_carlo_prediction(pi, N):\n",
    "  V = random initialization\n",
    "  all returns = {} # default = []\n",
    "  do N times:\n",
    "    states, returns = play_episode\n",
    "    for s, g in zip(states, returns):\n",
    "      if not seen s in this episode yet:\n",
    "        all_returns[s].append(g)\n",
    "        V(s) = sample_mean(all_returns[s])\n",
    "  return V\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "In the above pseudocode we can see the following:\n",
    "> * The input is a policy, and the number of samples we want to generate\n",
    "* We initialize $V$ randomly, and we create a dictionary to store our returns, with a default value being an empty list\n",
    "* We loop N times. Inside the loop we generate an episode by playing the game. \n",
    "* Next, we loop through the state sequence and return sequence. We only include the return if this the first time we have seen this state in this episode since this is first visit MC.\n",
    "* If so, we add this return to our list of returns for this state. \n",
    "* Next, we update V(s) to be the sample mean of all the returns we have collected for this state. \n",
    "* At the end, we return $V$. \n",
    "\n",
    "## 2.4 Sample Mean\n",
    "One thing that you may have noticed for the pseudocode, is that it requires us to store all of the returns that we get for each state so that the sample mean can be calculated. But, if you recall from our section on the multi armed bandit, there are more efficient ways to calculate the mean, such as calculating it from the previous mean. There are also techniques for nonstationary problems, like using a moving average. So, all of the techniques we have learned already still apply here. \n",
    "\n",
    "Another thing that we should notice about the MCM, is that because we are calculating the sample mean, all of the same rules of probability apply. That means that the confidence interval is approximately Gaussian, and the variance if the original variance of the data, divided by the number of samples collected:\n",
    "\n",
    "#### $$\\text{Variance of Estimate} = \\frac{\\text{variance of RV}}{N}$$\n",
    "\n",
    "Therefore, we are going to more confident in data that has more samples, but it grows slowly with respect to the number of samples. \n",
    "\n",
    "## 2.5 Calculating Returns from Rewards\n",
    "For full clarity, we will also quickly go over how to calculate the returns from the rewards in pseudocode. \n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "# Calculating State and Reward Sequences \n",
    "s = grid.current_state()\n",
    "states_and_rewards = [(s, 0)]\n",
    "while not game_over:\n",
    "  a = policy(s)\n",
    "  r = grid.move(a)\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards.append((s, r))\n",
    "  \n",
    "# Calculating the Returns\n",
    "G = 0 \n",
    "states_and_returns = []\n",
    "for s, r in reverse(states_and_rewards):\n",
    "  states_and_returns.append((s, G))\n",
    "  G = r + gamma*G\n",
    "states_and_returns.reverse()\n",
    "```\n",
    "\n",
    "---\n",
    "The above pseudocode shows two main steps:\n",
    "1. Calculating State and Reward Sequences. This is just playing the game, and keeping a log of all the states and rewards that we get, in the order we get them. Notice, this is a list of tuples. Also, first award is assumed to be 0. We do not get any reward simply for arriving at the start state. \n",
    "2. Calculating the Returns. We start with empty list, and then loop through the states and rewards in reverse order. In the first order of this loop, the state s represents the terminal state and G will be 0. Next we update G. Notice how, on the first iteration of the loop, this includes the reward for the terminal state. Once the loop is done, we reverse the list of states and returns, since we want it to be in the order that we visited the states. \n",
    "\n",
    "## 2.6 Note on MC\n",
    "One final thing to note about MC. Recall that one of the disadvantages of DP is that we have to loop through the entire set of states on every iteration, and that this is bad for most practical scenarios in which there are a large number of states. Notice how MC only updates the value for states that we actually visit. That means even if the state space is large, if we only ever visit a small subset of states, then it doesn't matter. \n",
    "\n",
    "Also notice, we don't even need to know what the states are! We can simply discover them by playing the game. So, there are some advantages to MC in situations where doing full exact calculations is infeasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3.0 Monte Carlo Policy Evaluation in Code\n",
    "We are now going to implement Monte Carlo for finding the State-Value function in code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import standard_grid, negative_grid, print_policy, print_values\n",
    "\n",
    "SMALL_ENOUGH = 10e-4\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: This is only policy evaluation, NOT optimization\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  \"\"\"Returns a list of states and corresponding returns\"\"\"\n",
    "  \n",
    "  # Reset game to start at a random position. We need to do this, because given our \n",
    "  # current deterministic policy (we take upper left path all the way to goal state, and \n",
    "  # for any state not in that path, to go all the way to losing state. Since MC only \n",
    "  # calculates values for states that are actually visited, and if we only started at the\n",
    "  # prescribed start state, there will be some states that we never visit. So we need this \n",
    "  # little hack at the beginning of play game, that allows us to start the game at any \n",
    "  # state. This is called the exploring starts method. \n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "  \n",
    "  # Play the game -> goal is to make a list of all states we have visited, and all rewards\n",
    "  # we have received\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s] \n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "    \n",
    "  # Calculate the returns by working backward from the terminal state. We visit each state\n",
    "  # in reverse, and recursively calculate the return. \n",
    "  G = 0 \n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards): \n",
    "    # The value of the terminal state is 0 by definition. We should ignore the first state \n",
    "    # we encounter, and ignore the last G, which is meaningless since it doesn't \n",
    "    # correspond to any move. \n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA * G\n",
    "  states_and_returns.reverse() # we want it in the order of state visited\n",
    "  return states_and_returns\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "  \n",
    "  # Initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't get to otherwise\n",
    "      V[s] = 0 \n",
    "      \n",
    "  # ------ Monte Carlo Loop ------\n",
    "  # Plays the games and gets the states and returns list. \n",
    "  for t in range(100):\n",
    "    # Generate an episode using pi. Create set to keep track of all the states that we \n",
    "    # have seen, since we only want to add a return if it is the first time we have seen\n",
    "    # the state in this episode. \n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    \n",
    "    # Loop through all states and returns.\n",
    "    for s, G in states_and_returns:\n",
    "      # Check if we have already seen s, called \"first-visit: MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s]) # Recalculate V(s) because we have new sample return\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4.0 Policy Evaluation in Windy Grid World\n",
    "We are now going to use MC predicton algorithm to find $V$, but this time we will be in windy gridworld and will be using a slightly different policy. One thing that you may have noticed with the last script is that MC wasn't really needed since the returns were deterministic. This was because the two probability distributions that we are interested in- $\\pi(a \\mid s)$ and $p(s,a \\mid s',r)$- were both _deterministic_.\n",
    "\n",
    "In windy gridworld the state transitions are not deterministic, so we will have a source of randomness, and hence a need for MC. Also, the policy will be different. In particular, this policy is always going to try and win; in other words, travel to the goal state. We will see that even though this policy is to go to the goal state, not all values will end up positive, since in windy gridworld the wind can still end up pushing you into the losing state. So on average, the return for that state is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.43| 0.55| 0.72| 0.00|\n",
      "---------------------------\n",
      " 0.33| 0.00| 0.21| 0.00|\n",
      "---------------------------\n",
      " 0.26| 0.19| 0.12|-0.14|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import standard_grid, negative_grid, print_policy, print_values\n",
    "\n",
    "SMALL_ENOUGH = 10e-4\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def random_action(a):\n",
    "  # 0.5 probability of performing chosen action\n",
    "  # 0.5/3 probability of doing some action a' != a\n",
    "  p = np.random.random()\n",
    "  if p < 0.5:\n",
    "    return a\n",
    "  else:\n",
    "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "    tmp.remove(a)\n",
    "    return np.random.choice(tmp)\n",
    "  \n",
    "def play_game(grid, policy):\n",
    "  \"\"\"Returns a list of states and corresponding returns\"\"\"\n",
    "  \n",
    "  # Reset game to start at a random position. We need to do this, because given our \n",
    "  # current deterministic policy (we take upper left path all the way to goal state, and \n",
    "  # for any state not in that path, to go all the way to losing state. Since MC only \n",
    "  # calculates values for states that are actually visited, and if we only started at the\n",
    "  # prescribed start state, there will be some states that we never visit. So we need this \n",
    "  # little hack at the beginning of play game, that allows us to start the game at any \n",
    "  # state. This is called the exploring starts method. \n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "  \n",
    "  # Play the game -> goal is to make a list of all states we have visited, and all rewards\n",
    "  # we have received\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s] \n",
    "    a = random_action(a) # ----- THIS IS THE UPDATE FOR WINDY GRIDWORLD -----\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "    \n",
    "  # Calculate the returns by working backward from the terminal state. We visit each state\n",
    "  # in reverse, and recursively calculate the return. \n",
    "  G = 0 \n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards): \n",
    "    # The value of the terminal state is 0 by definition. We should ignore the first state \n",
    "    # we encounter, and ignore the last G, which is meaningless since it doesn't \n",
    "    # correspond to any move. \n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA * G\n",
    "  states_and_returns.reverse() # we want it in the order of state visited\n",
    "  return states_and_returns\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "  \n",
    "  # state -> action\n",
    "  # found by policy_iteration_random on standard_grid\n",
    "  # MC method won't get exactly this, but should be close\n",
    "  # values:\n",
    "  # ---------------------------\n",
    "  #  0.43|  0.56|  0.72|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.33|  0.00|  0.21|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.25|  0.18|  0.11| -0.17|\n",
    "  # policy:\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   L  |   U  |   L  |\n",
    "  # ----- This policy has changed from the previous example! -----\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "  \n",
    "  # Everything else from here down is the same. The Monte Carlo algorithm doesn't change\n",
    "  # becasue the fact that we are taking averages already takes into account any \n",
    "  # randomness both in the policy and in the state transitions. \n",
    "  \n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "      \n",
    "  # repeat until convergence\n",
    "  for t in range(5000):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5.0 Monte Carlo Control\n",
    "If you recall, the pattern that we are generally trying to follow is:\n",
    "\n",
    "> First look at how to solve the _prediction problem_, which finding the value function given a policy. We then want to look at how to solve the _control problem_, which is how to find the optimal policy. \n",
    "\n",
    "We are now going to go about finding the optimal policy using MC. \n",
    "\n",
    "## 5.1 MC for Control Problem\n",
    "When we first try and do this, you will see that we come across a big problem. That is, we only have $V(s)$ for a given policy, but we don't know what actions will lead to a better $V(s)$ because we can't do a look ahead search. With MC, all we are able to do is play an episode straight through and use the returns as samples. The key to using MC for the control problem then, is to find $Q(s,a)$, since $Q$ is indexed by both $s$ and $a$, and we can choose the argmax over $a$ to find the best policy:\n",
    "\n",
    "#### $$argmax_a\\big( Q(s,a)\\big)$$\n",
    "\n",
    "## 5.2 MC for $Q(s,a)$\n",
    "So, how exactly do we use MC to find $Q$? The process is nearly the same as we used to find $V$, except that instead of just returning tuples of states and returns, (s, G), we will return triples of states, actions, and returns, (s, a, G). \n",
    "\n",
    "We can quickly see how this may be problematic. Recall that the number of values we need to store grows quadratically with the state set size and action set size. With $V(s)$ we only need $\\mid S \\mid$ different estimates. With $Q(s,a)$ we need $\\mid S \\mid * \\mid A \\mid$ different estimates. That means that we have a lot more values to approximate, and we need to do many more steps of MC in order to get a good answer. \n",
    "\n",
    "There is another problem with trying to estimate $Q$, which goes back to our explore/exploit dilemma. If we have a fixed policy, then every $Q(s,a)$ will only have samples for _one_ action. That means, out of the total $\\mid S \\mid * \\mid A \\mid$ values we need to fill in, we will only be able to fill in $\\frac{1}{\\mid A \\mid}$ of those values. \n",
    "\n",
    "The way to fix this, as we discussed earlier, is using the exploring starts method. In this case, we now randomly choose an initial state and an initial action. Thereafter we follow the policy. So, the answer that we get is $Q_\\pi(s,a)$. This is consistent with our defintion of $Q$:\n",
    "\n",
    "#### $$Q_\\pi(s,a) = E_\\pi \\big[G(t) \\mid S_t = s, A_t=a\\big]$$\n",
    "\n",
    "## 5.3 Back to Control Problem\n",
    "Let's now return to the control problem. If you think carefully, you'll realize that we already know the answer to this problem. We discussed earlier how the method that we always used to find an optimal policy is generalized policy iteration, where we alternate between policy evaluation and policy improvement. For policy evaulation, we just discussed it-we will simply use MC to get an estimate for $Q$. The policy improvement step is the same as always, we just take the argmax over the actions from $Q$:\n",
    "\n",
    "#### $$\\pi(s) = argmax_a Q(s,a)$$\n",
    "\n",
    "## 5.4 Problem with MC\n",
    "One issue with this as we have already discussed, is that because we need to find $Q$ over all states and all actions, this can take a very long time using sampling. We again have a problem where there is an iterative algorithm inside an iterative algorithm. \n",
    "\n",
    "The solution to this is to take the same approach that we do with value iteration. Instead of doing a fresh MC policy evaluation on each round, where it would take a long time to collect samples, we instead update the same $Q$ and do policy improvement after every single episode. This means that on every iteration of the outer loop we generate only one episode, use that episode to improve our estimate of $Q$, and then immediately do the policy improvement step. \n",
    "\n",
    "## 5.5 Pseudocode\n",
    "We can now look at this in pseudocode to solidify this idea:\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "Q = random, pi = random\n",
    "\n",
    "while True: \n",
    "  s, a = randomly select from S and A     # Exploring starts method\n",
    "  \n",
    "  # Generate an episode from this starting position, following current policy\n",
    "  # Receive a list of triply containing states, actions, and returns\n",
    "  states_actions_returns = play_game(start=(s,a))\n",
    "  \n",
    "  # Policy Evaluation Step\n",
    "  for s,a,G in states_actions_returns:\n",
    "    returns(s,a).append(G)\n",
    "    \n",
    "    # Recalculate Q as average of all returns for this state action pair\n",
    "    Q(s,a) = average(returns(s,a)) \n",
    "    \n",
    "  # Policy improvement step\n",
    "  for s in states:\n",
    "    pi(s) = argmax[a] { Q(s,a) }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5.6 One more problem\n",
    "There is one more subtle problem with this algorithm; Recall that if we take an action that results in us bumping into a wall, we end up in the same state as before. So, if we follow this policy we will never end up in a terminal state and therefore never finish the episode. To avoid this problem, we can introduce a hack: if we end up in the same state after doing an action, this will give us a reward of -100 and end the episode. \n",
    "\n",
    "## 5.7 MC \n",
    "It is interesting that this method converges, even though the returns that we average for $Q$ are for different policies. Intuitively, $Q$ has to converge, since if it is suboptimal, then the policy will change, and that will in turn cause $Q$ to change. We can only achieve stability when both the value and policy to converge to the optimal value and optimal policy. Interestingly, this have never been formally proven. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Monte Carlo Control in Code\n",
    "We are now going to use Monte Carlo exploring starts to solve the control problem, aka find the optimal policy and the optimal value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90| 1.00|\n",
      "---------------------------\n",
      "-0.90| 0.00|-0.90|-1.00|\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90|-0.90|\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ec24f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "final values:\n",
      "---------------------------\n",
      "-0.90|-0.08| 1.00| 0.00|\n",
      "---------------------------\n",
      "-1.89| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      "-3.07|-3.11|-2.34|-1.00|\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # Reset game to start at a random position. We need to do this, because given our \n",
    "  # current deterministic policy (we take upper left path all the way to goal state, and \n",
    "  # for any state not in that path, to go all the way to losing state. Since MC only \n",
    "  # calculates values for states that are actually visited, and if we only started at the\n",
    "  # prescribed start state, there will be some states that we never visit. So we need this \n",
    "  # little hack at the beginning of play game, that allows us to start the game at any \n",
    "  # state. This is called the exploring starts method. \n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "  \n",
    "  s = grid.current_state()\n",
    "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "  \n",
    "  \n",
    "  # Be aware of the timing. Each triple is s(t), a(t), r(t) but r(t) results \n",
    "  # from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  seen_states = set()\n",
    "  while True:\n",
    "    old_s = grid.current_state()\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    \n",
    "    if s in seen_states:\n",
    "      # Hack so that we don't end up in infinitely long episode, bumping into wall\n",
    "      states_actions_rewards.append((s, None, -100))\n",
    "      break\n",
    "    elif grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else: \n",
    "      a = policy[s]\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "    seen_states.add(s)\n",
    "    \n",
    "  # calculate the returns by working backwards from the terminal state, \n",
    "  # NOW ADDING IN ACTIONS\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "\n",
    "  # Function to do max and argmax from a dictionary (what we are using to store Q)\n",
    "  def max_dict(d):\n",
    "    # returns the argmax (key) and max (value) from a dictionary\n",
    "    # put this into a function since we are using it so often\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "      if v > max_val:\n",
    "        max_val = v\n",
    "        max_key = k\n",
    "    return max_key, max_val\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  # Try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.9)\n",
    "  \n",
    "  # Print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "  \n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "    \n",
    "  # Main Loop - repeat until convergence\n",
    "  deltas = [] # For debugging purposes \n",
    "  for t in range(2000):\n",
    "    if t % 100 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # Generate an episode using pi. Play a game, get states, actions, and returns triples\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_state_action_pairs = set() # Create set to store state-actions pairs we have seen\n",
    "    \n",
    "    # Loop through all state action pairs in episode, update their returns list, and \n",
    "    # update Q\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # ---- Policy Improvement Step ---- update policy \n",
    "    for s in policy.keys():\n",
    "      policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # find V\n",
    "  V = {}\n",
    "  for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
