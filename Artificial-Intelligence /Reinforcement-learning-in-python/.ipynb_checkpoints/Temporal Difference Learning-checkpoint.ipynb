{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Temporal Difference Learning Introduction\n",
    "We are now going to look at a third method for solving MDPs, _**Temporal Difference Learning**_. TD is one of the most important ideas in RL, and we will see how it combines ideas from the first two techniques, Dynamic Programming and Monte Carlo. \n",
    "\n",
    "Recall that one of the disadvantages of DP was that it requires a full model of environment, and never learns from experience. On the other hand, we saw that MC does learn from experience, and we will shortly see that TD learns from experience as well. \n",
    "\n",
    "With the Monte Carlo method, we saw that we could only update the value function after completing an episode. On the other hand, DP uses bootstrapping and was able to improve its estimates based on existing estimates. We will see that TD learning also uses bootstrapping, and furthermore is fully online, so we don't need to wait for an episode to finish before we start updating our value estimates. \n",
    "\n",
    "In this section we will take our standard approach:\n",
    "> 1. First we will look at the prediction problem, aka finding the value function given a policy.\n",
    "2. Second we will look at the control problem. We will look at 2 different ways of approaching the control problem: **SARSA** and **Q-Learning**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Prediction Problem - `TD(0)`\n",
    "We are now going to look at how to apply TD to the prediction problem, aka finding the value function. The reason that there is a 0 in the name is because there are other TD learning methods such as `TD(1)` and `TD(`${\\lambda}$`)`, but they are outside the scope of this course. They are similar, but not necessary to understand _Q-learning_ and _Approximation methods_, which is what we eventually want to get to.\n",
    "\n",
    "## 2.1 Monte Carlo Disadvantage\n",
    "One big disadvantage of Monte Carlo was that we needed to wait until the episode is finished before we can calculate the returns, since the return depends on all future rewards. Also, recall that the MC method is to average the returns, and that earlier in the course we looked at different ways of calculating averages. \n",
    "\n",
    "## 2.2 `TD(0)`\n",
    "In particular, we can look at the method that does not require us to store all of the returns: _the moving average_. Recall that $\\alpha$ can be constant or decay with time. So, if we use this formula it would be an alternative way of caculating the average reward for a state. \n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\big[G(t) - V(S_t)\\big]$$\n",
    "\n",
    "Now recall the definition of $V$; it is the expected value of the return, given a state:\n",
    "\n",
    "$$V(s) = E \\big[G(t) \\mid S_t = s\\big]$$\n",
    "\n",
    "But, remember that we can also define it recursively:\n",
    "\n",
    "$$V(s) = E \\big[R(t+1) + \\lambda V(S_{t+1}) \\mid S_t =s\\big]$$\n",
    "\n",
    "So, it is reasonable to ask if we can just replace the return in the update equation with this recursive definition of $V$! What we get from this, is the `TD(0)` method:\n",
    "\n",
    "$$V(S_t) = V(S_t) + \\alpha \\big[R(t+1) + \\lambda V(S_{t+1}) - V(S_t)\\big]$$\n",
    "\n",
    "<span style=\"color:#0000cc\">$$\\text{TD(0)} \\rightarrow V(s) = V(s) + \\alpha \\big[r + \\lambda V(s') - V(s)\\big]$$</span>\n",
    "\n",
    "We can also see how this is fully online! We are not calculating $G$, the full return. Instead, we are just using another $V$ estimate, in particular the $V$ for the next state. What this also tells us is that we cannot update $V(s)$ until we know $V(s')$. So, rather than waiting for the entire episode to finish, we just need to wait until we reach the next state to update the value for the current state. \n",
    "\n",
    "## 2.3 Sources of Randomness\n",
    "It is helpful to examine how these estimates work, and what the sources of randomness are. \n",
    "\n",
    "> * With MC, the randomness came from the fact that each episode could play out in a different way. So, the return for a state would be different if all the later state transitions had some randomness. \n",
    "* With **`TD(0)`** we have yet another source of randomness. In particular, we don't even know the return, so instead we use $r + \\gamma V(s')$ to estimate the return $G$. \n",
    "\n",
    "## 2.4 Summary\n",
    "We just looked at why `TD(0)` is advantageous in comparison to MC/DP. \n",
    "\n",
    "> * Unlike DP, we do not require a full model of the environment, we learn from experience, and only update V for states we visit.\n",
    "* Unlike MC, we don't need to wait for an episode to finish before we can start learning. This is advantageous in situations where we have very long episodes. We can improve our performance during the episode itself, rather than having to wait until the next episode.\n",
    "* It can even be used for continuous tasks, in which there are no episodes at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
