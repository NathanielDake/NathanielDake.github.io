{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf4e15a-ebae-46c7-b073-7a440c693002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d20a37-94ec-4461-86ab-c5bc371b59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafc3cde-57b5-42c0-9496-ddab74d03579",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99              # discount rate for computing temporal difference target\n",
    "BATCH_SIZE = 32           # How many transitions to sample from replay buffer when computing gradients\n",
    "BUFFER_SIZE = 50000       # Max number of transitions to store before overwriting old transitions\n",
    "MIN_REPLAY_SIZE = 1000    # How many transitions we want in replay buffer before starting to compute gradients, training\n",
    "EPSILON_START = 1.0       # Start epsilon\n",
    "EPSILON_END = 0.02        # End epsilon\n",
    "EPSILON_DECAY = 10000     # Decay period under which epsilon will linearly anneal from start to end \n",
    "TARGET_UPDATE_FREQ = 1000 # Number of steps where we set target parameters equal to online parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60be552d-054b-4afd-97e3-5ef25ed8a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0') # Cartpole allows quick iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f27759-b8d0-46d5-898d-ac2345926fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        \n",
    "        # How many neurons are in input layer of neural network\n",
    "        # 4 in the case of cartpole (prod is not necessary here, since 1 dim)\n",
    "        in_features = int(np.prod(env.observation_space.shape))  \n",
    "        \n",
    "        # 2 layer sequential linear network with 64 hidden units, separated by tanh nonlinearity\n",
    "        # Outputs in network are equal to number of actions agents can take \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n) # Q-learning can only be used with action spaces that have finite number of actions\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Given an input observation (our state) predict q values for that given state\n",
    "        return self.net(x)\n",
    "        \n",
    "    def act(self, obs):\n",
    "        # Select an action intelligently. Predict the q value of your current state, then pick the action \n",
    "        # i.e. the index that maximizes the value of the current state\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.forward(obs_t.unsqueeze(0)) # create fake batch dimension        \n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        \n",
    "        return action\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ffef88-d4f1-430e-8383-173e08231a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=BUFFER_SIZE)  \n",
    "\n",
    "rew_buffer = deque([0, 0], maxlen=100)  # reward buffer, stores rewards earned by agent during episode (track improvment of agent)\n",
    "\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345a139f-9cb5-4174-a60d-74154d88e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())  # Set target parameters to onine net parameters\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e05901b5-c2b2-4b4f-80c6-2fd67901486b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Observation**: [0.01056365 0.02887569 0.00452288 0.02949128]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Action**: 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reward**: 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Done** (is episode over): False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Info**: [0.03143533 0.21146269 0.03802817 0.01330967]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize replay buffer\n",
    "obs = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    new_obs, rew, done, info = env.step(action)\n",
    "    transition = (obs, action, rew, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "transition_labels = ['**Observation**: ', '**Action**: ', '**Reward**: ', '**Done** (is episode over): ', '**Info**: ']\n",
    "_ = [printmd(x + str(y)) for (x, y) in zip(transition_labels, transition)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b4e2d0-2b07-4c67-9afe-69376944388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Avg reward:  0.0\n",
      "Step:  1000\n",
      "Avg reward:  21.085106382978722\n",
      "Step:  2000\n",
      "Avg reward:  22.348314606741575\n",
      "Step:  3000\n",
      "Avg reward:  22.78\n",
      "Step:  4000\n",
      "Avg reward:  19.34\n",
      "Step:  5000\n",
      "Avg reward:  15.42\n",
      "Step:  6000\n",
      "Avg reward:  13.03\n",
      "Step:  7000\n",
      "Avg reward:  11.79\n",
      "Step:  8000\n",
      "Avg reward:  11.24\n",
      "Step:  9000\n",
      "Avg reward:  10.65\n",
      "Step:  10000\n",
      "Avg reward:  9.7\n",
      "Step:  11000\n",
      "Avg reward:  9.41\n",
      "Step:  12000\n",
      "Avg reward:  9.47\n",
      "Step:  13000\n",
      "Avg reward:  9.47\n",
      "Step:  14000\n",
      "Avg reward:  9.44\n",
      "Step:  15000\n",
      "Avg reward:  9.47\n",
      "Step:  16000\n",
      "Avg reward:  9.46\n",
      "Step:  17000\n",
      "Avg reward:  9.42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b8ba632733a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Gradient Descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/data-science/intuitiveml/venv/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/data-science/intuitiveml/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/development/data-science/intuitiveml/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "obs = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    \n",
    "    # First we must select an action to take in the environment. We are using epsilon greedy\n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "    \n",
    "    rnd_sample = random.random()\n",
    "    \n",
    "    if rnd_sample <= epsilon:   # Explore\n",
    "        action = env.action_space.sample() \n",
    "    else:                       # Exploit - Intelligently select action using network\n",
    "        action = online_net.act(obs)\n",
    "    \n",
    "    new_obs, rew, done, info = env.step(action)\n",
    "    transition = (obs, action, rew, done, new_obs) # Transition tuple\n",
    "    replay_buffer.append(transition)\n",
    "    \n",
    "    episode_reward += rew\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        \n",
    "        rew_buffer.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "    \n",
    "    # Start Gradient Step\n",
    "    \n",
    "    # Sample batch size of transitions. Break them out of individual tuples into lists. Turn into tensors\n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rews = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1) # unsqueeze to put batch dimension on tensors\n",
    "    rews_t = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1)     # we use -1 here to get dimension added at end\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1) \n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets for loss function \n",
    "    \n",
    "    # For each observation in our tensor, get a prediction of the q value. So each observation will be mapped to a \n",
    "    # 2 dimensional vector, the value of being in that state (new_obs) and taking action 0 and taking action 1\n",
    "    # For each observation we get a set of q values\n",
    "    # Note: The entire point of Q is to learn the value/quality of being in a given state and taking a certain action\n",
    "    # So if this has been learned correctly, we can easily know what action to take\n",
    "    # Note: this is the q value at the next observation!\n",
    "    target_q_values = target_net(new_obses_t) \n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0] # Must collapse down to the highest q value per observation\n",
    "    \n",
    "    targets = rews_t + GAMMA * (1 - dones_t) * max_target_q_values # Piece wise function in one step\n",
    "    \n",
    "    # Compute Loss and Gradients\n",
    "    # The online net calculates the q value at the current observation\n",
    "    q_values = online_net(obses_t)\n",
    "    # Get the q value for the actual action that we took in the transition\n",
    "    # This yields the effectively predicted q value of the action we took at the original time of the transition\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  # Compute gradient\n",
    "    optimizer.step() # Apply gradients\n",
    "    \n",
    "    # Update target network (at target update frequency steps)\n",
    "    if step % TARGET_UPDATE_FREQ:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        print('Step: ', step)\n",
    "        print('Avg reward: ', np.mean(rew_buffer))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Questions to answer:\n",
    "    # 1) When does the target net get updated?\n",
    "    # 2) How does target actually help us? How does it allow us to bootstrap forward?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f55d7f-6ba3-4786-9152-ef35cbedeaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce196f7e-867a-42ad-81bc-714904fe30ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1949, -0.0645], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_net.forward(torch.as_tensor(obs, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b8d901c-6b21-453e-8fa6-3f360de1d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_t = torch.as_tensor(obs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41121db8-7630-4df3-8f8d-24b052b13d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0539, -0.1741], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_net.forward(obs_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "844d38a9-2c70-45d9-af33-6a6738f1a653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0364, -0.0066,  0.0246, -0.0306])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "370ecc4b-8531-466c-a830-7ff931626832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0364, -0.0066,  0.0246, -0.0306]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_t.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1168ba77-88ee-4559-9098-f920f3264497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-81-81b701f5a0fa>\u001b[0m(25)\u001b[0;36mact\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     23 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 25 \u001b[0;31m        \u001b[0mmax_q_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_q_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0539, -0.1741]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-81-81b701f5a0fa>\u001b[0m(26)\u001b[0;36mact\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0mmax_q_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m        \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_q_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  max_q_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-81-81b701f5a0fa>\u001b[0m(28)\u001b[0;36mact\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     26 \u001b[0;31m        \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_q_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-13f15e9ffff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monline_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-81-81b701f5a0fa>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_q_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-81b701f5a0fa>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_q_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "online_net.act(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c1d69-8098-4b66-ba26-ffe59bb70807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
