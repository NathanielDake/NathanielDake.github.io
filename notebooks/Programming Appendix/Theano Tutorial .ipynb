{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Two Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import function\n",
    "import theano "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Theano, all symbols must be _typed_. In particular, below we will use `T.dscalar` as the type for a \"0-dimensional array (scalar) of type double\". This is a Theano `type`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dscalar('x')\n",
    "y = T.dscalar('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, `dscalar` is _not_ a class, so `x` and `y` are not instances of `dscalar`. They are instances of `TensorVariable`. `x` and `y`, however, are assigned the theano Type `dscalar` in their `type` field, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.tensor.var.TensorVariable"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorType(float64, scalar)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorType(float64, scalar)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.dscalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type is T.dscalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `T.dscalar` with a string argument, you create a _Variable_ representing a floating-point scalar quantity with the given name. If you provide no argument, the symbol will be unamed. Names are not required, but will often help with debugging. \n",
    "\n",
    "Next, we can combine `x` and `y` into their sum, `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`z` is yet another variable which represents the addition of `x` and `y`. You can use the `pp` function to pretty-print out the computation associated to `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x + y)\n"
     ]
    }
   ],
   "source": [
    "from theano import pp\n",
    "print(pp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a function taking `x` and `y` as inputs, and giving `z` as output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = function([x, y], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there is a slight delay when executing the `function` instruction. This because behind the scenes, `f` was being compiled into C code.\n",
    "\n",
    "The first argument to `function` is a list of Variables that will be provided as inputs to the function. The second argument is a single Variable or a list of Variables. For either case, the second argument is what we want to see as output when we apply the function. `f` may then be used like a normal Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Two Matrices\n",
    "This next step simply requires that we instantiate `x` and `y` using the matrix Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = x + y\n",
    "f = function([x, y] , z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are able to pass in either python lists or numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11., 22.],\n",
       "       [33., 44.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([[1,2],[3,4]], [[10,20],[30,40]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11., 22.],\n",
       "       [33., 44.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(np.array([[1,2], [3,4]]), np.array([[10,20], [30,40]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Variables\n",
    "It is also possible to make a function with an internal state. For example, let's say we want to make an accumulator: at the beginning, the state is initialized to zero. Then, on each function call, the state is incremented by the function's argument. \n",
    "\n",
    "First, let's define the `accumulator` function. It adds its argument to the internal state, and returns the old state value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import shared\n",
    "state = shared(0)\n",
    "inc = T.iscalar('inc')\n",
    "accumulator = function([inc], state, updates=[(state, state+inc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code introduces a few new concepts. The `shared` function constructs so-called _shared variables_. These are hybrid symbolic and non-symbolic variables whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions just like the objects returned by `dmatrices(...)` but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. It is called a _shared_ variable because its value is shared between many functions. The value can be accessed and modified by the `.get_value()` and `.set_value()` methods. \n",
    "\n",
    "The other new thing in this code is the `updates` parameter of `function`. `updates` must be supplied with a list of pairs of the form: `(shared-variable, new expression)`. It can also be a dictionary whose keys are shared-variables and values are the new expressions. Either way, it means “whenever this function runs, it will replace the `.value` of each shared variable with the result of the corresponding expression”. Above, our accumulator replaces the state‘s value with the sum of the state and the increment amount.\n",
    "\n",
    "We can now try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulator(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulator(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reset the state by using the `.set_value()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state.set_value(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, you can define more than one function to use the same shared variable. These functions can all update the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decrementor = function([inc], state, updates=[(state, state-inc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrementor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(state.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering why the updates mechanism exists. You can always achieve a similar result by returning the new expressions, and working with them in NumPy as usual. The updates mechanism can be a syntactic convenience, but it is mainly there for efficiency. Updates to shared variables can sometimes be done more quickly using in-place algorithms (e.g. low-rank matrix updates). Also, Theano has more control over where and how shared variables are allocated, which is one of the important elements of getting good performance on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Structures\n",
    "The first step in writing Theano code is to write down all mathematical relations using symbolic placeholders (variables). When writing down these expressions you use operations like `+`, `-`, `**`, `sum()`, `tanh()`. All these are represented internally as **ops**. An op represents a certain computation on some type of inputs producing some type of output. You can see it as a _function definition_ in most programming languages.\n",
    "\n",
    "Theano represents symbolic mathematical computations as graphs. These graphs are composed of interconnected _Apply_, _Variable_ and _Op_ nodes. _Apply_ node represents the application of an _op_ to some _variables_. It is important to draw the difference between the definition of a computation represented by an op and its application to some actual data which is represented by the apply node. Furthermore, data types are represented by Type instances. Here is a piece of code and a diagram showing the structure built by that piece of code. This should help you understand how these pieces fit together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=19KR8tIZk0EVPkyeV1o5FgDRm5r6am6VK\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrows represent references to the Python objects pointed at. The blue box is an Apply node. Red boxes are Variable nodes. Green circles are Ops. Purple boxes are Types.\n",
    "\n",
    "When we create _Variables_ and then _Apply Ops_ to them to make more Variables, we build a bi-partite, directed, acyclic graph. Variables point to the Apply nodes representing the function application producing them via their `owner` field. These Apply nodes point in turn to their input and output Variables via their `inputs` and `outputs` fields. (Apply instances also contain a list of references to their `outputs`, but those pointers don’t count in this graph.)\n",
    "\n",
    "The `owner` field of both `x` and `y` point to `None` because they are not the result of another computation. If one of them was the result of another computation, it’s `owner` field would point to another blue box like `z` does, and so on.\n",
    "\n",
    "Note that the `Apply` instance’s outputs points to `z`, and `z.owner` points back to the `Apply` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traversing the graph\n",
    "The graph can be traversed starting from outputs (the result of some computation) down to its inputs using the owner field. Take for example the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = theano.tensor.dmatrix('x')\n",
    "y = x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enter `type(y.owner)` you get `<class 'theano.gof.graph.Apply'>`, which is the apply node that connects the op and the inputs to get this output. You can now print the name of the op that is applied to get y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.gof.graph.Apply"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y.owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elemwise{mul,no_inplace}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.op.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, an elementwise multiplication is used to compute y. This multiplication is done between the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y.owner.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InplaceDimShuffle{x,x}.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the second input is not 2 as we would have expected. This is because 2 was first broadcasted to a matrix of same shape as x. This is done by using the op DimShuffle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.tensor.var.TensorVariable"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y.owner.inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.gof.graph.Apply"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y.owner.inputs[1].owner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<theano.tensor.elemwise.DimShuffle at 0x10f9752b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[1].owner.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorConstant{2}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.owner.inputs[1].owner.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative in Theano\n",
    "Now let’s use Theano for a slightly more sophisticated task: create a function which computes the derivative of some expression `y` with respect to its parameter `x`. To do this we will use the macro `T.grad`. For instance, we can compute the gradient of $x^2$ with respect to $x$:\n",
    "\n",
    "$$\\frac{d(x^2)}{dx} = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(8.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x)\n",
    "display(pp(gy)) # Pretty print gradient prior to optimization\n",
    "\n",
    "f = theano.function([x], gy)\n",
    "f(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(TensorConstant{2.0} * x)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(f.maker.fgraph.outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model: \n",
      "[ 1.54379689e+00  1.66714736e+00  9.37672181e-01 -2.61255422e-02\n",
      "  9.94943018e-01  2.98970392e-01  1.72044152e+00  1.54661462e-01\n",
      "  4.78673393e-01 -6.84652399e-02 -1.02263216e+00  8.20644211e-01\n",
      " -1.54131767e+00  3.15173490e-01 -5.22912056e-02  1.98288786e+00\n",
      "  6.29161221e-01 -4.60054710e-01 -4.06288355e-01  2.41440577e-01\n",
      " -8.33355575e-01 -2.23144351e+00  7.65137608e-02 -6.14199999e-01\n",
      " -5.09320426e-02 -2.00915828e+00 -6.71276486e-02 -3.36337364e-03\n",
      " -2.44088596e-01  1.86395845e-01 -1.19035445e+00  7.70154479e-01\n",
      "  4.79713791e-01  1.73952917e+00  1.20507064e+00 -9.15916881e-02\n",
      " -2.69536917e-01  1.98228827e+00 -5.26812764e-01 -9.72237140e-02\n",
      "  1.35997071e+00  5.90587699e-01 -2.99488123e-02 -3.82591629e-02\n",
      "  1.56182862e+00 -6.39931355e-01  5.53747099e-01  8.53853746e-01\n",
      "  6.25335866e-02  1.40593055e+00 -5.21166420e-02 -1.80091443e+00\n",
      " -1.80745771e+00 -1.07335911e+00 -1.67091989e+00 -8.76487706e-01\n",
      "  2.49786628e-01 -2.97215260e-01 -1.67604692e-01  3.02142096e-02\n",
      " -1.73993216e-01 -7.81524501e-01 -7.56563180e-01 -4.35348198e-01\n",
      "  3.91358794e-01 -1.09089234e+00  1.50048136e+00  1.75708554e+00\n",
      "  2.49507193e+00 -2.38721142e-01  4.86414795e-02 -5.19073314e-01\n",
      "  1.06136343e+00  2.43259074e-01 -1.37391527e-01 -8.86081299e-01\n",
      " -1.89458996e-01  3.47393862e-01  2.35428575e+00  5.46449957e-01\n",
      " -5.41796588e-02 -6.48305981e-01  1.41557696e+00 -4.62869380e-01\n",
      "  7.06049393e-01  1.25313032e+00  2.06381988e-01  1.12779985e+00\n",
      " -8.41061903e-01 -4.67952304e-01 -2.21389784e-01 -1.33462074e-01\n",
      "  1.45787012e+00  7.34201790e-01  1.08973886e-01 -3.10315768e-01\n",
      "  7.80087416e-01  5.40241441e-01  1.06981671e-01  2.94387413e-01\n",
      "  3.31666479e-01 -8.53715653e-01  7.46000172e-01 -2.47144460e+00\n",
      "  2.05100456e+00 -9.02522266e-01 -1.14896516e+00 -9.19365952e-01\n",
      "  4.12147768e-01 -8.71511707e-01  1.65776774e+00 -1.40171429e+00\n",
      "  7.00219496e-01 -1.64432037e+00  6.74393828e-01 -1.44243100e-01\n",
      " -8.61316609e-01 -1.62682435e+00  6.07467147e-01  1.31270230e-01\n",
      " -8.18609822e-01  1.06215687e+00 -1.45107535e+00  5.35497511e-01\n",
      "  1.06505738e+00  6.67005898e-01 -1.67913410e-01 -5.57882907e-01\n",
      " -4.17239164e-01  1.88855836e+00 -1.72296367e+00 -1.16244063e-01\n",
      "  1.25985629e+00 -1.31478185e+00 -3.34629323e-01  1.59464510e+00\n",
      " -5.02121788e-01  2.27250080e+00  1.00478464e-01 -2.56799224e-01\n",
      "  7.03266322e-01  9.81585235e-01 -4.64168248e-01  4.83322645e-03\n",
      "  5.13189049e-02 -9.97255584e-01 -1.49956909e+00  4.94318489e-01\n",
      "  6.06484003e-01  1.34092124e+00 -1.44856451e+00  7.67317033e-01\n",
      "  4.18148507e-01  1.03609670e-01 -1.43206681e+00 -9.42411081e-01\n",
      " -4.99734603e-01 -1.26049486e-01 -1.02711328e+00 -2.02848688e-01\n",
      "  1.95006542e+00  2.60473600e-01  4.19611729e-01  9.78710224e-01\n",
      " -1.03053727e+00 -5.45739992e-01 -6.81169419e-01  6.92905744e-02\n",
      " -4.44640930e-01  7.37361645e-01  2.32773516e-01  9.26093947e-01\n",
      "  1.11664616e+00 -1.55469928e+00 -3.80473331e-01  1.68609949e+00\n",
      "  2.54685924e-01 -7.66998234e-01  3.68449141e-01  5.29800443e-01\n",
      "  1.31078085e-01  7.73055770e-01  8.27306510e-01 -1.15797369e+00\n",
      " -8.80223309e-01  8.83058061e-01  1.05227344e+00  6.21395833e-01\n",
      " -3.98997270e-01 -3.09244931e-01  1.35860703e-01 -2.03031909e-01\n",
      " -7.26725216e-01  4.03285243e-02  1.77844155e-01 -1.22413239e+00\n",
      " -5.08448281e-01 -5.95031436e-02  4.90361562e-01 -6.83190473e-03\n",
      "  1.66253769e+00 -1.23631076e+00  5.14013861e-01  1.31680436e+00\n",
      " -1.28062180e+00  5.90218134e-01  1.10953190e-01  1.11263581e-01\n",
      "  3.01213383e-01 -2.27430869e+00  1.05278562e+00  2.33989486e-01\n",
      "  1.16015775e-01 -2.68738575e-01  1.10626825e+00 -2.69890165e+00\n",
      "  1.14581706e-01 -5.75795500e-01 -6.97484819e-01  2.56899618e-01\n",
      "  2.63680044e+00  6.45920477e-01  1.20234486e+00 -2.32463976e-01\n",
      "  2.64240995e-01  1.50293183e-01  9.22723956e-01  1.13441827e+00\n",
      "  8.76446813e-01 -6.53501329e-01 -4.42152697e-01 -6.20237902e-01\n",
      "  1.72673218e+00 -4.48134438e-01 -6.09181516e-01 -9.66214249e-01\n",
      " -1.44542788e+00 -8.55045869e-01 -2.22465168e-01 -1.35714933e+00\n",
      " -8.81080152e-01  2.30954184e-01  6.65832704e-01  1.69720048e+00\n",
      "  1.78810059e-01  1.23147953e+00  4.34057714e-01  2.35519366e+00\n",
      " -3.30942326e-02 -9.18399853e-03 -4.36065510e-02  1.37567582e-01\n",
      "  7.04697028e-01 -4.65445949e-01 -3.86639331e-01  1.20533889e-01\n",
      "  6.30446331e-01 -6.12074491e-01 -4.26936802e-01  2.10944414e-01\n",
      "  8.18853403e-01 -1.06109300e+00 -1.24895065e+00  5.66032739e-01\n",
      " -1.19769852e+00  5.14249122e-02 -7.27854483e-01  1.46661737e+00\n",
      " -7.78287428e-01  3.36147357e-01 -7.92122786e-01  8.78413682e-02\n",
      "  1.01642311e+00 -1.00802932e+00  7.34765473e-01  4.82887932e-01\n",
      "  7.20448877e-01 -1.73356883e-01  4.46243208e-01  6.44061849e-01\n",
      "  2.67995453e+00  5.94664583e-01  1.19456477e+00  1.88084236e+00\n",
      " -2.23851130e-01 -4.54385322e-01  1.92626200e+00  1.19359010e+00\n",
      " -1.36320635e-01  1.83470195e-01  6.50834077e-01  5.16121731e-01\n",
      " -1.10303592e+00  1.65476454e-01  5.38496233e-01  2.62728313e+00\n",
      "  1.48775613e-01  9.78771749e-02  1.02978504e+00 -3.26815066e+00\n",
      " -6.93401370e-01 -7.03207180e-01 -1.57399303e-01  4.83501903e-01\n",
      "  1.42558518e-01  6.11840699e-01 -1.36208228e+00  2.77167309e+00\n",
      " -1.29602097e+00  6.03649013e-01  2.36604084e-01  7.95977204e-02\n",
      "  8.98668409e-01  2.32509438e+00 -1.84295017e+00 -8.41538134e-01\n",
      " -4.33921534e-01  5.35444367e-02 -9.68292398e-01  1.64162362e+00\n",
      " -4.75406034e-01 -4.31377752e-02  1.36690074e-01 -1.78535796e+00\n",
      " -3.25656102e-02 -9.33010438e-01  4.25684738e-01 -5.53732939e-01\n",
      "  1.27641747e+00 -1.13841697e+00  1.29794977e-01  5.37776954e-01\n",
      " -2.40876657e-01 -5.95754839e-01  2.42958533e-01 -1.54707663e+00\n",
      " -1.56275640e+00 -2.05643565e+00 -5.32012120e-01 -1.59544201e+00\n",
      " -8.14909603e-01  1.55466995e+00  5.06940908e-01 -1.53153171e+00\n",
      " -1.08127430e+00 -5.89589849e-01 -4.97891995e-01 -6.01563395e-01\n",
      "  2.07707671e-01 -7.15958168e-01  1.38313058e+00  1.29325351e+00\n",
      "  1.11449331e-01  2.07211657e-01  6.17162019e-01  1.93003187e-01\n",
      "  4.02426402e-01 -1.32925239e+00 -2.96783188e+00  6.48824788e-01\n",
      "  8.62896173e-01  1.03987630e+00  2.98197143e-02  1.88834627e+00\n",
      "  1.07856629e+00  1.17184351e+00 -9.10240950e-01  8.39438040e-01\n",
      "  6.65830684e-01  2.29588726e-01 -1.12225100e+00 -7.88515828e-01\n",
      "  7.00101375e-01  1.86479657e-01 -1.85475631e+00 -4.71992286e-01\n",
      " -2.47980174e-02  1.99120301e-01 -6.82772647e-01 -3.09158681e-01\n",
      "  2.22702082e+00 -7.09532017e-02  5.84621493e-01 -9.72221997e-01\n",
      " -4.25458340e-01 -1.14106272e+00 -4.98947452e-01 -4.89406332e-01\n",
      "  3.98751160e-02 -3.39447079e-01 -3.72882816e-01  6.68936533e-01\n",
      " -8.88201431e-01 -2.92867146e-01 -7.45052860e-01 -5.68312708e-01\n",
      "  2.32759443e-01 -8.75097482e-02 -2.19705440e-02  3.64309489e-01\n",
      "  1.12486489e+00  1.06133023e+00 -9.03980367e-01 -1.15023134e+00\n",
      " -1.04462419e+00 -2.05705450e-02 -1.33306278e+00  6.28758581e-01\n",
      " -4.73299642e-01 -7.73745966e-01  7.10858095e-01 -6.79613114e-01\n",
      " -8.58101773e-01  1.54528705e+00 -7.12249000e-01  1.17375595e-02\n",
      " -1.89170283e-01  5.41027713e-01  6.92738105e-01 -2.26748159e-01\n",
      " -4.01525027e-01 -5.21373205e-01  1.73723067e-01 -6.42878320e-01\n",
      " -2.00503140e+00 -7.28268332e-01 -1.35240522e+00  2.58117327e+00\n",
      "  6.62401139e-01 -1.89222069e+00 -8.12414581e-01 -2.17010338e-01\n",
      "  6.18304898e-01 -1.00623479e+00 -6.91917407e-01  3.48774975e-01\n",
      "  8.15714007e-01  3.16444330e-01  3.36770311e-01  9.50581561e-01\n",
      " -1.30071286e+00  5.80162118e-01  1.36622603e+00 -1.60734663e+00\n",
      "  1.19560843e+00  5.85085864e-01 -4.99245293e-01  4.54971888e-01\n",
      " -3.95826824e-02  1.60576289e+00 -6.97773493e-01 -1.19050959e+00\n",
      " -4.25075013e-01  1.66291183e-01 -7.61130357e-02  1.01263317e+00\n",
      "  4.84133716e-01 -8.76795628e-01  2.91000705e+00  2.11059508e+00\n",
      " -5.28881138e-02  1.28976965e+00  1.88332242e-01 -5.20591850e-01\n",
      "  2.18053734e+00 -4.05804370e-01 -2.73033856e-01  1.13109238e+00\n",
      " -8.11784426e-01 -3.36913998e-01  5.00518823e-02  1.42355486e+00\n",
      " -1.71260465e-01 -9.88632811e-01 -4.35012669e-01 -1.33259088e+00\n",
      "  2.01068292e-01 -6.32726931e-01 -1.39759461e+00 -6.95497032e-01\n",
      " -8.01847596e-02 -1.41838098e+00 -2.07515872e-01 -7.86839675e-01\n",
      " -1.41630975e+00 -2.20208841e-01  8.36220503e-01  1.41503625e+00\n",
      " -9.33648636e-01  5.72592327e-01 -7.76301361e-02 -2.41213753e+00\n",
      "  1.49322520e-01  6.76067067e-02 -1.08796626e+00 -1.01975019e+00\n",
      " -1.71397252e+00 -1.62243247e-01  2.11116807e+00  1.62890501e+00\n",
      "  1.33324760e-01 -7.10737613e-01  1.98694462e-01  1.96066379e-01\n",
      "  3.23892808e-01 -4.78234936e-01 -5.18598577e-01 -1.35438608e+00\n",
      "  2.72277047e-01 -5.24690330e-01  1.84352918e+00 -6.66928224e-01\n",
      " -2.51032079e-01 -7.06181008e-01  1.73088536e-01  4.94322577e-01\n",
      "  6.71863140e-02  1.17822179e+00 -1.74925762e-01  1.20852390e+00\n",
      "  5.77682003e-02  1.09457662e+00  7.59205279e-01  3.95888489e-01\n",
      "  1.49380983e+00 -1.86735415e+00  1.89161017e+00  6.84105075e-01\n",
      " -1.35452428e+00 -2.51346022e-02 -7.46580109e-01  3.21026478e-01\n",
      "  1.16546456e+00 -1.15090766e-01 -8.12477761e-01 -2.47796608e+00\n",
      "  9.80087072e-01 -7.71503360e-01 -1.11142884e+00 -2.06734915e+00\n",
      "  3.71278715e+00  9.05485916e-01 -1.41672359e+00 -1.18761264e+00\n",
      "  7.84041271e-01  1.00539828e+00  2.35918578e+00 -9.64089734e-01\n",
      "  6.47554083e-01  7.23903268e-02 -1.09893258e+00 -8.36868660e-01\n",
      "  1.07854103e-01 -5.25351168e-01  9.01816383e-01  5.50728793e-01\n",
      "  3.02881001e-01  4.15335806e-01  1.02072491e+00  1.65284157e+00\n",
      "  7.48471736e-01 -1.21772167e-01 -6.00550978e-01 -2.02500885e+00\n",
      "  1.64240583e+00  1.78244475e+00 -4.94954170e-01 -8.93611855e-01\n",
      "  2.62627355e-01  3.07358098e-01 -2.14351760e+00 -2.44300576e+00\n",
      "  1.93000234e+00 -1.07029820e+00 -1.62190303e-01 -1.19106143e+00\n",
      "  5.25961047e-01 -7.96307652e-02  4.23773090e-01 -1.26344053e+00\n",
      "  1.20088872e+00  1.43692137e+00 -2.41209897e-01 -5.12066177e-01\n",
      "  6.29088967e-01 -1.47951512e+00 -7.78264144e-01 -1.42115962e+00\n",
      " -9.02701742e-01 -6.08816111e-01 -6.86305873e-01 -6.14681310e-01\n",
      " -6.51295275e-01  1.55619218e-03  2.68421153e+00 -1.71581552e+00\n",
      " -4.48344771e-01  4.39339248e-01  3.96059492e-01  2.66674212e-01\n",
      " -1.47808277e+00 -9.75576473e-01 -6.17155030e-01  1.03613681e+00\n",
      " -9.93225579e-01  5.79936117e-01  1.32996845e+00 -1.89691252e+00\n",
      "  1.14379318e+00 -5.19469299e-01 -1.06532804e+00 -1.06742883e+00\n",
      " -1.68454351e+00 -3.28014950e-01  1.71434352e+00  2.33219042e-01\n",
      " -1.20861536e+00 -9.32772310e-01  7.39586393e-01 -2.93318180e-01\n",
      "  4.03387710e-01 -1.98838197e+00 -2.47012623e-01  9.92014567e-01\n",
      "  2.60288102e-01 -1.84580403e-01  1.77877672e+00  3.17549711e-01\n",
      " -2.08244365e-02  2.95522031e-01 -1.04277586e+00 -1.49934108e+00\n",
      "  7.05406921e-02 -1.72809989e+00 -2.91259851e-01  1.42950884e+00\n",
      "  5.06987741e-01  9.80543506e-01 -8.08754867e-02 -2.92514691e-01\n",
      " -2.36111533e-01 -9.18637892e-01 -1.43190633e+00 -2.45071156e-01\n",
      "  9.85457126e-02  1.06229828e+00  8.63713886e-01 -1.09555588e+00\n",
      "  4.09599350e-01 -3.47564582e-01 -1.91377183e-01  7.20434221e-01\n",
      " -9.45409252e-01 -6.90229458e-01 -4.71819368e-01  6.75845576e-01\n",
      "  6.63201813e-01  4.27810223e-01 -1.37220818e+00  3.44903391e-01\n",
      "  1.54582661e+00 -6.06361677e-01 -2.05031999e+00 -3.62073043e-01\n",
      " -7.41390157e-01  6.06256470e-01 -7.28148441e-01 -1.93801400e-01\n",
      "  1.37443153e-01 -2.91794238e-01  1.31246768e-01  4.07626037e+00\n",
      "  2.51578638e-01 -8.11432290e-01  1.57863700e-01  3.24645421e-01\n",
      " -1.28893668e+00  2.12103850e-01  7.29356856e-01 -5.51039323e-01\n",
      " -8.44485855e-01 -1.07686877e+00 -7.40744860e-01  1.39775929e+00\n",
      "  4.79047050e-01  1.87447367e+00  7.56464530e-01  5.53599410e-01\n",
      "  8.18756311e-01  2.11526403e-01 -2.90540896e-01  1.25255010e-01\n",
      " -3.16090458e-02  4.26219148e-01 -1.24284759e-01  6.72571434e-01\n",
      "  1.43762911e+00  4.30013348e-02 -1.72204231e+00  5.54092527e-01\n",
      "  7.50642393e-01 -3.47089418e-01 -1.16256137e+00 -2.95667180e-01\n",
      " -7.17580995e-01 -1.07348826e+00 -1.26036565e+00 -6.16076337e-01\n",
      " -8.23548341e-01 -7.38368187e-01 -2.23550026e+00 -1.40413328e+00\n",
      " -1.77295904e-01  1.56727027e+00 -1.93355520e+00 -2.23775616e+00\n",
      "  1.17270526e+00 -2.96587291e+00 -4.27552135e-01  3.16834504e+00\n",
      " -1.05253898e+00 -2.85834734e-01  8.15342415e-01  1.05252179e+00\n",
      "  5.57388987e-01  8.72444807e-02  6.03615643e-03 -7.34163978e-01\n",
      "  1.72070559e+00 -5.62697419e-02  6.48456926e-01 -2.83461806e-01\n",
      " -1.26235452e+00 -6.19981970e-01 -4.10901838e-01 -2.41632255e-01\n",
      "  1.89119201e+00 -4.79675899e-01 -2.50609252e+00  9.36638483e-01\n",
      " -1.01733206e+00  9.84217777e-01 -1.46267450e+00  1.80807544e-01\n",
      " -2.82427053e-01  3.18706554e-01  3.64480104e-01  6.62468749e-01\n",
      "  1.25020255e+00 -1.82767285e+00  5.25504722e-01 -2.54074846e-01\n",
      "  6.03983732e-01 -2.40778248e-01  4.27164705e-01  1.40858666e+00\n",
      " -6.93522209e-01  8.94434561e-01  1.34988463e+00 -4.50108619e-01\n",
      " -5.35591819e-01  1.94983533e-01  5.70972302e-01 -1.25777154e-01\n",
      "  2.33098682e-01  1.29782616e+00 -1.20146146e+00  5.88664955e-01\n",
      " -2.64334939e-01 -1.26054161e+00  7.08685808e-01  1.43562988e+00\n",
      "  3.47312082e-02  9.25988056e-01 -4.75800045e-01  1.17450169e+00\n",
      " -4.39246970e-01  6.65573698e-01 -6.00023332e-01  1.82928564e+00\n",
      " -3.52697775e-01  1.17524786e+00 -2.53365660e-01  5.35122538e-02]\n",
      "0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model:\n",
      "[-1.43967984e-02  2.51516705e-02 -2.17543586e-02  3.80772268e-02\n",
      "  1.87556467e-02  8.66290328e-02 -6.96147812e-02  1.20688873e-02\n",
      "  9.14800709e-02 -1.70165037e-01 -8.54772876e-02  6.20245005e-02\n",
      " -3.60309187e-02  1.25488711e-02 -8.13697221e-02 -2.37536067e-02\n",
      "  1.91861278e-01  6.77952608e-02  7.31826765e-02  3.19839058e-03\n",
      " -1.53041182e-02 -1.57256386e-02  8.04234874e-02  5.95462531e-03\n",
      "  4.14901496e-03 -8.56252129e-02  7.41784273e-02  1.63092658e-01\n",
      "  2.77743408e-02  5.54133578e-02  3.94750837e-02 -2.72766613e-01\n",
      " -1.77850552e-02 -8.64965603e-04 -1.94108930e-01 -7.65133207e-02\n",
      " -1.92922073e-01  1.07940793e-01  9.80115998e-02  5.83033520e-02\n",
      " -4.73050632e-02 -9.88776306e-02 -3.40057273e-02 -5.23304867e-02\n",
      "  3.80320952e-02 -1.25949937e-01 -4.68581512e-02 -6.79512739e-02\n",
      "  5.66820384e-02  5.12201543e-02  9.11578628e-02  1.61090818e-01\n",
      " -3.59888252e-02  1.59828105e-01  7.95472852e-02 -2.51488354e-02\n",
      "  1.22219180e-01 -1.71385266e-01  2.08274583e-01 -1.97954961e-02\n",
      " -4.91793037e-02  1.02998763e-01  1.27654476e-01  1.11957233e-01\n",
      " -7.79945218e-02 -2.33493074e-02 -1.62692403e-01  1.15985153e-01\n",
      "  3.13755888e-02  1.28953496e-01  1.20891252e-01 -5.92891696e-02\n",
      "  8.71539146e-02 -1.53930104e-01 -1.49677297e-01 -1.02803565e-01\n",
      " -2.61034975e-02 -2.98266940e-03  1.07647508e-01 -3.23122839e-02\n",
      " -8.92641141e-02  6.25646570e-02  2.56650233e-03  6.52773893e-02\n",
      " -4.61256655e-02 -8.48945724e-02 -1.27567468e-01  1.11696269e-01\n",
      " -5.73453120e-02  1.82564916e-02  7.21707123e-02  4.77135217e-02\n",
      " -1.38141053e-01 -6.64051576e-02 -2.31485039e-01 -1.47858438e-02\n",
      " -1.36352500e-01  8.34275784e-03 -4.70223421e-02 -4.23275123e-02\n",
      "  5.22158980e-02  1.37158051e-01  1.07656252e-01  1.64572120e-01\n",
      " -1.17821262e-01  1.55133048e-01 -4.30134718e-02 -1.25465826e-02\n",
      " -8.86453784e-02 -3.36511189e-02  1.66403647e-01  2.75034634e-01\n",
      "  2.14969067e-02  2.54408886e-02 -3.69668455e-02 -1.74082308e-01\n",
      " -8.71750923e-02  7.38861265e-02  9.90639098e-02 -9.88638978e-03\n",
      " -7.35033997e-02  7.87263535e-02 -1.72153745e-01  2.26226668e-02\n",
      " -7.24254089e-03  7.32099164e-02  8.53495859e-02  1.30223030e-01\n",
      " -1.19178285e-01 -6.00186328e-02  5.89197251e-02 -2.30989357e-01\n",
      "  8.37284135e-02 -3.07292548e-03  3.36749086e-02  1.49579761e-01\n",
      " -1.33689727e-02  2.91880248e-03 -8.71036168e-02  1.42876268e-01\n",
      " -1.46161599e-01 -9.56106465e-02 -5.75353948e-02  4.78507641e-02\n",
      " -8.91506990e-02 -6.55861377e-02  1.80119604e-01 -4.95812805e-02\n",
      " -5.35156061e-02 -2.52362876e-01 -1.06291906e-01 -4.73111886e-02\n",
      " -1.56345691e-02 -1.84689028e-02  3.37255498e-02  5.17502943e-02\n",
      " -3.07484374e-03 -1.07719273e-01  1.48460665e-01 -7.82144463e-02\n",
      "  2.35675405e-02 -8.09831235e-02  9.68694352e-02  3.42481515e-02\n",
      "  1.13842848e-01 -6.45943244e-03 -4.84916009e-02 -1.60598198e-02\n",
      " -6.30337378e-02 -1.39708876e-02 -2.03317303e-02 -2.50497910e-01\n",
      " -4.50052944e-02  8.01693148e-02  4.64657545e-03  6.07606674e-03\n",
      " -8.33715321e-03  7.70759485e-03 -2.03951537e-01  3.42535049e-02\n",
      "  1.89021795e-01 -7.54960395e-02  8.16882095e-02  1.78929591e-01\n",
      " -5.25416785e-03 -4.22603750e-02  5.43991678e-02 -3.95023804e-02\n",
      "  1.10450683e-01 -4.04417734e-02  1.46219734e-01  2.62756549e-02\n",
      " -4.06091586e-02  9.17405106e-02  1.28360618e-01  4.87255519e-03\n",
      "  9.32396193e-02 -4.27284202e-02 -1.53005931e-02  7.03728859e-02\n",
      " -3.58163366e-02  4.49179958e-02 -1.02064976e-01  2.55704658e-02\n",
      " -7.81524339e-02 -8.26499967e-03 -8.09902502e-02 -1.92025803e-01\n",
      " -9.85244522e-02 -5.94615041e-02 -5.57313215e-02  2.48015811e-02\n",
      " -1.61782492e-02  5.58326961e-02  3.12055989e-02 -2.00540456e-02\n",
      "  1.12170337e-01 -1.25996953e-02 -3.52348504e-02  1.10885703e-01\n",
      " -6.66496539e-03  1.20927623e-01 -6.03791626e-03 -7.73278439e-02\n",
      "  3.65447315e-02  5.14889323e-02  9.17606780e-03 -1.04651460e-01\n",
      "  4.35960623e-02 -1.15194367e-01 -9.73849741e-02 -7.66310167e-03\n",
      " -9.56629140e-02 -2.74315737e-02 -8.16941140e-02  2.81378191e-03\n",
      "  7.99823703e-02  1.09193719e-01 -2.48876540e-02  1.39886363e-02\n",
      "  7.52458600e-03 -4.88778185e-03 -9.07885967e-02 -1.40250461e-01\n",
      " -2.26835570e-02  7.58986907e-02  4.62430989e-02 -4.36960657e-02\n",
      " -1.89725216e-02 -6.28209141e-03  1.76199190e-01  8.43200851e-02\n",
      "  3.61163988e-02  4.98477813e-02  4.88931115e-02 -8.52832102e-02\n",
      "  1.50019856e-02  5.89145528e-02  3.21487726e-02  2.20848840e-01\n",
      " -2.16621668e-02 -1.09244760e-01 -3.49782298e-02 -2.47098871e-02\n",
      " -1.39615522e-01  1.54120877e-01  5.47760740e-02  7.49455194e-02\n",
      "  8.22508724e-02 -2.61931482e-03  2.55652464e-02  1.30238051e-02\n",
      " -1.93987603e-01 -4.99176803e-02 -8.47214222e-02  1.73603916e-01\n",
      " -6.86233979e-02  7.20288493e-02 -1.02388741e-01  1.47732714e-01\n",
      " -1.61814321e-01  4.10821228e-03 -5.72277800e-02  5.94700162e-02\n",
      "  3.85768621e-02 -5.09303384e-02  1.02434412e-01  1.71385503e-01\n",
      "  3.51353284e-02 -1.93404996e-03 -3.76665614e-02  3.49724722e-02\n",
      " -1.49068721e-01  5.97179459e-02 -5.92132511e-02  1.49134059e-02\n",
      " -1.54874195e-01 -1.48401301e-01 -4.44177504e-02  2.62349403e-02\n",
      " -5.20249796e-02  3.30760561e-04  6.76536034e-02 -1.37407762e-01\n",
      "  1.26521277e-02  1.30871276e-01  9.33122321e-02 -5.40719823e-02\n",
      "  2.24180451e-02  9.70829067e-03 -1.68354436e-02 -7.99682277e-02\n",
      " -7.00990522e-02  5.10242841e-02 -6.24148855e-02  1.18116016e-01\n",
      "  8.84463359e-02 -3.69463250e-02  1.73606824e-01  3.30501725e-02\n",
      " -2.26693154e-04  5.72499124e-02  5.39517559e-02  1.03073402e-01\n",
      "  9.90144367e-02  4.07640537e-02  4.25784934e-02 -5.38494786e-02\n",
      "  2.60548504e-02 -3.22396479e-01  2.19080307e-01  4.59107014e-02\n",
      "  9.26007502e-02 -9.84053059e-02  1.60261623e-01  7.58495142e-02\n",
      "  7.29407691e-02 -2.34483682e-01 -1.14165622e-01  1.11173816e-01\n",
      " -3.69372989e-02  8.73670079e-03 -9.07149482e-02  4.05686529e-02\n",
      "  5.37615556e-02  2.72428465e-02 -1.01569344e-01  1.34785252e-01\n",
      " -2.06630289e-02 -2.91815472e-03  1.99606447e-01 -1.83284364e-02\n",
      " -8.02050380e-02  5.17007999e-02  7.40300592e-02 -7.97089814e-02\n",
      "  1.06479331e-02 -1.93442779e-01 -1.17529021e-01  1.60269722e-01\n",
      "  4.47167295e-02  1.65808775e-02 -7.87139307e-02 -1.00908366e-01\n",
      "  8.85718847e-02  8.89879820e-02 -1.93372000e-01  4.28617190e-02\n",
      "  6.09470330e-03  6.86422231e-02  1.28957230e-01 -1.69950876e-01\n",
      " -4.96234896e-02 -9.41265686e-02 -1.83491141e-02  3.07209227e-02\n",
      " -5.57867985e-02 -5.16670003e-02  7.84605323e-02  5.41436686e-02\n",
      "  3.68454545e-02 -2.01377936e-01 -2.03034652e-02 -3.79623501e-02\n",
      " -1.72491400e-01 -1.40647434e-01 -5.35196120e-02 -9.40388640e-02\n",
      " -1.36528007e-02  7.78165826e-02 -4.20158796e-02 -9.03619003e-02\n",
      " -4.43119001e-02  1.16106884e-01  7.42747134e-02  1.48869326e-01\n",
      " -3.56594613e-02  2.94230005e-03 -6.20590884e-02 -7.59446728e-02\n",
      " -4.80695908e-02 -5.71401628e-02 -5.97020414e-02 -5.45908512e-02\n",
      "  1.34945861e-02  6.13390912e-02  1.43199430e-02 -1.72017767e-01\n",
      "  2.88707217e-02  1.27580824e-01  2.72124447e-03  1.21116105e-01\n",
      " -3.73932672e-02 -7.07561887e-02 -2.44247874e-02  8.47248823e-02\n",
      "  4.48257501e-02 -9.85297586e-02  2.91272073e-02 -8.18525295e-03\n",
      "  1.23146006e-01  1.70144117e-01  3.87299549e-03 -1.83558055e-02\n",
      " -5.07763086e-02 -1.04506155e-01 -1.21265578e-01  1.74143877e-02\n",
      "  3.41812517e-02 -1.89045923e-01  2.93804448e-02 -6.10216416e-02\n",
      " -4.61346697e-02 -2.77491935e-02 -1.04469444e-01 -4.77287359e-02\n",
      "  2.64405138e-02  1.03615599e-01 -9.14766642e-03 -2.73408844e-02\n",
      "  8.28202629e-02 -6.88148905e-02  7.43551862e-02  1.01154001e-01\n",
      "  4.57202396e-02  2.71108094e-02  5.29831203e-02  4.18683018e-02\n",
      " -1.53834983e-02 -6.22143964e-02  9.08921941e-02 -8.75508730e-02\n",
      "  3.48819287e-02 -5.52476410e-02 -2.19983061e-02 -5.66206318e-02\n",
      "  2.24225635e-02 -4.76622831e-03 -8.84117325e-02  4.03939695e-02\n",
      " -3.00777079e-02  8.32866908e-02 -5.89426196e-02 -8.62475020e-02\n",
      "  1.37917339e-01 -3.48543452e-03 -1.26861068e-01 -2.61330241e-02\n",
      "  7.63251810e-03 -1.95717169e-01 -4.58278325e-02 -7.87141544e-02\n",
      " -5.94599789e-02 -7.73098015e-02 -1.18294706e-01 -8.64173781e-02\n",
      "  5.21975243e-02 -6.47868120e-02 -8.71462954e-02 -1.28560920e-01\n",
      " -4.41506598e-02 -6.28701241e-02  6.47160479e-03  2.10860054e-02\n",
      " -1.32958560e-01  7.08097050e-02 -2.55272582e-02 -2.16835991e-02\n",
      "  2.41263818e-01  6.01917092e-02 -2.30033461e-02 -2.33102836e-02\n",
      "  4.73850555e-02 -1.75194810e-01 -1.03180223e-01 -9.72618583e-02\n",
      " -1.00084241e-01  2.11270323e-04 -6.29980599e-02 -1.03113172e-01\n",
      " -1.04459542e-01  1.15856786e-01 -2.00843467e-02 -3.71759723e-02\n",
      " -2.22237062e-01 -1.83456467e-01  2.26013324e-02  1.43358225e-01\n",
      "  1.19055343e-02  8.94444424e-02 -4.13591323e-02  1.07429843e-01\n",
      "  1.40375996e-01 -3.67986630e-02 -1.38803006e-01  3.39923333e-02\n",
      "  4.25755489e-03  7.34261684e-02 -2.46069771e-02  5.16887317e-02\n",
      " -4.08775123e-02  8.08121494e-02 -7.22443621e-02 -6.57216842e-02\n",
      " -5.11862184e-02 -7.38831464e-02  3.73712536e-02 -6.26103853e-02\n",
      " -2.82767005e-02  1.79050227e-02  5.79782370e-02  1.70044537e-02\n",
      " -1.00198587e-01  7.64388469e-02 -6.26331711e-02 -4.09481595e-02\n",
      "  1.74616202e-02  1.77424471e-02 -8.63552427e-02  1.77474321e-01\n",
      "  2.61636939e-01 -7.58993277e-02  8.65788845e-02  1.06792255e-02\n",
      "  1.36553807e-01  9.52583898e-02  1.62422692e-03 -1.41904682e-03\n",
      "  1.79336848e-01 -1.26760608e-01 -4.61685394e-02 -4.45370946e-02\n",
      " -1.77877687e-01  3.77130984e-02  8.91879211e-03  1.47517884e-01\n",
      "  7.88075956e-02 -5.49847694e-02 -5.80347656e-02 -1.57777767e-02\n",
      "  2.25203405e-02 -2.74302502e-02  1.59897308e-02 -5.85958120e-02\n",
      "  1.40301300e-02  8.93567321e-02 -4.29210501e-02 -1.44010054e-01\n",
      " -2.07471396e-02 -2.15549004e-02  1.33527622e-01  3.36127867e-02\n",
      "  4.09723042e-02 -3.45730131e-02  3.84689828e-02  4.46707181e-02\n",
      " -6.62717233e-02  1.03150522e-02  8.29932482e-04 -5.21429700e-02\n",
      " -1.44706440e-03 -4.86895513e-02  1.33339005e-01  7.16875622e-02\n",
      " -3.41881822e-02  1.45484539e-02  7.82845526e-02 -6.78397105e-02\n",
      " -4.43449225e-02 -8.91718215e-02  8.58909707e-02  1.08325004e-01\n",
      "  9.80379288e-03  2.55024828e-02 -6.59078350e-02 -4.15484461e-02\n",
      " -1.28618275e-02 -8.13547475e-03  1.30127577e-01 -1.55373680e-01\n",
      " -1.48175240e-01 -2.10727990e-01 -3.08476822e-02 -2.26486184e-02\n",
      "  6.78310489e-02 -1.29008866e-01  1.31646453e-01 -6.72532100e-02\n",
      "  1.72891289e-02  3.58247563e-02 -1.14316944e-02 -7.11147847e-02\n",
      "  3.33892293e-02 -9.19675379e-02 -2.27244829e-02  1.56431300e-02\n",
      " -1.60614398e-01  1.47902269e-01 -6.42564922e-02  1.26508094e-01\n",
      "  2.73782238e-02 -4.42210774e-02  6.86293628e-02 -8.50357359e-02\n",
      "  8.25872985e-02  4.37985408e-02 -6.99961015e-03 -1.76865742e-02\n",
      " -1.46052261e-01  2.81196274e-02 -4.49563854e-02 -1.14490905e-02\n",
      " -3.48058432e-02 -1.55566217e-01  2.72031811e-02 -1.78139570e-01\n",
      "  4.37306099e-02  2.39392636e-02 -7.36135518e-02  1.87693915e-03\n",
      " -8.47081170e-02 -1.87652662e-01  1.04931212e-02 -5.68690180e-02\n",
      "  1.00737471e-01  4.36505631e-02 -2.29904500e-02 -1.17246276e-01\n",
      " -1.75516928e-01  1.31057070e-01 -5.42168581e-02  7.44480986e-02\n",
      "  6.27413616e-02 -7.72628084e-02  2.65129385e-02  1.22793235e-01\n",
      " -1.24922476e-01  6.32421366e-02 -5.27822529e-02 -4.13484052e-02\n",
      "  1.00471096e-01 -2.96719269e-02 -1.14429808e-02  1.12309399e-01\n",
      "  3.09077514e-02  5.96500074e-02 -5.13506902e-03 -1.81047644e-02\n",
      " -9.88428999e-02  2.29619128e-02 -1.02425887e-01 -1.32510895e-01\n",
      "  6.37085828e-02 -7.22006550e-02 -4.58918672e-02  6.28443130e-02\n",
      " -3.53227372e-02  1.28960095e-02 -6.53886866e-02  8.04911429e-02\n",
      " -7.37471368e-02  1.50959378e-01 -7.53570493e-02 -1.90554334e-01\n",
      " -5.96452126e-02 -1.07067217e-02  1.49914744e-01  1.70464166e-01\n",
      "  2.25687260e-02  2.04414483e-02 -1.31098024e-01 -1.09406403e-02\n",
      "  1.36112523e-01  8.07295401e-02 -8.84048262e-02  1.92875984e-02\n",
      "  2.63777452e-02  4.46389105e-02  3.45711971e-02 -1.05245349e-04\n",
      " -3.07497727e-02 -7.55397859e-02  6.20012150e-02 -1.59764215e-01\n",
      " -1.33375746e-01 -9.46656178e-02 -2.01426121e-01  4.85219146e-03\n",
      "  5.99385874e-02 -8.58993707e-02 -1.24926620e-01 -5.41889922e-02\n",
      "  1.86936681e-01  1.00638489e-01 -1.01755884e-01 -1.18138665e-01\n",
      " -2.45728641e-01  7.28166813e-02  1.67072162e-02 -1.17564288e-01\n",
      " -4.91528018e-03  3.88557175e-02 -3.91866819e-02 -9.66148516e-02\n",
      "  7.49450478e-02 -4.65670661e-02  5.21616203e-02 -5.02386170e-02\n",
      "  7.26104699e-02  1.48186946e-01  3.54400074e-02  2.60057634e-01\n",
      " -1.32395971e-01 -2.35694214e-02  4.31190496e-02  7.28151494e-02\n",
      "  8.73730760e-04  9.27474204e-02  1.07436853e-01 -4.16798185e-02\n",
      "  3.92156550e-03  1.00052787e-02  1.02498758e-01 -1.35389028e-01\n",
      " -3.19004909e-02 -1.08711007e-02  2.25878458e-01  1.33710123e-01\n",
      "  5.09223946e-03 -2.28435666e-02  2.87705718e-01  1.79103846e-01\n",
      "  1.08761711e-02 -2.96851057e-02  3.96366015e-03 -1.31122864e-01\n",
      " -3.19160661e-02 -2.09884474e-01  3.53927097e-02 -2.61582843e-01\n",
      "  7.28548100e-02  3.27446677e-02  1.68131665e-01 -1.08850977e-01\n",
      " -3.29899607e-02 -1.08926071e-01  1.48806273e-01  1.01455168e-01\n",
      " -6.22232803e-02 -4.79050054e-02 -5.97117631e-02 -5.44633187e-02\n",
      " -1.05602948e-01 -1.88923127e-01 -2.13134058e-03  6.40174561e-02\n",
      "  8.65150376e-02  5.40722540e-02 -1.01103388e-01 -4.09270258e-02\n",
      "  1.76447893e-01  1.73381045e-02  2.37317686e-01 -1.34089313e-02]\n",
      "0.43362766839772854\n",
      "target values for D:\n",
      "[0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
      " 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0\n",
      " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
      " 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0]\n",
      "prediction on D:\n",
      "[False False  True False  True False False  True False  True False False\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True False  True False  True False False False False False False False\n",
      " False  True  True False  True  True False  True False  True  True False\n",
      "  True False False False  True False  True  True False False False False\n",
      "  True  True False False False False False  True  True  True False False\n",
      "  True  True False False  True False False  True False  True  True  True\n",
      " False False  True False  True False  True False False  True False  True\n",
      "  True False  True  True False  True False  True  True  True False False\n",
      " False  True  True  True False False False False  True  True False False\n",
      "  True  True False  True  True  True  True False  True  True  True False\n",
      "  True  True  True False False False False  True  True False  True  True\n",
      " False  True False False  True  True  True  True  True  True False  True\n",
      " False False False False  True  True  True  True  True False False False\n",
      " False  True False  True False False  True False  True  True False  True\n",
      " False  True False False False  True False False False  True False False\n",
      "  True False False False  True False  True False False False  True  True\n",
      " False  True  True False  True False  True False  True False False False\n",
      "  True False  True False False False  True  True  True  True  True False\n",
      " False  True  True  True False  True False  True  True  True  True  True\n",
      "  True  True False  True False  True False False  True  True  True False\n",
      "  True  True  True  True False  True  True False  True False  True  True\n",
      " False False False False  True  True  True  True  True False  True False\n",
      "  True False  True  True  True False  True  True  True  True False False\n",
      " False False False False False  True False False  True False  True  True\n",
      "  True False  True False False False  True False  True False False False\n",
      " False False  True False  True  True  True  True  True  True  True  True\n",
      "  True  True False False  True  True  True False  True  True False  True\n",
      "  True  True False  True  True  True  True False False False  True False\n",
      " False  True False False  True False False False  True False  True False\n",
      " False False False False  True False False  True  True  True  True False\n",
      "  True  True  True  True False  True  True  True  True  True False False\n",
      "  True False False  True False False  True  True  True  True False  True\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "N = 400      # Training sample size\n",
    "feats = 784  # Number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "# D[0].shape = (400, 784), D[1].shape = (400,)\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.dmatrix('x')\n",
    "y = T.dvector('y')\n",
    "\n",
    "# Initialize the weight vector w randomly\n",
    "# \n",
    "# This and the following bias variable b \n",
    "# are shared so they keep their udpate values \n",
    "# between training iterations (updates)\n",
    "w = theano.shared(rng.randn(feats), name='w')\n",
    "\n",
    "# Initialize bias term\n",
    "b = theano.shared(0., name='b')\n",
    "\n",
    "print('Initial model: ')\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "\n",
    "# ------- Construct Theano Expression Graph ---------\n",
    "# Prediction, Probability that target = 1 \n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))      \n",
    "prediction = p_1 > 0.5                        # Prediction threshold\n",
    "\n",
    "# Cross entropy loss function, returns an array of cross entropy's\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) \n",
    "\n",
    "# Get the average of all the cross entropy's, add regularization  \n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum()    \n",
    "\n",
    "# Compute the gradient of the cost (w/ reg), w.r.t weight vector w and bias term b\n",
    "gw, gb = T.grad(cost, [w,b])                  \n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "  inputs=[x,y], \n",
    "  outputs=[prediction, xent], \n",
    "  updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb))\n",
    ")\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "  pred, err = train(D[0], D[1])\n",
    "  \n",
    "print(\"Final model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(D[1])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(D[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression: Classifying MNIST Digits\n",
    "Recall that Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix W and a bias vector b. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $x$ is a member of a class $i$, a value of a stochastic variable $Y$, can be written as:\n",
    "\n",
    "$$P(Y = i \\mid x, W, b) = softmax\\big(Wx + b\\big)$$\n",
    "\n",
    "$$= \\frac{e^{W_ix + b_i}}{\\sum_j e^{W_jx + b_j}}$$\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "\n",
    "$$y_{pred} = argmax_i P\\big(Y = i \\mid x, W, b\\big)$$\n",
    "\n",
    "This can be done in Theano as follows:\n",
    "\n",
    "```python\n",
    "# Initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "self.W = theano.shared(\n",
    "  value=np.zeros((n_in, n_out),dtype=theano.config.floatX),\n",
    "  name='W',\n",
    "  borrow=True\n",
    ")\n",
    "\n",
    "# Initialize biases b as a vector of n_out 0s\n",
    "self.b = theano.shared(\n",
    "  value=np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "  name='b',\n",
    "  borrow=True\n",
    ")\n",
    "\n",
    "# Creating a symbolic expression for computing the matrix of class-membership probabilities\n",
    "# Where:\n",
    "# W is a matrix where column-k represents the separation hyperplane for class-k\n",
    "# x is a matrix where row-j represents input training sample-j\n",
    "# b is a vector where element-k represents the free parameter of hyperplane-k\n",
    "self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "# Symbolic description of how to compute prediction as class whose probability is maximal\n",
    "self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "```\n",
    "\n",
    "Since the parameters of the model must maintain a persistent state throughout training, we allocate shared variables for $W$, $b$. This declares them both as being symbolic Theano variables, but also initializes their contents. The dot and softmax operators are then used to compute the vector $P(Y|x,W,b)$. The result `p_y_given_x` is a symbolic variable of vector-type.\n",
    "\n",
    "To get the actual model prediction, we can use the `T.argmax` operator, which will return the index at which `p_y_given_x` is maximal (i.e. the class with maximum probability).\n",
    "\n",
    "Now of course, the model we have defined so far does not do anything useful yet, since its parameters are still in their initial state. The following section will thus cover how to learn the optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Loss Function\n",
    "Learning optimal model parameters involves minimizing a loss function. In the case of multi-class logistic regression, it is very common to use the negative log-likelihood as the loss. This is equivalent to maximizing the likelihood of the data set $\\cal{D}$ under the model parameterized by $\\theta$. Let us first start by defining the likelihood $\\cal{L}$ and loss $\\ell$:\n",
    "\n",
    "$$L(\\theta= \\{ W,b \\},D) = \\sum_{i=0}^{|D|} log\\big(P(Y= y^{(i)} \\mid x^{(i)}, W, b)\\big)$$\n",
    "\n",
    "$$\\ell(\\theta=\\big\\{W, b\\big\\}, D) = - L(\\theta = \\{ W,b \\}, D)$$\n",
    "\n",
    "Recall that $y$ is representing the class that was predicted for training sample $i$. Hence, technically the top equation could have have a $target$ (true distribution) multiplied by the log likelihood, where the target is equal to 1. However, it is safe to drop in this scenario. \n",
    "\n",
    "```python\n",
    "# y.shape[0] is (symbolically) the number of rows in y, i.e. the\n",
    "# number of examples (n) in the minibatch\n",
    "# T.arange(y.shape[0]) is a symbolic vector which will contain \n",
    "# [0,1,2,...,n-1]. NOTE: y is the TARGET!\n",
    "# T.log(self.p_y_given_x) is a matrix of Log-probabilities (call\n",
    "# it LP, our predictions) with one row per example and one column per class\n",
    "# LP[T.arange(y.shape[0]), y] is a vector v containing \n",
    "# [LP[0,y[0]], LP[1, y[1]], LP[2, y[2]],..., LP[n-1, y[n-1]]]\n",
    "# And T.mean(LP[T.arange(y.shape[0], y]) is the mean (across \n",
    "# minibatch samples) of the elements in v, i.e., the mean log \n",
    "# likelihood across the minibatch.\n",
    "\n",
    "return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "```\n",
    "\n",
    "## Creating a Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix `W`\n",
    "    and bias vector `b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=np.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        \n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "        \n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "      \n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Class\n",
    "```python\n",
    "# Generate symbolic variables for input (x and y represent a minibatch)\n",
    "x = T.matrix('x')  # Data, presented as rasterized images\n",
    "y = T.ivector('y') # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "# Construct the logistic regression class \n",
    "# Each MNIST image has size 28x28\n",
    "classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by allocating symbolic variables for the training inputs $x$ and their corresponding classes $y$. Note that `x` and `y` are defined outside the scope of the `LogisticRegression` object. Since the class requires the input to build its graph, it is passed as a parameter of the __init__ function. This is useful in case you want to connect instances of such classes to form a deep network. The output of one layer can be passed as the input of the layer above. (This tutorial does not build a multi-layer network, but this code will be reused in future tutorials that do.)\n",
    "\n",
    "Finally, we define a (symbolic) `cost` variable to minimize, using the instance method `classifier.negative_log_likelihood`.\n",
    "\n",
    "```python\n",
    "# The cost we minimize during training is the negative log likelihood\n",
    "# of the model in symbolic format\n",
    "cost = classifier.negative_log_likelihood(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `x` is an implicit symbolic input to the definition of `cost`, because the symbolic variables of `classifier` were defined in terms of `x` at initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Model\n",
    "Learning the Model\n",
    "To implement MSGD in most programming languages (C/C++, Matlab, Python), one would start by manually deriving the expressions for the gradient of the loss with respect to the parameters: in this case $\\partial{\\ell}/\\partial{W}$, and $\\partial{\\ell}/\\partial{b}$, This can get pretty tricky for complex models, as expressions for $\\partial{\\ell}/\\partial{\\theta}$ can get fairly complex, especially when taking into account problems of numerical stability.\n",
    "\n",
    "With Theano, this work is greatly simplified. It performs automatic differentiation and applies certain math transforms to improve numerical stability.\n",
    "\n",
    "To get the gradients $\\partial{\\ell}/\\partial{W}$ and $\\partial{\\ell}/\\partial{b}$ in Theano, simply do the following:\n",
    "\n",
    "```python\n",
    "g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g_W` and `g_b` are symbolic variables, which can be used as part of a computation graph. The function `train_model`, which performs one step of gradient descent, can then be defined as follows:\n",
    "\n",
    "```python\n",
    "# Specify how to update the parameters of the model as a list of \n",
    "# (variable, update expression) pairs\n",
    "updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "           (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "# Compiling a Theano function `train_model` that returns the cost, but\n",
    "# at the same time updates the parameters of the model based on the rules\n",
    "# defined in `updates`\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: train_set_y[index * batch_size: (index + 1) * batch_size],\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
