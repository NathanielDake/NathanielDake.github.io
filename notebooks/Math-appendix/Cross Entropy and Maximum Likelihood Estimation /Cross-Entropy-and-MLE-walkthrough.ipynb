{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy and Maximum Likelihood Estimation\n",
    "<br></br>\n",
    "If you have gone through any of my other walkthroughs on machine learning, particularly those on **Logistic Regression**, **Neural Networks**, **Decision Trees**, or **Bayesian machine learning** you have definitely come across the concept of **Cross Entropy** and **Maximum Likelihood Estimation**. Now, when discussed separately, these are relatively simple concepts to understand. However, during the creation of these notebooks, particularly the sections on logisitic regression and neural networks (and the cost functions involved), I felt as though it was not clear why they were related in certain cases. \n",
    "\n",
    "This notebook is meant to do three things:\n",
    "1. Describe **Cross Entropy** in detail\n",
    "2. Describe **Maximum Likelihood Estimation** in detail\n",
    "3. Describe how the **Cross Entropy** can be equivalent to the negative **log-likelihood**, such as in the cost function in a neural network.\n",
    "\n",
    "So, with that said, lets get started talking about Cross Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cross Entropy\n",
    "## 1.1 Introduction\n",
    "When we develop a model for probabilistic classification, we are trying to figure out how to map the models **inputs** to **probabilistic predictions**, with the goal that these are very close to the **ground-truth probabilities**. The **training** process is done by iteratively adjusting the model's parameters so that our predictions get closer and closer to the ground-truth. \n",
    "\n",
    "Say we are trying to build a model that can determine whether an image contains a dog, a cat, or a fish. If, for instance, we input an image that contains a fish, we are hoping that the output is: \n",
    "\n",
    "$$y = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Since that vector represents the **ground-truth class probabilities**- in this case 0 for dog, 0 for cat, and 1 for fish. If our model ended up predicting a different probability distribution, for instance:\n",
    "\n",
    "$$\\hat{y}= \\begin{bmatrix}\n",
    "    0.3 \\\\\n",
    "    0.2 \\\\\n",
    "    0.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then we would want to adjust our parameters so that $\\hat{y}$ gets closer to $y$. The question is: What exactly do we mean when we say \"get's closer to\"? How should we measure the difference between $\\hat{y}$ and $y$? One possible measure is **cross entropy**. \n",
    "\n",
    "## 1.2 Entropy \n",
    "In the context of **information theory**, what is **entropy**? Let's look at an example first. Say you are standing along a road, and you want to communicate each car model that you see pass to a friend. The only means that you have to communicate with your friend is a binary channel where you can only send 0 or 1, and each particular bit costs 10 cents. To do this, you will need bit sequences, one to represent each car model. \n",
    "\n",
    "Lets assume you are trying to minimize the amount of money you have to spend. How would you decide to assign bit sequences to car models? Would you use the same number of bits for a Toyota Camry as you would for a corvette? No, you wouldn't! Clearly you know that the camry is far more common, and you will be communicating to your friend that you saw a camry far more often, so you want to assign it a smaller sequence of bits. In other words, you are exploiting your knowledge about the distribution over car models to reduce the number of bits that you need to send on average. \n",
    "\n",
    "Well, it turns out that if you have access to the underlying distribution of cars on the road, $y$, then if you want to use the smallest number of bits on average, you should assign $log(\\frac{1}{y_i})$ bits to the $i$th symbol. (Remember, $y_i$ is the probability of the $i$th symbol). \n",
    "\n",
    "For example, if we assume that seeing a Camry is 128 times as likely as seeing a corvette, then we'd give the Camry 7 less bits than the Tesla symbol:\n",
    "\n",
    "$$b_{camry} = log\\frac{1}{128p_{corvette}} = log\\frac{1}{p_{corvette}} + log\\frac{1}{128} = b_{corvette}-7$$\n",
    "\n",
    "If we fully exploit the known distribution of cars, $y$, in this way, we can achieve an optimal number of bits per transmission. The optimal number of bits is known as **entropy**. Mathematically, it's just the expected number of bits under this optimal encoding. \n",
    "\n",
    "$$H(y) = \\sum_iy_ilog\\frac{1}{y_i} = -\\sum_iy_ilog(y_i)$$\n",
    "\n",
    "Where again, $y_i$ is the probability of seeing the $i$-th symbol, i.e. a corvette, and $log\\frac{1}{y_i}$ is the number of bits we have assigned to it. So this equation just means that we take the probability of seeing each car, and multiply that by the number of bits we have assigned to it, and that is the total number of bits we would be expecting to transmit.\n",
    "\n",
    "<br></br>\n",
    "## 1.3 Cross Entropy\n",
    "Now, if we think of a distribution as the tool we use to encode symbols, then entropy measures the number of bits we'll need if we use the *correct* tool, $y$, the **ground-truth probability distribution**. This is optimal, in that we can't encode the symbols using fewer bits on average. \n",
    "\n",
    "**However**, **cross entropy** represents the number of bits we will need if we encode symbols from $y$ using the *wrong* tool $\\hat{y}$. In other words, if the probabilty distribution that our model learns, $\\hat{y}$, is not the same as $y$ (which it almost never will be), then **cross entropy** represents the number of bits used in the encoding. We would have encoded the $i$th symbol with $log\\frac{1}{\\hat{y_i}}$ bits, instead of $log\\frac{1}{y_i}$ bits. \n",
    "\n",
    "We of course will still be utilizing the value of the true distribution $y$, since that is the distribution we will actually encounter (if this is unclear, all it means is that you create the encoding scheme *before* you actually see the number of cars, meaning you use a distribution you are hoping is close the the correct one. We call this distribution $\\hat{y}$. However, when actually determining how many bits you use, that is based on *actual* cars that pass you, which is the true distribution, $y$.). Mathematically this looks like:\n",
    "\n",
    "$$H(y, \\hat{y}) = \\sum_iy_ilog\\frac{1}{\\hat{y_i}} = -\\sum_iy_ilog\\frac{1}{\\hat{y_i}}$$\n",
    "\n",
    "**Cross entropy** is *always* larger than **entropy**. Encoding symbols according to the wrong distribution $\\hat{y}$ will always make us use more bits. The only exception is in the trivial case where $y$ and $\\hat{y}$ are equal, and in this case entropy and cross entropy are equal.\n",
    "\n",
    "<br></br>\n",
    "## 1.4 KL Divergence \n",
    "The **KL Divergence** from $\\hat{y}$ to $y$ is simply the **difference** between **cross entropy** and **entropy**:\n",
    "\n",
    "$$KL(y \\;||\\; \\hat{y}) = \\sum_iy_ilog\\frac{1}{\\hat{y_i}} - \\sum_iy_ilog\\frac{1}{y_i} = \\sum_iy_ilog\\frac{y_i}{\\hat{y_i}}$$\n",
    "\n",
    "It measures the extra bits we'll need on average if we encode symbols from $y$ according to $\\hat{y}$. It is never negative, and it is only 0 if $y$ and $\\hat{y}$ are the same. Note that minimizing the **cross entropy** and minimizing the **KL divergence** from $y$ to $\\hat{y}$ are the same thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Maximum Likelihood\n",
    "## 2.1 Introduction \n",
    "One thing that we try and do during machine learning is maximize the likelihood of our data, given our particular model. This really means that we are trying to say: \"How likely is it that we received these outputs, given our model is true\". As an example, say we have trained a model to predict whether an image is of a cat or a dog. Now lets say we run predictions on 3 images of dogs, but our model predicts they are all cats. In that case our data was 3 images of dogs, and our model got all predictions wrong. So we could say that, if our model was in fact correct, the likelihood of our input data really being 3 dogs is very low. Our goal is to find a model that **maximizes the likelihood of our data**. So we would want a model that predicts all 3 images are dogs. \n",
    "\n",
    "<br></br>\n",
    "## 2.2 Coin Toss Example\n",
    "Let's look at an example where try and calculate the likelihood for a biased coin. Say we have a coin with a probability of heads, $p(H)$, equal to $p$:\n",
    "$$p(H) = p$$\n",
    "In this case, $p$ is a parameter. The probability of tails (since this is a bernoulli trial), is:\n",
    "$$p(T) = 1 - p$$\n",
    "Now we are going to run an experiment to help us determine $p$. We flip a coin 10 times and we get 7 heads and 3 tails. We want to know how we would write the total likelihood, which is the probability of receiving the data (result) that we saw. The general form equation for the likelihood in a binomial experiment is:\n",
    "\n",
    "$$L(X\\;|\\;p) = p^k(1-p)^{N-k}$$\n",
    "\n",
    "In this case $k$ is the total number of success's and $N$ is the total number of trials. In our example, we had 7 heads, so 7 success's, and 3 tails, so 3 failures, with 10 trials total. Our likelihood function then looks like:\n",
    "\n",
    "$$L(X\\;|\\;p) = p^7(1-p)^3$$\n",
    "\n",
    "Note that we are able to do this because each coin toss is independent. Therefore we can multiply each probability! In other words the above equation came from multiplying each probability of each result together:\n",
    "\n",
    "$$p*p*p*p*p*p*p*(1-p)*(1-p)*(1-p)$$\n",
    "\n",
    "Also, the likelihood can look just like a conditional probability. This is because the likelihood is used when our data, in this case $X$, has already been observed. \n",
    "\n",
    "Now, we want to **maximize the likelihood**. In other words, we want to maximize $L$ with respect to $p$, our parameter. This means we want to choose a $p$ that maximizes $L$. This can be done using basic calculus. Note that in most of these problems we take the log and maximize the likelihood. This is acceptable because the log function is monotonically increasing. \n",
    "\n",
    "Before we perform the maximizing of our likelihood, lets get a quick idea of what our Likelihood function actually looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a011be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = np.power(x, 7)*np.power(1-x,3)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great, we can see that from a visual perspective we should be expecting a value of ~0.7 to be the value of $p$ that maximizes the likelihood of observing our data!\n",
    "\n",
    "Lets now go through the derivation. First we will take the log of the likelihood:\n",
    "$$log(L) = l = log\\Big(p^7*(1-p)^3\\Big)$$\n",
    "Then use the multiplication rule:\n",
    "$$log(p^7) + log\\Big((1-p)^3\\Big)$$\n",
    "Then the power rule:\n",
    "$$7log(p) + 3log(1-p)$$\n",
    "Set the derivative equal to zero:\n",
    "$$\\frac{dl}{dp} = \\frac{7}{p}+\\Big(-1*\\frac{3}{1-p}\\Big)= 0$$\n",
    "And solve for p:\n",
    "$$\\frac{7}{p} = \\frac{3}{1-p}$$\n",
    "$$\\frac{1-p}{p} = \\frac{3}{7}$$\n",
    "$$\\frac{1}{p} -1 = \\frac{3}{7}$$\n",
    "$$\\frac{1}{p} = \\frac{10}{7}$$\n",
    "$$p = \\frac{7}{10}$$\n",
    "This is what we would expect (visually it agrees)! If this is still slightly abstract, here is a visualization of exactly what is going on:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1F6Vm2ucJCfZVy2Iy4sKz6NujXSV-b6FW\">\n",
    "\n",
    "We already have observed our data $X$, and we slowly change our parameter (in the visual above the parameter is now $\\mu$, but it can be thought of as $p$. As we change the parameter, we can see that our probability distribution moves to the right. Once we find the parameter that maximizes the likelihood, we can see that the distribution on the left also is in the best place it can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 2. Predictive Power\n",
    "At this point, you may be happy to just use the cross entropy to measure the difference between two distributions, $y$ and $\\hat{y}$, and then using the total cross entropy over all training examples as our loss function. In particular, if we let $n$ index our training examples, the overal loss would be:\n",
    "\n",
    "$$H(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\sum_nH(y^{(n)}, \\hat{y}^{(n)})$$\n",
    "\n",
    "But let's look at another approach. What if we want our objective function to be a direct measure of our model's predictive power (at least with respect to our training data)? One common approach is to tune our parameters so that the **likelihood** of our data under the model is **maximized**. When performing **classification** we are often using a *discriminative model*, our \"data\" often just consists of the labels we are trying to predict. We can reason that a model that often predicts the ground-truth labels given the inputs might be useful, while a model that fails to predict the ground-truth labels is not useful. \n",
    "\n",
    "Because we generally assume that our samples are **independent and identically distributed**, the likelihood over all of our examples decomposes into a product over the likelihoods of individual examples:\n",
    "\n",
    "$$L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\prod_nL(y^{(n)}, \\hat{y}^{(n)})$$\n",
    "\n",
    "And what is the likelihood of the $n$th example? It is just the particular entropy of $\\hat{y}^{(n)}$ that corresponds to the ground truth label specified by $y^{(n)}$! \n",
    "\n",
    "If we go back to our original example, if the first training image is of a fish, then:\n",
    "\n",
    "$$y^{(1)} = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This tells us that the likelihood $L(y^{(1)}, \\hat{y}^{(1)})$ is just the last entry of:\n",
    "\n",
    "$$\\hat{y}= \\begin{bmatrix}\n",
    "    0.3 \\\\\n",
    "    0.2 \\\\\n",
    "    0.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Which is $y^{(1)} = 0.5$. Now lets that we have 4 training images labeled: *fish, cat, dog, cat*. This gives us our ground truth distribution: \n",
    "$$y^{(1)} = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "\\end{bmatrix}$$\n",
    "$$y^{(2)} = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0\n",
    "\\end{bmatrix}$$\n",
    "$$y^{(3)} = \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{bmatrix}$$\n",
    "$$y^{(4)} = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Our model would predict 4 other distributions: \n",
    "\n",
    "$$\\hat{y}^{(1)},\\hat{y}^{(2)},\\hat{y}^{(3)},\\hat{y}^{(4)}$$\n",
    "\n",
    "And our overall likelihood would just be: \n",
    "\n",
    "$$L(\\{ y^{(1)},y^{(2)},y^{(3)},y^{(4)}\\}, \\{\\hat{y}^{(1)},\\hat{y}^{(2)},\\hat{y}^{(3)},\\hat{y}^{(4)} \\}) = \\hat{y}^{(1)}\\hat{y}^{(2)}\\hat{y}^{(3)}\\hat{y}^{(4)}$$\n",
    "\n",
    "Maybe we would have been previously happy with just minimizing the cross entropy during training, but after seeing this, are we still happy? Why shouldn't we instead maximize the likelihood of our data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unified Loss Function\n",
    "Let's take a minute to play with the expression above. Because a logarithm is monotonic, we know that maximizing the likelihood is equivalent to maximizing the **log likelihood**, which is in turn equivalent to *minimizing* the **negative log likelihood**. \n",
    "\n",
    "$$-log\\Big(L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\})\\Big)= - \\sum_nlog\\Big(L(y^{(n)}, \\hat{y}^{(n)})\\Big)$$\n",
    "\n",
    "But, from the work we did earlier, we also know that the log likelihood of $y^{(n)}$ is just the log of a particular entry of $\\hat{y}^{(n)}$. In fact, its the entry $i$ that satisfies $y_i^{(n)} = 1$. We can therefore rewrite the log likelihood for the nth training example in the following way: \n",
    "\n",
    "$$ \\log L(y^{(n)}, \\hat{y}^{(n)}) = \\sum_i y^{(n)}_i \\log \\hat{y}^{(n)}_i $$\n",
    "\n",
    "Which in turn gives us an overall negative log likelihood of: \n",
    "\n",
    "$$ - \\log L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = -\\sum_n \\sum_i y^{(n)}_i \\log \\hat{y}^{(n)}_i $$\n",
    "\n",
    "Does this look familiar? This is exactly the **cross entropy**, summed over all training examples: \n",
    "\n",
    "$$ -\\log L(\\{y^{(n)}\\}, \\{\\hat{y}^{(n)}\\}) = \\sum_n \\big[-\\sum_i y_i \\log \\hat{y}^{(n)}_i\\big] = \\sum_n H(y^{(n)}, \\hat{y}^{(n)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusions\n",
    "\n",
    "When we develop a probabilistic model over mutually exclusive classes, we need a way to measure the difference between predicted probabilities $\\hat{y}$ and ground-truth probabilities $y$, and during training we try to tune parameters so that this difference is minimized.In this post we saw that cross entropy is a reasonable choice.\n",
    "\n",
    "From one perspective, minimizing cross entropy lets us find a $\\hat{y}$ that requires as few extra bits as possible when we try to encode symbols from $y$ using $\\hat{y}$.\n",
    "\n",
    "From another perspective, minimizing cross entropy is equivalent to minimizing the negative log likelihood of our data, which is a direct measure of the predictive power of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
