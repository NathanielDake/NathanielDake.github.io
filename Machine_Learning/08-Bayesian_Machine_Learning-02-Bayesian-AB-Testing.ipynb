{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian A/B Testing\n",
    "In this post we are going to discuss Bayesian Methods and their application to Bayesian A/B testing. In particular we will:\n",
    "* Look at the shortcomings of standard frequentist A/B testing\n",
    "* Motivate why Bayesian Methods are an improvement\n",
    "* Implement Bayesian A/B Testing in code\n",
    "\n",
    "Note that I am writing with the assumption that you have gone through my post on statistical inference and frequentist A/B testing, or are very comfortable with both of those topics. With that out of the way, let's get started.\n",
    "\n",
    "## 1. Drawbacks of Frequentist A/B Testing\n",
    "To begin, consider the following example:\n",
    "\n",
    "> A drug you are testing is working well-can you stop the test and improve quality of life of all participants? \n",
    "\n",
    "Standard frequentist statistics would say _no_. This is because it is bad to stop early when doing frequentist statistics, since it increases the chances that you reach a false positive conclusion. Remember, the $p$-value can pass below/above the threshold over time. Clearly this can be a shortcoming in many scenarios, and in some even go as far as being deadly. \n",
    "\n",
    "### 1.1 Multi-Armed Bandit\n",
    "Now, imagine the following scenario, commonly referred to as the **Multi-Armed Bandit** problem: You are at the casino playing slots (hence the term arm). The slot machines are bandits because they are taking your money. Not all of the slots machines are equal. Specifically: \n",
    "* 1 pays out 30% of the time\n",
    "* 1 pays out 20% of the time\n",
    "* 1 pays out 10% of the time \n",
    "\n",
    "This is the same problem as the click-through or conversion rate! Only now we would substitute click for getting a prize. So, if we wanted to find out what arm we should pull the receive the largest payout over time, we would setup an A/B test, run the experiment, and by the end you would know if any one arm was significantly better than the others. In other words, we would run the A/B test for a specific number of $N$ trials, then calculate the $p$-value.\n",
    "\n",
    "Now, let me ask another question: what would you do in real life? For instance, say that 1 arm gives you a prize 3 out of 3 times, and the other arm gives you a prize 0 out of 3 times. Well, most people (myself included) would intuitively play the arm that payed out 3 times! Why is this? Because we _adapt_. Even though three plays for each arms is most likely not enough to gain statistical significance, most people still feel compelled to believe that the first arm is better. \n",
    "\n",
    "### 1.2 Reinforcement Learning\n",
    "If you have gone through my posts on AI you will certainly have seen those related to Reinforcement Learning, which faces the same problem that we are here. Specifically, in RL we are trying to teach a machine to play a game. It models rewards it gets based on the actions it takes. The problem is that this process is **stochastic** and your model of the rewards is only an estimate (it is not feasible to try every possible action, in every possible state). In general, early on there are few actions to take, and the machine is unsure about most of the rewards. It can't \"choose the action that leads to the best reward\", because the knowledge about rewards is currently minimal. Only after collection a lot of data will the estimate be accurate. \n",
    "\n",
    "### 1.3 Explore vs. Exploit\n",
    "This problem that we are dealing with has a name: **The Explore-Exploit Dilemma**. We can use this name to frame the following question: \n",
    "\n",
    "> If we have gotten 3/3 on bandit 1, and 0/3 from bandit 2, should we:\n",
    "* **Exploit** bandit 1 more?\n",
    "* **Explore** at random to gather more data?\n",
    "\n",
    "This is the problem that we are going to be trying to solve in this post. We will look at several solutions, all of which are _adaptive_, but only some will be Bayesian. We will show the bayesian method last. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adaptive Learning Methods\n",
    "\n",
    "### 2.1 Epsilon-Greedy Solution \n",
    "One method of solving the explore-exploit dilemma is called the **epsilon-greedy** algorithm (and it is intuitive). It just so happens that this algorithm is also used in reinforcement learning. Here is how it works:\n",
    "\n",
    "> Say you have some software that is serving 2 advertisements ($A$ & $B$), you will *not* run a full A/B test by blindly serving ad $A$ and ad $B$ an equal number of times. Instead you will adapt to the data you have collected. If ad $A$ is performing better, it will be shown more often. \n",
    "\n",
    "Note: even with this adaptive system, you can still do traditional A/B test after collecting data. Remember the contingency table for the chi-square test doesn't require both samples to be the same size. The thing changing here is not the test, but the machine that serves the ads. It will adapt based on the performance of each ad. \n",
    "\n",
    "#### How does it work?\n",
    "It is simpler than it sounds! We start by choosing a small number epsilon ($\\epsilon$) between 0 and 1 which will serve as our _probability of exploration_. Typical values are 5% or 10%. In pseudocode it may look like:\n",
    "\n",
    "```\n",
    "epsilon = 0.1 \n",
    "while True:\n",
    "    r = rand()\n",
    "    if r < epsilon:\n",
    "        # explore\n",
    "        # show random advertisement\n",
    "    else:\n",
    "        # exploit\n",
    "        # show best advertisement (as determined by #clicks/#impressions)\n",
    "```\n",
    "\n",
    "#### Analyis of Epsilon-Greedy\n",
    "We can clearly see that that was pretty simple, so let's analyze further. One problem is that this algorithm does the same thing *forever*. For instance, even when ad $A$ is statistically significantly better than $B$, it will still sometimes choose $B$. Suppose the test has converged, $A$ is better than $B$, how much reward will we get? If we say that:\n",
    "\n",
    "$$reward(click) = 1 \\;,\\; reward(noclick) =0$$\n",
    "\n",
    "Then after $N$ impressions:\n",
    "\n",
    "$$reward = N(1-\\frac{\\epsilon}{2})$$\n",
    "\n",
    "In other words, for $\\epsilon = 0.1$, we get $0.95N$ instead of $N$. However, this is still better than traditional A/B testing with no adaptation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 UCB1\n",
    "Alright, so we have now looked at one adaptive learning method. The question is, can we improve upon it? To find out, let's look at the **UCB1** algorithm. Recall earlier discussions about confidence intervals. We determined an upper and lower limit to represent where we believe the true CTR is. We'll now look at a very similar idea, but we are not going to use the confidence bound we discussed before.\n",
    "\n",
    "If you are unsure how confidence intervals relate to the scenario at hand, considering the following: Intuitively, we know that if you collect 10 samples and take the mean, this is not as accurate as if you collect 1000 samples and take the mean. If you were to draw a confidence bound around the 10 sample mean, it would be much wider than that which was around the 1000 sample mean. This concept can be used algorithmically in order to dynamically update when each advertisement should be served.\n",
    "\n",
    "Now, to be clear, we are not going to be using the exact same confidence interval that we have worked with before. Rather, we are going to use a _tighter_ bound,: the **Chernoff-Hoeffding** bound. It says for some:\n",
    "\n",
    "$$\\epsilon > 0$$\n",
    "\n",
    "That the following probability inequality is true:\n",
    "\n",
    "$$P \\big(\\hat{\\mu}>\\mu+\\epsilon \\big) \\leq e^{-2\\epsilon^2N} $$\n",
    "\n",
    "It also follows that the opposite side is also true:\n",
    "\n",
    "$$P \\big(\\hat{\\mu} < \\mu+\\epsilon \\big) \\leq e^{-2\\epsilon^2N}  $$\n",
    "\n",
    "Note, this bound is a little different from the confidence interval that we have seen before. Namely, the confidence interval gave us specific numbers, and said that the probability that the true parameter lied within that interval was 95% (this isn't _entirely accurate_, but is an approximate description):\n",
    "\n",
    "$$P(\\text{lower bound} < \\mu < \\text{upper bound} ) = 95\\%$$\n",
    "\n",
    "On the other hand, our new equation says that:\n",
    "\n",
    "$$P(\\mu \\text{in interval}) > f(N)$$\n",
    "\n",
    "Or in english, \"the probability that the true parameter lies within this range is greater than some function of $N$\". In order to get the greater than sign, we just need to combine and arrange the two probabilities:\n",
    "\n",
    "$$P(\\hat{\\mu} - \\mu > \\epsilon) \\leq e^{-2\\epsilon^2N}  \\longleftrightarrow P(\\hat{\\mu} - \\mu \\leq \\epsilon) > 1 - e^{-2\\epsilon^2N} $$\n",
    "\n",
    "$$P(\\hat{\\mu} - \\mu < - \\epsilon) \\leq e^{-2\\epsilon^2N}  \\longleftrightarrow P(\\hat{\\mu} - \\mu \\geq - \\epsilon) > 1 - e^{-2\\epsilon^2N} $$\n",
    "\n",
    "$$P \\big( |\\hat{\\mu} - \\mu| \\leq \\epsilon \\big) > 1 - 2 e^{-2\\epsilon^2N} $$\n",
    "\n",
    "#### Upper Confidence Bound\n",
    "In any case, for us we are only interested in the upper bound, because we want to get the highest reward possible. How exactly does this help us? Well, intuitively if we have a tighter upper bound, we can be more confident above that maximum possible win rate of any bandit. We are skipping some math here, but essentially for any arm $j$, we \"choose\" $\\epsilon$ to be:\n",
    "\n",
    "$$\\epsilon_j = \\sqrt{\\frac{2 lnN}{N_j}}$$\n",
    "\n",
    "Where $N$ is the total number of games played so far, and $N_j$ is the total times that arm $j$ has been played. We can define the upper confidence bound via:\n",
    "\n",
    "$$\\mu_{UCB-j} = \\hat{\\mu}_j + \\epsilon_j$$\n",
    "\n",
    "$$\\mu_{UCB-j} = \\hat{\\mu}_j + \\sqrt{\\frac{2 lnN}{N_j}}$$\n",
    "\n",
    "So, in other words, we are picking whichever bandit arm gives us the largest _upper confidence bound_ for any given turn. Again, as with epsilon-greedy, we will be using a greedy strategy. However, we are not simply greedy with respect to the sample mean $j$, but rather the upper confidence bound of the sample mean. In psuedocode, we would have:\n",
    "\n",
    "```\n",
    "N = 0\n",
    "N_j = 0 for all bandits j\n",
    "\n",
    "while True:\n",
    "    j* = argmax(mu_j + sqrt( (2ln(N)) / (N_j) ))\n",
    "    \n",
    "    Play arm j*\n",
    "    \n",
    "    Update mu_j*\n",
    "    \n",
    "    N++; Nj*++\n",
    "```\n",
    "\n",
    "In english the above can be described as follows:\n",
    "1. We set $N$ and $N_j$ equal to 0, for all bandits $j$.\n",
    "2. Then, in a loop, we get the $argmax$ of the estimated mean, $\\hat{\\mu}$, plus the epsilon that we just defined, $\\epsilon_j = \\sqrt{\\frac{2 lnN}{N_j}}$. In other words, we choose the bandit with the highest upper bound.\n",
    "3. We play that bandits arms, and then we update the estimated mean for that arm, $\\hat{\\mu}_j$.\n",
    "4. Finally, we increment $N$ and $N_j$.\n",
    "\n",
    "#### How does this help with the Explore vs. Exploit problem?\n",
    "You may look at this algorithm and wonder how it helps us explore and exploit. Well, let's look at where the main logic of this algorithm is held, the following line:\n",
    "\n",
    "$$j^* = argmax \\Big( \\hat{\\mu}_j + \\sqrt{\\frac{2 lnN}{N_j}} \\Big)$$\n",
    "\n",
    "Let's now look at each term individually. The first term is just the estimated mean:\n",
    "\n",
    "$$\\hat{\\mu}_j$$\n",
    "\n",
    "If that is high, then we will be exploiting it more often. If you look at the second term:\n",
    "\n",
    "$$\\sqrt{\\frac{2 lnN}{N_j}}$$\n",
    "\n",
    "You will see that it depends on $N$, the total number of plays, and $N_j$, the number of plays for arm $j$. If $N$ is large while $N_j$ is low, then that will encourage us to explore arm $j$ more, because it increases the upper confidence bound. We also should consider the asymptotic behavior of:\n",
    "\n",
    "$$\\frac{ln(N)}{N}$$\n",
    "\n",
    "Above, as $N \\rightarrow \\infty$, $\\frac{ln(N)}{N}$ approaches 0, so we use only the true CTR (estimated means) when $N$ is large.\n",
    "\n",
    "We can see that from a coding/implementation perspective, this is no more complex than the epsilon-greedy algorithm. The right question to ask at this point, however, is what is our expected loss from this algorithm. With a good deal of math it can be shown that the loss is proportional to $ln(N)$. Compare this to epsilon-greedy where the loss is proportional to $N$. This means that in the long run, **UCB1** will perform much better than epsilon-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian Adaptive Learning\n",
    "\n",
    "### 3.1 Bayesian Paradigm\n",
    "We are finally approaching the point of implementing full blown bayesian learning. But before we do, I think it would be useful to make a distinction as to what the Bayesian Paradigm really is, and how it differs from the frequentist paradigm. \n",
    "\n",
    "Now, intuitively we know that as we collect more data, our estimates will become more accurate. As far as frequentist statistics are concerned, the method that we use to determine how accurate our measurements are is the _confidence interval_. \n",
    "\n",
    "One key point to remember is that the estimated click through rates (or whatever we are trying to measure) is a distribution! To be clear, an estimated parameter (like the mean) is a random variable, because it is the sum of independent and identically distributed random variables. Therefore, it has a distribution that is approximately gaussian, due to the central limit theorem. \n",
    "\n",
    "So, a _very important_ distinction that we want to make is that the true CTR is **fixed**. It is something that we are trying to find by obtaining data. We call this **Frequentist Statistics**. In frequentist statistics:\n",
    "* The parameters of the distribution are set (think population mean), we just don't know what they are.\n",
    "* Data is this randomly generated via those distributions (which are based on the population parameters)\n",
    "\n",
    "On the other hand, we have **Bayesian Statistics**. This essentially works in the opposite way. We give the parameter a distribution. It becomes a random variable that has an actual probability distribution. The data on the other hand is **fixed**. You can make the argument that this better reflects reality because data _is_ fixed. This is what happens in the real world. There is no randomness to a coin toss result after the fact. In this way we can then model the distribution of the parameter, given some data (i.e. $p(parameter \\mid data)$).\n",
    "\n",
    "To be clear, we can classify the two paradigms as follows:\n",
    "\n",
    "#### Frequentist Statistics\n",
    "When doing frequentist statistics we are trying to find the argmax of the likelihood. In other words, the value of the parameter that maximizes the likelihood of our data, $X$. This is a point estimate.\n",
    "\n",
    "$$\\hat{\\theta} = argmax_{\\theta} P(X \\mid \\theta)$$\n",
    "\n",
    "For example, when using the frequentist paradigm, we measure things like mean and click through rate with _point estimates_:\n",
    "\n",
    "$$\\frac{\\sum X} {N}$$\n",
    "\n",
    "The problem that you may see is that the above method doesn't take into account how accurate those estimates are. The frequentist solution is to use the central limit theorem to show that the confidence interval was approximately gaussian and from there we could get an upper and lower bound on the 95% confidence interval.\n",
    "\n",
    "So, the frequentist paradigm:\n",
    "1. Calculates the likelihood-the probability of your data, given the parameter. For example, the probability that you observed the data that you did, given a gaussian distribution with a mean $\\mu =4$\n",
    "2. Maximizes the likelihood with respec to the parameter in question. For example, find the mean $\\mu$ (and associated gaussian distribution) that maximizes the probability you observed the data that you did.\n",
    "3. This leaves us with a maximum likelihood estimate for that parameter.\n",
    "\n",
    "This can all be represented by the equation we saw earlier:\n",
    "\n",
    "$$\\hat{\\theta} = argmax_{\\theta} P(X \\mid \\theta)$$\n",
    "\n",
    "It is general, but think of the case: the likelihood of the data we observed ($X$) maximized by a given value of the mean $(\\theta = \\mu)$. The final value for $\\hat{\\theta}$ is a point estimate, not a distribution.\n",
    "\n",
    "#### Bayesian Statistics\n",
    "When we do Bayesian Statistics, we are trying to find the **posterior distributon**:\n",
    "\n",
    "$$P(\\theta \\mid X)$$\n",
    "\n",
    "This is the probability of a parameter, given the data. Data is given, and data is fixed. So, in bayesian statistics our parameter, $\\theta$, has a distribution! This is the opposite of frequentist statistics where $\\theta$ is a point estimate. \n",
    "\n",
    "So, in Bayesian Statistics we treat $\\theta$ as a random variable too. Because of this, and the subsequent fact that it has a distribution, we can use the shape of it's distribution to tell us how confident we are for any value of that parameter. We can apply bayes rule to help us with this:\n",
    "\n",
    "$$P(\\theta \\mid X) = \\frac{P(X \\mid \\theta) P(\\theta)}{P(X)} = \\frac{P(X \\mid \\theta) P(\\theta)}{Z}$$\n",
    "\n",
    "Where above: \n",
    "\n",
    "$$Z \\rightarrow \\text{normalizing constant}$$\n",
    "\n",
    "$$P(X \\mid \\theta) \\rightarrow \\text{Likelihood of data given current }\\theta$$\n",
    "\n",
    "$$P(\\theta) \\rightarrow \\text{Prior, old beliefs about }\\theta$$\n",
    "\n",
    "$$P(\\theta \\mid X) \\rightarrow \\text{Posterior, new beliefs about }\\theta \\text{after seeing data}$$\n",
    "\n",
    "Now, at this point you may be thinking that this should be easy! This is just multiplication and division! Not so fast. These are all probability distributions. In particular, remember that $P(X)$ is the integral over:\n",
    "\n",
    "$$\\int P(X \\mid \\theta) P(\\theta) d\\theta$$\n",
    "\n",
    "Generally speaking, this integral is either very hard or impossible to solve. One solution is to use sampling methods like **Markov Chain Monte Carlo**, but we are not going to cover that since we are going to arrive at a much more elegant solution.\n",
    "\n",
    "### 3.2 Conjugate Priors\n",
    "The answer to our problem from above is **conjugate priors**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
