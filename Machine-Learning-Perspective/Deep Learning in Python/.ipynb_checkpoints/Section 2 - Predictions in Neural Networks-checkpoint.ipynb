{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In the past we have looked at both logistic regression and binary classification. There, we would collect some data, and then try to predict 1 of 2 possible labels. For example, if we were dealing with an e-commerce stie, we could collect _**time spent on site**_ and _**number of pages viewed**_, and then try to predict whether someone is going to buy something on the site. \n",
    "\n",
    "In this case, we only have 2 dimensions. We will plot the information, and then try to use a straight line to classify the classes (buy or not buy):\n",
    "\n",
    "$$\\sigma \\big( w_1*(\\text{time spent on site}) + w_2 (\\text{number pages viewed})\\big)$$\n",
    "\n",
    "If we are able to find a line that goes between the classes, they are _linearly seperable_. When we are dealing with data that is linearly seperable, logistic regression is fine, since it is a linear classifier. So, in 2 dimensions linearly seperable data can be separated via a line, in 3 dimensions a plane, and and 4+ dimensions a hyperplane. The point is, no matter how many dimensions we are dealing with, our decision boundary is going to be straight, not curved. \n",
    "\n",
    "## 1.1 Neural Networks Add Non-linearity\n",
    "Now, as we get into the realm of Neural Networks, things begin to change. We can have non-linearly seperable variables, such as: \n",
    "\n",
    "<img src=\"images2/nonlinear-data.png\" width=\"400\">\n",
    "\n",
    "Logistic regression would _not_ be appropriate for this, while neural networks would! Recall, a linear function has the form:\n",
    "\n",
    "$$w_1x_1 + w_2x_2+...+w_nx_n$$\n",
    "\n",
    "$$w^T x$$\n",
    "\n",
    "Where, just a reminder, in the vector notation $w_T x$, the weights are transposed because by convention they are stored as a column vector, but we need to be able to perform matrix vector multiplicaton (akin to the dot product in this case) with the input vector $x$. \n",
    "\n",
    "So, we can see that anything that cannot be simplified into $w^Tx$ is nonlinear. Now, $x^2$ and $x^3$ are both nonlinear, but neural networks are nonlinear in a _very specific way_. Neural Networks achieve nonlinearity by:\n",
    "\n",
    "> _Being a combination of multiple logistic regression units (neurons) put together._\n",
    "\n",
    "That is going to be the focus of this section; determining how we can build a nonlinear classifier (in this case a neural network), by combining logistic regression units (neurons). We will then use this nonlinear classifier to make _**predictions**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Logistic Regression $\\rightarrow$ Neural Networks\n",
    "We are now ready to start the transition from logistic regression to neural networks. Recall that logistic regression is a neuron, and we are going to be connecting many together to make a network of neurons. The most basic way to do this is the _**feed forward method**_. For logistic regression, we have a weight corresponding to every input:\n",
    "\n",
    "<img src=\"images2/logistic-reg-unit.png\" width=\"500\">\n",
    "\n",
    "This is seen clearly in the image above. We have two input features, $x_1$ and $x_2$, but of course there can be many more. Each input feature has a corresponding weight, $w_1$ and $w_2$. In order to determine the output $y$, we multiply each input by its weight, sum them all together, add a bias term, and put it through a sigmoid function:\n",
    "\n",
    "$$z = x_1w_1 + x_2w_2 + bw_0$$\n",
    "\n",
    "$$y = p(y \\mid x) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "$$prediction = round \\big( p(y \\mid x)\\big)$$\n",
    "\n",
    "If our prediction is greater than 0.5, we predict class 1, otherwise we predict class 0.\n",
    "\n",
    "## 2.1 Extend to a Neural Network\n",
    "Now, in order to extend this concept to that of a neural network, the main thing we need to do is just add more logistic regression layers (i.e. neurons):\n",
    "\n",
    "<img src=\"images2/multilayer-neurons.png\" width=\"500\">\n",
    "\n",
    "We will be working mainly with 1 extra layer, but an arbitrary number can be added. In recent years, researchers have found amazing success with deeper networks, hence the term _deep learning_. The first step is of course to just add 1 layer, and our calculations are the exact same! \n",
    "\n",
    "> We multiply each input by its weight (linear combination), add the bias, and pass it through a sigmoid function. \n",
    "\n",
    "That is how we get each value at node $z$:\n",
    "\n",
    "$$z_j = \\sigma \\big(\\sum_i (W_{ij}x_j + b_j)\\big)$$\n",
    "\n",
    "Where in the above equation our $x$ inputs are indexed by $i$, and the $z$ nodes are indexed by $j$. Also, notice that our set of weights $w$ is now a matrix. This is because there needs to be a weight for each input-output pair. Hence, since we have 2 inputs and 3 outputs, we need there to be 6 weights in total. \n",
    "\n",
    "## 2.2 Nonlinearities\n",
    "We have already spoken about the fact that the main things that makes neural networks so powerful is that they are nonlinear classifiers. We have already gone over one of the most common nonlinear functions utilized in this architecture, the sigmoid:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "There are also several other nonlinearities that we will cover soon, such as _tanh_ and the _RELU_. \n",
    "\n",
    "## 2.3 Vectorization\n",
    "From an implementation standpoint, remember that it is faster to use the numpy matrix and vector operations, compared to using python for loops. Hence, we are going to treat our system layers as follows:\n",
    "\n",
    "> * $X$ is going to be a **_D dimensional vector_** (D is 2 in the above diagram)\n",
    "* $Z$ is going to be an **_M dimensional vector_** (M is 3 in the above diagram)\n",
    "\n",
    "This means that Z is going to look like: \n",
    "\n",
    "$$z_j = \\sigma \\big(\\sum_i (W_{ij}x_i) + b_j\\big)$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$z_j = \\sigma \\big(W_{j}^Tx) + b_j\\big)$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$z = \\sigma \\big(W^Tx) + b\\big)$$\n",
    "\n",
    "And $p(y \\mid x)$ will look like:\n",
    "\n",
    "$$p(y \\mid x) = \\sigma \\big(\\sum_j (v_{j}z_j) + c\\big)$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$p(y \\mid x) = \\sigma \\big(v^Tz + c\\big)$$\n",
    "\n",
    "## 2.4 Matrix Notation\n",
    "The above is pretty good, but we can do even better! In general, we have _many_ data points, and we want to consider more than one sample at a time. So, we can further vectorize our calculations, by using the full input matrix of data! \n",
    "\n",
    "This will be an $NxD$ matrix, where $N$ is the number of samples and $D$ is the dimensionality. Because we will then be calculating everything at the same time, $z$ will then be an $NxM$ matrix. The output $y$ will be an $Nx1$ matrix (for binary classification, for $k$ classes it will be $Nxk$. It is important that our weights all have the correct shape. Hence:\n",
    "\n",
    "> **Weights:**\n",
    "* $W$ is $DxM$\n",
    "* The first bias, $b$ is $Mx1$\n",
    "* The output weight $v$ is an $Mx1$ vector\n",
    "* The output bias $c$ is a scalar\n",
    "\n",
    "Visually, our current configuration can be interpreted as seen in the diagram below:\n",
    "\n",
    "<img src=\"images2/matrix-visualization-1.png\" width=\"600\">\n",
    "\n",
    "And after switching to a matrix representatino, we end up with the following equations:\n",
    "\n",
    "$$Z = \\sigma \\big(XW + b\\big) $$\n",
    "\n",
    "$$Y = \\sigma \\big(Zv + c\\big)$$\n",
    "\n",
    "## 2.5 Intuition Via the Dot Product\n",
    "Now, when just learning about Neural Networks, it is _very_ easy to become lost in a fog of matrices, arrows pointing from node to node, transposes, and so on. Something that I feel is absolutely crucial to really have a good understanding for what is going on here is having a good understanding of the _**dot product**_, and being able to keep it in mind as we get into more complex networks. \n",
    "\n",
    "I have a notebook that delves into the dot product, linear combinations, and linear transformations in great detail in my _linear algebra_ section of the Math Appendix. I highly recommend reviewing it if you are a bit hazy on the dot product, but I will give a quick overview here as well. \n",
    "\n",
    "Consider for a moment that our $x$ input is a _vector_. We have been representing it simply via an array of numbers, but we can think about it _geometrically_ as well. This is of course only possible in 3 dimensions, since humans cannot think in any dimensionality larger than that, but for now let's assume that $x$ is a 3 dimensional vector. Now, if we have another 3 dimensional vector, $v$, and take the dot product of it with $x$:\n",
    "\n",
    "$$x \\cdot v$$\n",
    "\n",
    "We are essentially getting a measure of _similarity_. The closer these two vectors are in direction and magnitude, the _larger their dot product_. If they are perpendicular to eachother, their dot product is 0. If they are pointing in opposite directions, their dot product is negative. \n",
    "\n",
    "Now, in both the our logistic regression unit (neuron) and neural networks, we are taking dot products! (In both cases, we start with simply taking a linear combination, but after transitioning to matrix vector notation we are able to refer to it as a dot product). This can be seen clearly in the diagram below:\n",
    "\n",
    "<img src=\"images2/dot-product-1.png\" width=\"500\">\n",
    "\n",
    "Let's walk through what is happening above:\n",
    "> * We have an $X$ matrix as our input, but for now we are only focusing on the first row (which we can think of as a row vector)\n",
    "* This row vector of $X$, seen in the green box, then multiplied (via the dot product) by the first column in the weight matrix $W$. Technically this is matrix vector multiplication of a 1x3 matrix by a 3x1 vector, but based on _duality_ if functions in the same fashion as the dot product. \n",
    "* This dot product results in _a single value_, $z_1$, the first node in the matrix $Z$. \n",
    "\n",
    "Now that the mechanics of the above process have been outlined, consider this: We know that the weights will eventually need to be _learned_. We don't know how to do this yet, but we know that that is coming up. Assume that they have been learned. What does the first column of $W$ represent? It is a learned representation of the first node of $Z$! From a high level we can view this as follows:\n",
    "\n",
    "> * Input a single sample, and vector $x$. \n",
    "* In order to calculate the value of $z_1$ (the first node in $Z$), take the dot product with the first column of the weight matrix, $W$.\n",
    "* $z_1$ will be large when the vector $x$ and the first column of $W$ are _similar_! And it will be negative when they are very different (geometrically). \n",
    "* Hence, a way to think about the learned weights of the first column in $W$ is that they are meant to be a vector represenation of $z_1$. \n",
    "\n",
    "This is a pretty powerful way of viewing something that can so frequently be incredibly abstract. If it is still slightly confusing, the diagram below should help:\n",
    "\n",
    "<img src=\"images2/dot-product-2.png\" width=\"500\">\n",
    "\n",
    "Above we have the _hidden layer_ to the _output layer/node_. This is no different than the simple logistic neuron, and we can even think of the hidden layer as the input layer if it helps simplify things. Here, again we have 3 nodes in the first row of $Z$, that are being multiplied via $V$. This will give us a prediction. Now, if the result of the dot product between the first row of $Z$ and $V$ is large, then the value of our output node (which is representing class 1), will be large after going through the sigmoid function, and we will predict class 1. _If_ the dot product ends up being negative, then we will predict class 0. \n",
    "\n",
    "This means that our $V$ matrix can be thought of as a learned representation of class 1! Now, this of course loses meaning as we add layers and different components to our network, particularly the nonlinearities introduced by the sigmoid/RELU. But it is still a useful intuiton to have, making use of the underlying geometry of the system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Softmax\n",
    "It is now time to dig into handling the scenario when your output has more than 2 categories. Previously, we have only discussed binary classification, for which there _are_ many real world applications. However, there are countless situations where you want to be able to classify $k$ classes. For instance, consider the famous MNIST data set, or if you wanted to tag Faces, cars, friends, etc. \n",
    "\n",
    "## 3.1 Extending the Logistic Unit\n",
    "Let's quickly talk about the best way to extend the logistic unit so that it can handle more than 2 classes. Recall that when we have two classes, we only need 1 output: \n",
    "\n",
    "$$P(Y=1\\mid X)$$\n",
    "\n",
    "This is because:\n",
    "\n",
    "$$P(Y=0 \\mid X) = 1 - P(Y=1 \\mid X)$$\n",
    "\n",
    "This can be visualized below:\n",
    "\n",
    "<img src=\"images2/binary-classification.png\" width=\"400\">\n",
    "\n",
    "Where again we can think of the weight vector $W$ as a learned representation of the $Y=1$ vector! When we take the dot product with our input vector $x$, if the input vector is similar, we have a high value and classify the vector as 1. \n",
    "\n",
    "However, there is another way that we could go about doing this.\n",
    "\n",
    "### 3.1.1 Binary Output With Two Output Nodes\n",
    "Instead of having a single output node, we could have two nodes, and then just normalize them so that they sum to 1. For instace, the diagram below:\n",
    "\n",
    "<img src=\"images2/soft-max.png\" width=\"400\">\n",
    "\n",
    "We have $a_1$ and $a_2$, where they are just:\n",
    "\n",
    "$$a_1 = w_1^T x$$\n",
    "\n",
    "$$a_2 = w_2^T x$$\n",
    "\n",
    "We can exponentiate both of the output values ($a_1$ and $a_2$) in order to make sure they are both positive. \n",
    "\n",
    "$$P(Y=1 \\mid X) = \\frac{e^{a_1}}{e^{a_1} + e^{a_2}}$$\n",
    "\n",
    "$$P(Y=0 \\mid X) = \\frac{e^{a_2}}{e^{a_1} + e^{a_2}}$$\n",
    "\n",
    "Notice in the diagram that our weights are no longer a vector as they were in logistic regression. Since every input node has to be connected with every output node, and we have $D$ input nodes and 2 output nodes, the total number of weights is $2D$ and they are stored in a $Dx2$ matrix. \n",
    "\n",
    "## 3.2 Softmax for $K$ Classes\n",
    "We can easily extend this framework to $K$ classes:\n",
    "\n",
    "$$P(Y=k \\mid X) = \\frac{e^{a_k}}{Z}$$\n",
    "\n",
    "$$Z = e^{a_1} + e^{a_2} + ... + e^{a_k}$$\n",
    "\n",
    "And $W$ is a $DxK$ matrix. Note, the output which we have been calling $a$ is usually the activation.\n",
    "\n",
    "## 3.3 Sigmoid vs. Softmax\n",
    "Let's take a moment to determine when the sigmod and softmax are equivalent. We can begin by restating what we already know:\n",
    "\n",
    "$$P(Y=1 \\mid X) = \\frac{e^{w_1^Tx}}{e^{w_1^Tx} + e^{w_0^Tx}}$$\n",
    "\n",
    "$$P(Y=0 \\mid X) = \\frac{e^{w_0^Tx}}{e^{w_1^Tx} + e^{w_0^Tx}}$$\n",
    "\n",
    "Now, let's divide the first equation, $P(Y=1 \\mid X)$, by $e^{w_1^Tx}$:\n",
    "\n",
    "$$P(Y=1 \\mid X) = \\frac{1}{1 + e^{(w_0 - w_1)^Tx}}$$\n",
    "\n",
    "And we can see that this has the same form as the sigmoid! Now, in this case $w = w_1 - w_0$. What does this mean? This means that having 2 weights is actually redundant when there are only two classes. However, from a software design perspective, it is safer to always just use softmax, since that will allow it to be more general! It will be able to handle the sigmoid case where $K=2$, but also any other value of $K$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Feed-Forward Example\n",
    "We are now ready to make our first predictions via a Neural Network! The way that this is done is via the _**Feedforward method**_. Here is the scenario we will go over:\n",
    "\n",
    "> * We have training data that consists of inputs, $X$, and targets, $Y$.\n",
    "* We will have 3 training examples, $N = 3$, and two dimensions, $D=2$\n",
    "* Our $X$ examples will look like:\n",
    "$$X = \\begin{bmatrix}\n",
    "    0 & 3.5 \\\\\n",
    "    1 & 2 \\\\\n",
    "    1 & 0.5 \n",
    "\\end{bmatrix}$$\n",
    "* And our $Y$ output classes will be:\n",
    "$$Y = \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    0  \n",
    "\\end{bmatrix}$$\n",
    "* First we are going to consider binary classification, and then move on to softmax.\n",
    "* Our targets, $Y$, are not used during prediction, only in training. That will be covered in the next section. \n",
    "* The rows of $X$ represent different samples (in this case they are people), and we are trying to predict whether or not each person will succeed at studying deep learning. The first column of $X$ represents if the subject has a technical degree, and the second column represents the number of hours they spend studying per day. Note, has a technical degree is a binary feature, 1 if true, 0 if false\n",
    "\n",
    "## 4.1 Neural Network Structure\n",
    "The structure of our neural network will be as follows:\n",
    "\n",
    "<img src=\"images2/feedforward-1.png\" width=\"500\">\n",
    "\n",
    "Where we have an input of size two, because we have two input features. The output size is fixed to 1 because we have 1 binary output prediction. The output is going to predict the probability that the subject succeeds at deep learning. Our hidden layer is of size 3. You may ask, why is that? Well, it is rather advanced, and we will cover the reasoning for that later on. \n",
    "\n",
    "We can now add in the weights for our network:\n",
    "\n",
    "<img src=\"images2/feedforward-2.png\" width=\"500\">\n",
    "\n",
    "Which can be written as a matrix of size $2x3$:\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "    W_{11} & W_{12} & W_{13}\\\\\n",
    "    W_{21} & W_{22} & W_{23}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And for our case, our weights will have the values:\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "    0.5 & 0.1 & -0.3\\\\\n",
    "    0.7 & -0.3 & 0.2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The first row of $W$ corresponds to the weights that relate $x_1$ to the hidden layer, and the second row of $W$ corresponds to the weights that relate $x_2$ to the hidden layer. \n",
    "\n",
    "Our bias terms can be held in a $3x1$ vector $b$:\n",
    "\n",
    "$$b = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "With values:\n",
    "\n",
    "$$b = \\begin{bmatrix}\n",
    "   0.4 \\\\\n",
    "   0.1 \\\\\n",
    "   0 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We can now look at the hidden weights as well, where $V$ maps each node in the hidden layer to every output node (currently, we have 1 output node). Hence, with 3 hidden layer nodes, we need 3 weights: \n",
    "\n",
    "$$V = \\begin{bmatrix}\n",
    "    V_1 \\\\\n",
    "    V_2 \\\\\n",
    "    V_3 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$V = \\begin{bmatrix}\n",
    "    0.8 \\\\\n",
    "    0.1 \\\\\n",
    "    -0.1 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We can also specify a bias term, which we need on per output node. In this case, with a only one output node, our bias term can be a scalar:\n",
    "\n",
    "#### $$c = 0.2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2 Calculations $\\rightarrow$ Via Summations\n",
    "Alright, now that we have all of our values defined and sorted out, we can take the first sample and walk through a calculation trying to solve whether the person will succeed at deep learning or not. In other words, we are going to find $p(y =1 \\mid x)$!\n",
    "\n",
    "At first, we are going to calculate this using summations, and also use the tanh activation function. We could have also used the sigmoid-we will discuss our choice later. \n",
    "\n",
    "### 4.2.1 Calculate $z_1$, the activation at hidden layer unit 1\n",
    "From a visual perspective, the activation at unit 1 in the hidden layer looks like: \n",
    "\n",
    "<img src=\"images2/nn-predict-1.png\" width=\"500\">\n",
    "\n",
    "The activation is a linear combination of all input features values, $x$, and their corresponding weights. This linear combination is then used as the input to the tanh function, and the result is our activation:\n",
    "\n",
    "#### $$z_1 = tanh(x_1w_{11} + x_2w_{21} + 1*b_1)$$\n",
    "\n",
    "#### $$z_1 = tanh(0.5*0 + 0.7*3.5 + 0.4) = tanh(2.85) = 0.993$$\n",
    "\n",
    "### 4.2.2 Calculate $z_2$, the activation at hidden layer unit 2\n",
    "This same process can be followed for $z_2$:\n",
    "\n",
    "<img src=\"images2/nn-predict-2.png\" width=\"500\">\n",
    "\n",
    "#### $$z_2 = tanh(x_1w_{12} + x_2w_{22} + 1*b_2)$$\n",
    "\n",
    "#### $$z_2 = tanh(0.1*0 - 0.3*3.5 + 0.1) = tanh(-0.95) = -0.740$$\n",
    "\n",
    "### 4.2.3 Calculate $z_3$, the activation at hidden layer unit 3\n",
    "And again, for $z_3$:\n",
    "\n",
    "<img src=\"images2/nn-predict-3.png\" width=\"500\">\n",
    "\n",
    "#### $$z_3 = tanh(x_1w_{13} + x_2w_{23} + 1*b_3)$$\n",
    "\n",
    "#### $$z_3 = tanh(-0.3*0 + 0.2*3.5 +0) = tanh(0.7) = 0.604$$\n",
    "\n",
    "### 4.2.4 Hidden layer activations, $z_i$, with summations\n",
    "It is important to note that we can rewrite our equations for the activations in the hidden layer via summations, like so:\n",
    "\n",
    "#### $$z_1 = tanh\\big(\\sum_{i=1}^2 W_{i1}x_i + b_1\\big)$$\n",
    "\n",
    "#### $$z_2 = tanh\\big(\\sum_{i=1}^2 W_{i2}x_i + b_2\\big)$$\n",
    "\n",
    "#### $$z_3 = tanh\\big(\\sum_{i=1}^2 W_{i3}x_i + b_3\\big)$$\n",
    "\n",
    "Now, at this point, we have determined all of the activations, $z$, for the hidden layer! \n",
    "\n",
    "### 4.2.5 Calculate Output, $p(Y=1 \\mid X)$\n",
    "At this point we would like to calculate the output, $p(Y=1 \\mid X)$. In other to do this, we need to multiply our hidden layer by $V$, add $c$, and run that value through the sigmoid function. Just as with the input to hidden weights, each of the Vs also connects 1 node in the hidden layer to the output node. So, visually our calculation looks like:\n",
    "\n",
    "<img src=\"images2/nn-predict-4.png\" width=\"500\">\n",
    "\n",
    "And it can be written mathematically as:\n",
    "\n",
    "#### $$p(Y=1 \\mid X) = \\sigma \\big( V_1z_1 + V_2z_2 + V_3z_3 + c\\big)$$\n",
    "\n",
    "Or, via summations as:\n",
    "\n",
    "#### $$p(Y=1 \\mid X) = \\sigma \\big(\\sum_{j=1}^3 V_jz_j + c \\big)$$\n",
    "\n",
    "Which we can then plug in the known values for those variables:\n",
    "\n",
    "#### $$p(Y=1 \\mid X) = \\sigma \\big(0.8*0.993 + 0.1*-0.74 - 0.1*0.604 + 0.2\\big)$$\n",
    "\n",
    "#### $$p(Y=1 \\mid X) = \\sigma \\big(0.86\\big) = 0.70$$\n",
    "\n",
    "### 4.2.6 Our Prediction\n",
    "After the succession of calculations, we finally arrive at our prediction:\n",
    "\n",
    "> Our subject has a 70% chance of succeeding at deep learning!\n",
    "\n",
    "## 4.3 Calculations $\\rightarrow$ Vectorized\n",
    "Now, that was a lot of work to calculated just one sample, which is why we want to vectorize our operations, via _numpy_.\n",
    "\n",
    "### 4.3.1 Hidden layer, $z$, vectorized\n",
    "We can start by vectorizing the hidden layer. Recall, the hidden layer is determined via:\n",
    "\n",
    "\n",
    "#### $$z_1 = tanh\\big(\\sum_{i=1}^2 W_{i1}x_i + b_1\\big)$$\n",
    "\n",
    "#### $$z_2 = tanh\\big(\\sum_{i=1}^2 W_{i2}x_i + b_2\\big)$$\n",
    "\n",
    "#### $$z_3 = tanh\\big(\\sum_{i=1}^2 W_{i3}x_i + b_3\\big)$$\n",
    "\n",
    "Which, we can rewrite as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    z_1\\\\\n",
    "    z_2  \\\\\n",
    "    z_2 \n",
    "\\end{bmatrix} = \n",
    "tanh \\Big (\n",
    "\\begin{bmatrix}\n",
    "    W_{11} & W_{21} \\\\\n",
    "    W_{12} & W_{22} \\\\\n",
    "    W_{13} & W_{23} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3\n",
    "\\end{bmatrix}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Which can be written compactly as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    z_1\\\\\n",
    "    z_2  \\\\\n",
    "    z_2 \n",
    "\\end{bmatrix} = \n",
    "tanh (W^Tx+b)\n",
    "$$\n",
    "\n",
    "Now, just to prove that that matrix notation is equivalent, we can quickly perform the calculation via numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x vector: \n",
      " [[0. ]\n",
      " [3.5]]\n",
      "W Matrix: \n",
      " [[ 0.5  0.1 -0.3]\n",
      " [ 0.7 -0.3  0.2]]\n",
      "b vector: \n",
      " [[0.4]\n",
      " [0.1]\n",
      " [0. ]]\n",
      "W transpose matrix: \n",
      " [[ 0.5  0.7]\n",
      " [ 0.1 -0.3]\n",
      " [-0.3  0.2]]\n",
      "z vector: \n",
      " [[ 0.99333039]\n",
      " [-0.73978305]\n",
      " [ 0.60436778]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([0,3.5]).reshape(2,1)\n",
    "W = np.array([[0.5, 0.1, -0.3],[0.7, -0.3, 0.2]])\n",
    "b = np.array([0.4, 0.1, 0]).reshape(3,1)\n",
    "W_transpose = np.transpose(W)\n",
    "print('x vector: \\n', x)\n",
    "print('W Matrix: \\n', W)\n",
    "print('b vector: \\n', b)\n",
    "print('W transpose matrix: \\n', W_transpose)\n",
    "\n",
    "# Perform Calculation\n",
    "z = np.tanh(W_transpose.dot(x) + b)\n",
    "print('z vector: \\n', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have arrived at our expected answer! \n",
    "\n",
    "### 4.3.2 Output layer vectorized\n",
    "Of course, we can do the exact same thing for the hidden to output layer! Our calculation with summations had the form:\n",
    "\n",
    "#### $$p(Y=1 \\mid X) = \\sigma \\big(\\sum_{j=1}^3 V_jz_j + c \\big)$$\n",
    "\n",
    "And we can vectorize it so that it looks like:\n",
    "\n",
    "#### $$p(Y=1 \\mid X) = \\sigma \\big(V^Tz +c)$$\n",
    "\n",
    "Which, when written out fully will look like:\n",
    "\n",
    "$$\n",
    "p(Y=1 \\mid X) = \n",
    "\\sigma \\Big (\n",
    "\\begin{bmatrix}\n",
    "    V_1 & V_2 & V_3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    z_1 \\\\\n",
    "    z_2 \\\\\n",
    "    z_3\n",
    "\\end{bmatrix} +\n",
    "0.2\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Again, we can verify that this operation is equivalent to our summation calculation from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70271272]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# V starts as column vector by convention\n",
    "V = np.array([0.8, 0.1, -0.1]).reshape(3,1)  \n",
    "V_transpose = np.transpose(V)\n",
    "c = 0.2\n",
    "\n",
    "# Define sigmoid\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Calculation\n",
    "sigmoid(V_transpose.dot(z) + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we arrive out the exact output we were expecting! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Multiple Sample $\\rightarrow$ Matrixization\n",
    "At this point we have effectively vectorized the prediction of a single sample. However, in general we may have _a set of samples_ that we want to classify. We are able to take our vectorization process one step further and create an _input matrix_, $X$, of samples. \n",
    "\n",
    "Keep in mind that when we are dealing with a single input sample, $x$, it is a column vector; however, once we switch to deal with a matrix of input samples, by convention they are held as row vectors. In other words, when dealing with a single sample we have:\n",
    "\n",
    "$$x = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    3.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And when dealing with multiple samples:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    0 & 3.5 \\\\\n",
    "    1 & 2 \\\\\n",
    "    1 & 0.5 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "So, we want to find a way to pass in _an entire input matrix_, $X$, to our prediction process! This means that we will no longer be calculating a $z$ hidden layer vector, but a $Z$ hidden layer matrix. Likewise, we will not longer be determining a single output, but rather a vector holding a prediction for each corresponding row of the input matrix.\n",
    "\n",
    "To do this, we must first consider that our input samples are now _rows_ in $X$:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    0 & 3.5 \\\\\n",
    "    1 & 2 \\\\\n",
    "    1 & 0.5 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "In order to ensure that our hidden layer is still calculated correctly we must make an update. Previously, we had been taking the tranpose of our weight matrix:\n",
    "\n",
    "$$W^T =\n",
    "\\begin{bmatrix}\n",
    "    W_{11} & W_{21} \\\\\n",
    "    W_{12} & W_{22} \\\\\n",
    "    W_{13} & W_{23} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "However, that is no longer what we want to do, since our samples are rows of $X$, and not single column vectors. Hence, we want to keep $W$ in its original form:\n",
    "\n",
    "$$W = \\begin{bmatrix}\n",
    "    W_{11} & W_{12} & W_{13}\\\\\n",
    "    W_{21} & W_{22} & W_{23}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And have it be multipled (linearly transformed) by $X$:\n",
    "\n",
    "$$XW =\n",
    "\\begin{bmatrix}\n",
    "    0 & 3.5 \\\\\n",
    "    1 & 2 \\\\\n",
    "    1 & 0.5 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    W_{11} & W_{12} & W_{13}\\\\\n",
    "    W_{21} & W_{22} & W_{23}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, the next step is determining if/how our bias vector, $b$, must be updated. $b$ is of size $Mx1$, and looks like:\n",
    "\n",
    "$$b = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "However, the operation $XW$, leaves us with an $NxM$ matrix. Because matrix addition is element wise, when you add two matrices together they must be the same size. So, doing $XW + b$ does not work anymore! \n",
    "\n",
    "We know that what should happen is that the bias, $b$, should be applied to all $N$ input samples. For that to happen, we need to transpose $b$ so that it is a horizontal row vector, and then repeat it $N$ times. That would give us an $NxM$ matrix, and we would have two $NxM$ matrices that we could add together!\n",
    "\n",
    "$$\n",
    "XW_{NxM} + \\begin{bmatrix}\n",
    "    \\leftarrow b^T  \\rightarrow \\\\\n",
    "    \\leftarrow b^T \\rightarrow \\\\\n",
    "    \\leftarrow b^T \\rightarrow \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Luckily, numpy does now enforce that we do this explicitly and will automatically apply the process when call: \n",
    "\n",
    "```\n",
    "X.dot(W) + b\n",
    "```\n",
    "\n",
    "So, our matrixization currently has gotten us to:\n",
    "\n",
    "#### $$XW + b$$\n",
    "\n",
    "Because the sigmoid and $tanh$ functions are applied element-wise, you pass scalars, vectors, and matrices in legally. This means that we can pass the entire expression above into $tanh$, like so:\n",
    "\n",
    "#### $$tanh(XW + b)$$\n",
    "\n",
    "The resulting matrix from this process, $Z$ (which holds the activations of all hidden units for each input sample), has a shape $NxM$:\n",
    "\n",
    "$$tanh(XW + b) = Z = \n",
    "\\begin{bmatrix}\n",
    "    Z_{11} & Z_{12} & Z_{13}\\\\\n",
    "    Z_{21} & Z_{22} & Z_{23}\\\\\n",
    "    Z_{21} & Z_{22} & Z_{23}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we recall, $V$ has a shape $Mx1$. Hence, in order to keep consistency and ensure our calculations remain accurate, we must update our multiplcation operation to be:\n",
    "\n",
    "$$ ZV = \n",
    "\\begin{bmatrix}\n",
    "    Z_{11} & Z_{12} & Z_{13}\\\\\n",
    "    Z_{21} & Z_{22} & Z_{23}\\\\\n",
    "    Z_{21} & Z_{22} & Z_{23}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    V_1 \\\\\n",
    "    V_2 \\\\\n",
    "    V_3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Meaning that our entire calculation can be updated as:\n",
    "\n",
    "#### $$tanh(XW + b)V$$\n",
    "\n",
    "And, since $c$ is a scalar, it will be added to all entries of the above matrix, and we can add it normally:\n",
    "\n",
    "#### $$tanh(XW + b)V + c$$\n",
    "\n",
    "Because the sigmoid will be applied elementwise, we can pass then entire result from the above equation through the sigmoid, yielding our final output vector! \n",
    "\n",
    "$$y = \n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    y_3 \n",
    "\\end{bmatrix} = \n",
    "\\sigma \\big( tanh(XW + b)V + c \\big) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Feed-Forward with Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
