{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In the past we have looked at both logistic regression and binary classification. There, we would collect some data, and then try to predict 1 of 2 possible labels. For example, if we were dealing with an e-commerce stie, we could collect _**time spent on site**_ and _**number of pages viewed**_, and then try to predict whether someone is going to buy something on the site. \n",
    "\n",
    "In this case, we only have 2 dimensions. We will plot the information, and then try to use a straight line to classify the classes (buy or not buy):\n",
    "\n",
    "$$\\sigma \\big( w_1*(\\text{time spent on site}) + w_2 (\\text{number pages viewed})\\big)$$\n",
    "\n",
    "If we are able to find a line that goes between the classes, they are _linearly seperable_. When we are dealing with data that is linearly seperable, logistic regression is fine, since it is a linear classifier. So, in 2 dimensions linearly seperable data can be separated via a line, in 3 dimensions a plane, and and 4+ dimensions a hyperplane. The point is, no matter how many dimensions we are dealing with, our decision boundary is going to be straight, not curved. \n",
    "\n",
    "## 1.1 Neural Networks Add Non-linearity\n",
    "Now, as we get into the realm of Neural Networks, things begin to change. We can have non-linearly seperable variables, such as: \n",
    "\n",
    "<img src=\"images2/nonlinear-data.png\" width=\"400\">\n",
    "\n",
    "Logistic regression would _not_ be appropriate for this, while neural networks would! Recall, a linear function has the form:\n",
    "\n",
    "$$w_1x_1 + w_2x_2+...+w_nx_n$$\n",
    "\n",
    "$$w^T x$$\n",
    "\n",
    "Where, just a reminder, in the vector notation $w_T x$, the weights are transposed because by convention they are stored as a column vector, but we need to be able to perform matrix vector multiplicaton (akin to the dot product in this case) with the input vector $x$. \n",
    "\n",
    "So, we can see that anything that cannot be simplified into $w^Tx$ is nonlinear. Now, $x^2$ and $x^3$ are both nonlinear, but neural networks are nonlinear in a _very specific way_. Neural Networks achieve nonlinearity by:\n",
    "\n",
    "> _Being a combination of multiple logistic regression units (neurons) put together._\n",
    "\n",
    "That is going to be the focus of this section; determining how we can build a nonlinear classifier (in this case a neural network), by combining logistic regression units (neurons). We will then use this nonlinear classifier to make _**predictions**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Logistic Regression $\\rightarrow$ Neural Networks\n",
    "We are now ready to start the transition from logistic regression to neural networks. Recall that logistic regression is a neuron, and we are going to be connecting many together to make a network of neurons. The most basic way to do this is the _**feed forward method**_. For logistic regression, we have a weight corresponding to every input:\n",
    "\n",
    "<img src=\"images2/logistic-reg-unit.png\" width=\"500\">\n",
    "\n",
    "This is seen clearly in the image above. We have two input features, $x_1$ and $x_2$, but of course there can be many more. Each input feature has a corresponding weight, $w_1$ and $w_2$. In order to determine the output $y$, we multiply each input by its weight, sum them all together, add a bias term, and put it through a sigmoid function:\n",
    "\n",
    "$$z = x_1w_1 + x_2w_2 + bw_0$$\n",
    "\n",
    "$$y = p(y \\mid x) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "$$prediction = round \\big( p(y \\mid x)\\big)$$\n",
    "\n",
    "If our prediction is greater than 0.5, we predict class 1, otherwise we predict class 0.\n",
    "\n",
    "## 2.1 Extend to a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
