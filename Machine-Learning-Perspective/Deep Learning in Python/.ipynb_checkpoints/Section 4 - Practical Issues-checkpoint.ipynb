{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Issues Overview\n",
    "Up until this point we have focused mainly on theory and the two main functions of a neural networks:\n",
    "1. How to **Predict**\n",
    "2. How to **Train**\n",
    "The question may arise, in practice, what else do we need to focus on? Well, we have mentioned that neural networks are nonlinear classifiers, which means that they can classify things when you can't draw a line between the two classes. Two famous examples of these are the **XOR** problem, and the **Donut Problem**. \n",
    "\n",
    "<img src=\"images/donut.png\">\n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/xor.png\">\n",
    "\n",
    "We have shown that **logistic regression** cannot solve those two problems unless you manually create new features. This can be time consuming and rather tedious, so ideally we would have an algorithm **learn** those features for us. However, the ability to learn these features automatically does not come for free. What you may have started to notice is that neural networks come with a variety of options: how many hidden units to use, how many hidden layers to use, the type of activation function, etc. These are **hyperparameters** and choosing them is not trivial, so we will discuss the process for this as well. \n",
    "\n",
    "This section is all about applying what we have already learned about neural networks in order to gain deeper insight into how they work, and what kind of baggage they come with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "# 1. XOR Problem in Code \n",
    "Let's now go through the XOR problem in code, using a Neural Network to solve. This should be slightly simpler than the Neural Networks that we have been working with so far since this is a binary classification problem. We can start with our imports as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our forward function. Note, since this is only **binary classification** we are basically just doing two sigmoids in a row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z = 1 / (1 + np.exp( -(X.dot(W1) + b1) ))   # output of hidden layer\n",
    "    activation = Z.dot(W2) + b2                 # activation at output layer\n",
    "    Y = 1 / (1 + np.exp(-activation))           # output at output layer\n",
    "    return Y, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predict function will no longer be using `argmax`, since we are not using the `softmax`, instead it will just use `round` (anything less than 0.5 is classified as 0, anything great than 0.5 is classified as 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2): \n",
    "    Y, _ = forward(X, W1, b1, W2, b2)\n",
    "    return np.round(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivatives are mostly the same. Note that `Z` is an **(N x M)** matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_w2(Z, T, Y):\n",
    "    return (T - Y).dot(Z)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    return (T - Y).sum()\n",
    "\n",
    "def derivative_w1(X, Z, T, Y, W2):\n",
    "    dZ = np.outer(T-Y, W2) * (1 - Z * Z)\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "def derivative_b1(Z, T, Y, W2):\n",
    "    dZ = np.outer(T - Y, W2) * (1 - Z * Z)\n",
    "    return dZ.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note, regarding the derivative for $W_1$ and $b_1$, we utilize the `np.outer` function instead of the `np.dot` that we had been using in section 3. This may at first seem unexpected, but remember that in reality with our given configuration we are really just doing two sigmoids in a row. Let's look at our data for the XOR problem and this will make more sense. In the process we will make it a little more clear the architecture that we are working with.\n",
    "\n",
    "We can see that our **X** has dimensions (4 x 2), meaning that we have 4 training examples, and each example has 2 dimenions. So our neural network is going to have an input layer containing **2 nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])    # we can see that X is (4 x 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **Y** output has dimensions (4 x 1), meaning our output layer has **1 node**, containing our prediction value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([0, 1, 1, 0])                        # Y is (4 x 1)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **W1** matrix has the shape (2 x 5), where each row holds the weights that map from a specific node in the input layer, to all of the nodes in the output layer. The appendix of section 3 training a neural network has a good visualization of this. Keep in mind, the shape of the **W1** tells us that we have **5 nodes** in our hidden layer, and hence the shape of **Z** should be (4 x 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93163268,  0.91146108,  0.50436259,  0.78321648,  1.31769257],\n",
       "       [-1.10142287, -1.21103262, -1.32615   , -1.35481673,  1.29434063]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = np.random.randn(2, 5)                        # W1 is (2 x 5)\n",
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **W2** matrix has shape (5 x 1), which is a column vector. This is because we need **W2** to be a size that brings us back to **1 node** for the output layer. It should be kept in mind that our final network architecture is 2 input nodes, 5 hidden layer nodes, and 1 output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = np.random.randn(5)                           # W2 is (5 x 1)\n",
    "W2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate **Z** via the dot product to ensure that it is in fact (4 x 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93163268,  0.91146108,  0.50436259,  0.78321648,  1.31769257],\n",
       "       [-1.10142287, -1.21103262, -1.32615   , -1.35481673,  1.29434063]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-1.10142287, -1.21103262, -1.32615   , -1.35481673,  1.29434063],\n",
       "       [ 0.93163268,  0.91146108,  0.50436259,  0.78321648,  1.31769257],\n",
       "       [-0.16979019, -0.29957154, -0.8217874 , -0.57160025,  2.61203321]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = X.dot(W1)                                     # Z is (4 x 5) \n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, **Z** is (4 x 5) and each row represents the values of the nodes (5 nodes total) at **Z** for a given training example (4 training examples total). Remember: the first column of **X** holds the value for **input node 1** at a specific training example, and the second column holds the value for **input node 2**. When we perform the dot product, we start by taking the 1st column of **W2**, which holds the weight mapping from **input node 1** to **hidden node 1**, and the weight mapping from **input node 2** to **hidden node 1**, and applying it to **X**. The result is the dot product, and the first column of our **Z** matrix is the result.\n",
    "\n",
    "Now, with that said lets move on to the calculation of our prediction **pY**. We should be expecting a shape of (4 x 1), since we want it to match our targets, **Y**, since we want to have 1 prediction for each of the 4 examples. We will apply the column vector **W2** to each row of **Z**, performing the dot product and ending up with a (4 x 1) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0446353 ,  0.81210986, -0.57296673,  0.26730075, -0.52501608])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2              # quickly visualizing W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       , -2.4159301,  0.9419845, -1.4739456])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY = Z.dot(W2)                                    # pY is (4 x 1)\n",
    "pY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gone through the **forward prediction** step (keep in mind that no activation was applied since this was just to demonstrate the shapes of the matrices and element wise operations would not affect that, so we didn't worry about them for now), the question still remains: why was the `np.outer` function used instead of `np.dot` in our derivative for **W2**? Well, lets look at the shape of (Y - pY) and W2 again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 3.4159301, 0.0580155, 1.4739456])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y - pY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0446353 ,  0.81210986, -0.57296673,  0.26730075, -0.52501608])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So (Y - pY) has a shape (4 x 1), and W2 has a shape (5 x 1). Now, we need to end up with a shape that matches **Z**, (4 x 5). If you look at the numpy tutorial, you can see that if you have two vectors, the first a column vector and the second a row vector, if they have the same size you can perform the dot product. However, because these dimensions do not match up, the outer product allows us to apply the term representing our error (Y - pY) to each weight value in W2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -0.        ,  0.        , -0.        ],\n",
       "       [ 3.56840117,  2.77411052, -1.9572143 ,  0.91308068, -1.79341822],\n",
       "       [ 0.06060504,  0.04711496, -0.03324095,  0.01550759, -0.03045907],\n",
       "       [ 1.53973561,  1.19700576, -0.84452179,  0.39398676, -0.77384514]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ = np.outer((Y - pY), W2)\n",
    "dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, our shape is (4 x 5) which matches **Z**. Now lets get back to the remaining functions that we needed to define. The **cost function** is next on our list. In this case we are using the **cross entropy**, which is equivalent to the **negative log likelihood**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood(T, Y):\n",
    "    return np.sum(T * np.log(Y) + (1 - T)*np.log(1 - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to actually test the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_xor():\n",
    "    # create XOR data\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    Y = np.array([0, 1, 1, 0])\n",
    "    W1 = np.random.randn(2, 5)\n",
    "    b1 = np.zeros(5)\n",
    "    W2 = np.random.randn(5)\n",
    "    b2 = 0\n",
    "    \n",
    "    LL = []                       # keep track of log-likelihoods\n",
    "    \n",
    "    # set our hyper parameters\n",
    "    learning_rate = 1e-2\n",
    "    regularization = 0.\n",
    "    last_error_rate = None\n",
    "    \n",
    "    for i in range(30000):\n",
    "        pY, Z = forward(X, W1, b1, W2, b2)\n",
    "        ll = get_log_likelihood(Y, pY)\n",
    "        prediction = predict(X, W1, b1, W2, b2)\n",
    "        er = np.mean(prediction != Y)\n",
    "\n",
    "        LL.append(ll)\n",
    "        W2 += learning_rate * (derivative_w2(Z, Y, pY) - regularization * W2)\n",
    "        b2 += learning_rate * (derivative_b2(Y, pY) - regularization * b2)\n",
    "        W1 += learning_rate * (derivative_w1(X, Z, Y, pY, W2) - regularization * W1)\n",
    "        b1 += learning_rate * (derivative_b1(Z, Y, pY, W2) - regularization * b1)\n",
    "        if i % 1000 == 0:\n",
    "            print(ll)\n",
    "\n",
    "    print(\"final classification rate:\", np.mean(prediction == Y))\n",
    "    plt.plot(LL)\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.4058365611452395\n",
      "-2.462538393784536\n",
      "-1.7262100187732923\n",
      "-1.3721060642312712\n",
      "-1.0856836220696875\n",
      "-0.9105181312830212\n",
      "-0.7958018660122069\n",
      "-0.7031967562796098\n",
      "-0.6248922290146786\n",
      "-0.5598826202731333\n",
      "-0.5071566863376937\n",
      "-0.46460595307630437\n",
      "-0.43092943630015706\n",
      "-0.40578397671486854\n",
      "-0.3878127281614125\n",
      "-0.37119257248625037\n",
      "-0.34613508076318533\n",
      "-0.31418097643709714\n",
      "-0.2848552838883065\n",
      "-0.26104101815620917\n",
      "-0.2417888124981114\n",
      "-0.22584032122971961\n",
      "-0.2123145531869809\n",
      "-0.20064753711804298\n",
      "-0.1904746408603161\n",
      "-0.18155457256053462\n",
      "-0.17372623119141228\n",
      "-0.16688161965716874\n",
      "-0.16094305222566485\n",
      "-0.15583812904433347\n",
      "('final classification rate:', 1.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_xor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write out the code that will run through the donut problem. Some things to note are that we explicitly define 8 hidden units and use that to define the size of our weight matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    # sigmoid\n",
    "    # Z = 1 / (1 + np.exp( -(X.dot(W1) + b1) ))\n",
    "\n",
    "    # tanh\n",
    "    # Z = np.tanh(X.dot(W1) + b1)\n",
    "\n",
    "    # relu\n",
    "    Z = X.dot(W1) + b1\n",
    "    Z = Z * (Z > 0)\n",
    "\n",
    "    activation = Z.dot(W2) + b2\n",
    "    Y = 1 / (1 + np.exp(-activation))\n",
    "    return Y, Z\n",
    "\n",
    "\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    Y, _ = forward(X, W1, b1, W2, b2)\n",
    "    return np.round(Y)\n",
    "\n",
    "\n",
    "def derivative_w2(Z, T, Y):\n",
    "    # Z is (N, M)\n",
    "    return (T - Y).dot(Z)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    return (T - Y).sum()\n",
    "\n",
    "\n",
    "def derivative_w1(X, Z, T, Y, W2):\n",
    "    # dZ = np.outer(T-Y, W2) * Z * (1 - Z) # this is for sigmoid activation\n",
    "    # dZ = np.outer(T-Y, W2) * (1 - Z * Z) # this is for tanh activation\n",
    "    dZ = np.outer(T-Y, W2) * (Z > 0) # this is for relu activation\n",
    "    return X.T.dot(dZ)\n",
    "\n",
    "\n",
    "def derivative_b1(Z, T, Y, W2):\n",
    "    # dZ = np.outer(T-Y, W2) * Z * (1 - Z) # this is for sigmoid activation\n",
    "    # dZ = np.outer(T-Y, W2) * (1 - Z * Z) # this is for tanh activation\n",
    "    dZ = np.outer(T-Y, W2) * (Z > 0) # this is for relu activation\n",
    "    return dZ.sum(axis=0)\n",
    "\n",
    "\n",
    "def get_log_likelihood(T, Y):\n",
    "    return np.sum(T*np.log(Y) + (1-T)*np.log(1-Y))\n",
    "\n",
    "\n",
    "\n",
    "def test_donut():\n",
    "    # donut example\n",
    "    N = 1000\n",
    "    R_inner = 5\n",
    "    R_outer = 10\n",
    "\n",
    "    # distance from origin is radius + random normal\n",
    "    # angle theta is uniformly distributed between (0, 2pi)\n",
    "    R1 = np.random.randn(N//2) + R_inner\n",
    "    theta = 2*np.pi*np.random.random(N//2)\n",
    "    X_inner = np.concatenate([[R1 * np.cos(theta)], [R1 * np.sin(theta)]]).T\n",
    "\n",
    "    R2 = np.random.randn(N//2) + R_outer\n",
    "    theta = 2*np.pi*np.random.random(N//2)\n",
    "    X_outer = np.concatenate([[R2 * np.cos(theta)], [R2 * np.sin(theta)]]).T\n",
    "\n",
    "    X = np.concatenate([ X_inner, X_outer ])\n",
    "    Y = np.array([0]*(N//2) + [1]*(N//2))\n",
    "\n",
    "    n_hidden = 8\n",
    "    W1 = np.random.randn(2, n_hidden)\n",
    "    b1 = np.random.randn(n_hidden)\n",
    "    W2 = np.random.randn(n_hidden)\n",
    "    b2 = np.random.randn(1)\n",
    "    LL = [] # keep track of log-likelihoods\n",
    "    learning_rate = 0.00005\n",
    "    regularization = 0.2\n",
    "    last_error_rate = None\n",
    "    for i in range(3000):\n",
    "        pY, Z = forward(X, W1, b1, W2, b2)\n",
    "        ll = get_log_likelihood(Y, pY)\n",
    "        prediction = predict(X, W1, b1, W2, b2)\n",
    "        er = np.abs(prediction - Y).mean()\n",
    "        LL.append(ll)\n",
    "        W2 += learning_rate * (derivative_w2(Z, Y, pY) - regularization * W2)\n",
    "        b2 += learning_rate * (derivative_b2(Y, pY) - regularization * b2)\n",
    "        W1 += learning_rate * (derivative_w1(X, Z, Y, pY, W2) - regularization * W1)\n",
    "        b1 += learning_rate * (derivative_b1(Z, Y, pY, W2) - regularization * b1)\n",
    "        if i % 300 == 0:\n",
    "            print(\"i:\", i, \"ll:\", ll, \"classification rate:\", 1 - er)\n",
    "    plt.plot(LL)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i:', 0, 'll:', -1483.7333411539862, 'classification rate:', 0.53)\n",
      "('i:', 300, 'll:', -304.78580984193627, 'classification rate:', 0.969)\n",
      "('i:', 600, 'll:', -201.77128229516939, 'classification rate:', 0.987)\n",
      "('i:', 900, 'll:', -142.32115738962548, 'classification rate:', 0.991)\n",
      "('i:', 1200, 'll:', -106.2165370236086, 'classification rate:', 0.993)\n",
      "('i:', 1500, 'll:', -83.82530683283403, 'classification rate:', 0.992)\n",
      "('i:', 1800, 'll:', -68.82802128419723, 'classification rate:', 0.993)\n",
      "('i:', 2100, 'll:', -57.979539772564465, 'classification rate:', 0.993)\n",
      "('i:', 2400, 'll:', -50.08223521073666, 'classification rate:', 0.993)\n",
      "('i:', 2700, 'll:', -43.97950461278596, 'classification rate:', 0.993)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_donut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "# 2. Neural Networks for Regression\n",
    "We are now going to take a brief look at how to utilize Neural Networks in a Regression problem. In general, they are used for classification, for a variety of reasons (most of the famous problems they have solved have been classification, they often perform better in that setting, etc). However, they can sometimes be very effective in a regression scenario, and are even utilized in certain state of the art applications such as deep reinforcement learning. \n",
    "\n",
    "## 2.1 Neural Network Binary Classification Structure \n",
    "\n",
    "<img src=\"images/logistic-regression-arch.png\">\n",
    "\n",
    "For Binary classification we will have a stack of nonlinear hidden layers, followed by a logistic regression at the end. We can call this logistic regression, because this takes on the exact form of logistic regression; aka a linear transformation, $Wx+b$, and a sigmoid (shown in the circle on the right). For the hidden layers, we are not going to call them logistic regression since they still take in a linear transformation, but the activation function is not necessarily a sigmoid, it could be **tanh**, **relu**, etc. Still, the concept of stacking layers of neurons is important to consider since that is what lead us to these new structures in the first place. \n",
    "\n",
    "<br></br>\n",
    "## 2.2 Neural Network Multi-class Classification Structure \n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/multi-class-arch.png\">\n",
    "\n",
    "For multi-class classification we have nearly the same thing, only now the output layer has more units, one corresponding to each class. However, notice that everything up until the last layer is the same as before. We have stacks on **linear transformations**, along with **nonlinear activation functions**, and at the end we have a **multi-class logistic regression**, which is a **linear transformation** and a **softmax**. \n",
    "\n",
    "<br></br>\n",
    "## 2.3 Neural Network Regression Structure \n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/regression-arch.png\">\n",
    "\n",
    "Finally we have the regression scenario. Again, notice how everything up until the last layer is the same as before. We have stacks on **linear transformations**, along with **nonlinear activation functions**, and at the end we have **only** a **linear transformation**, and this is of course exactly **linear regression**. \n",
    "\n",
    "<br></br>\n",
    "## 2.4 The Lesson\n",
    "We know that from prior studies that if we just consider *just* linear models, we have **logistic regression** for classification, and **linear regression** for **regression**. In a sense we can consider everything up until the last layer just the feature extraction/transformation, where we are transforming the input into something different. After that, we are just doing what we already know: logistic regression for classification, and linear regression for regression. \n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/high-level-arch.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
