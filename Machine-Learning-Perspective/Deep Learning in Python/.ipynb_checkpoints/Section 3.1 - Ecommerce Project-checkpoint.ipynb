{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Project - Logistic Regression\n",
    "We are now going to look at training **logistic regression** with softmax. In my other walk throughs on logistic regression, we weren't looking at multiclass classification (just binary) so we had just been using the **sigmoid** function, and not softmax. This will give us the chance to see how logistic regression performs compared to a neural network. Remember, for logistic regression our architecture looks like:\n",
    "\n",
    "<img src=\"images/logistic-neuron.png\">\n",
    "\n",
    "The only difference now is that instead of using the sigmoid at at the logistic neuron, we are going to use the softmax as we are performing multiclass classification. \n",
    "\n",
    "<img src=\"images/logistic-regression-softmax.png\">\n",
    "\n",
    "We can start with our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets define our `get_data` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.read_csv('data/ecommerce_data.csv')\n",
    "    data = df.as_matrix()\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:,:-1]\n",
    "    Y = data[:,-1].astype(np.int32)\n",
    "\n",
    "    # one-hot encode the categorical data\n",
    "    N, D = X.shape\n",
    "    X2 = np.zeros((N, D+3))\n",
    "    X2[:,0:(D-1)] = X[:,0:(D-1)] # non-categorical\n",
    "\n",
    "    # one-hot\n",
    "    for n in range(N):\n",
    "      t = int(X[n,D-1])\n",
    "      X2[n,t+D-1] = 1\n",
    "    X = X2\n",
    "\n",
    "    # split train and test\n",
    "    Xtrain = X[:-100]\n",
    "    Ytrain = Y[:-100]\n",
    "    Xtest = X[-100:]\n",
    "    Ytest = Y[-100:]\n",
    "\n",
    "    # normalize columns 1 and 2\n",
    "    for i in (1, 2):\n",
    "        m = Xtrain[:,i].mean()\n",
    "        s = Xtrain[:,i].std()\n",
    "        Xtrain[:,i] = (Xtrain[:,i] - m) / s\n",
    "        Xtest[:,i] = (Xtest[:,i] - m) / s\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need a function to get the indicator matrix from the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2indicator(y, K):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N,K))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1 \n",
    "    return ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can can get our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest = get_data()\n",
    "D = Xtrain.shape[1]\n",
    "K = len(set(Ytrain) | set(Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert our `Y` data into an indicator matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_ind = y2indicator(Ytrain, K)\n",
    "Ytest_ind = y2indicator(Ytest, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is shape (400 x 4) because we have have four classes that make up Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now randomly initialize our weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(D, K)\n",
    "b = np.zeros(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can define our `softmax` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    expA = np.exp(a)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets define our forward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    return softmax(X.dot(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(P_Y_given_X):\n",
    "    return np.argmax(P_Y_given_X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classification rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y == P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the **Cross Entropy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(T, pY):\n",
    "    return -np.mean(T * np.log(pY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all of that, we can enter our train loop. Note: our weight update rule in gradient descent is based on the rule we derived last lecture: `Z.T.dot(T - Y)`. Only in the case of logistic regression, we only have and input and output layer, no hidden layer, so in this case `Z` is going to be `X`. We are also now going to be performing gradient descent, not ascent, so we have a minus in our update, and we are subtracting the targets from the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_costs = []\n",
    "test_costs = []\n",
    "learning_rate = 0.001\n",
    "for i in range(10000):\n",
    "    pYtrain = forward(Xtrain, W, b)        # find the predicitons for y train\n",
    "    pYtest = forward(Xtest, W, b)        # find the predicitons for y test\n",
    "    \n",
    "    ctrain = cross_entropy(Ytrain_ind, pYtrain)     # Ytrain_ind === targets in this case\n",
    "    ctest = cross_entropy(Ytest_ind, pYtest)\n",
    "    train_costs.append(ctrain)\n",
    "    test_costs.append(ctest)\n",
    "    \n",
    "    # now we can perform gradient descent\n",
    "    W -= learning_rate * Xtrain.T.dot(pYtrain - Ytrain_ind)  \n",
    "    b -= learning_rate * (pYtrain - Ytrain_ind).sum(axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print i, ctrain, ctest\n",
    "print(\"Final training classification rate: \", classification_rate(Ytrain, predict(pYtrain)))\n",
    "print(\"Final testing classification rate: \", classification_rate(Ytest, predict(pYtest)))\n",
    "\n",
    "legend1, = plt.plot(train_costs, label='train cost')\n",
    "legend2, = plt.plot(test_costs, label='test cost')\n",
    "plt.legend([legend1, legend2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
