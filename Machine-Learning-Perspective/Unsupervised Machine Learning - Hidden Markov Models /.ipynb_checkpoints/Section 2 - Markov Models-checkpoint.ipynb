{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Markov Property\n",
    "What is the markov property? \n",
    "> The markov property is when tomorrows weather only depends on todays weather, but not yesterdays weather. It is when the next word in the sentence only depends on the previous word in a sentence, but not on any other words. It is when tomorrows stock price only depends on today's stock price. \n",
    "\n",
    "The markov property is also called the markov assumption, because can clearly see that this is a strong assumption! We are essentially throwing away all historical data, except for the most recent. \n",
    "\n",
    "In more general terms, what we have been referring to as `weather`, or `stock price`, can be thought of as a **state**. We say that the markov assumption is that the current state only depends on the previous state, or that the next state only depends on the current state. Another way of saying this is that the distribution of the state at time $t$, only depends on the distribution of the state at time $t-1$:\n",
    "### $$State \\; at \\; time \\; t \\rightarrow s(t)$$\n",
    "### $$p\\Big(s(t) \\; | \\; s(t-1), s(t-2),...,s(0)\\Big) = p\\Big(s(t) \\; | \\; s(t-1) \\Big)$$\n",
    "\n",
    "Why do we want to do this? Well, the goal here is to model the joint probability; in other words *the probability of seeing an entire specific sequence*. \n",
    "\n",
    "In other words, if we had 4 states, then without the markov property our joint probability would look like:\n",
    "\n",
    "#### $$p(s4, s3, s2, s1) = p(s4 \\;|\\; s3, s2, s1)p(s3, s2, s1)$$\n",
    "#### $$p(s4, s3, s2, s1) = p(s4 \\;|\\; s3, s2, s1)p(s3\\;|\\; s2, s1)p(s2, s1)$$\n",
    "#### $$p(s4, s3, s2, s1) = p(s4 \\;|\\; s3, s2, s1)p(s3\\;|\\; s2, s1)p(s2\\;|\\; s1)p(s1)$$\n",
    "\n",
    "On the other hand, if we do use the markov property, it looks like: \n",
    "#### $$p(s4, s3, s2, s1) = p(s4 \\;|\\; s3)p(s3 \\;|\\; s2)p(s2 \\;|\\;s1)p(s1)$$\n",
    "\n",
    "\n",
    "Think about the sequence: $s1, s2, s3$. How often does that occur? If it doesn't happen that often, how can we accurately measure $p(s4 \\;|\\; s3, s2, s1)$?\n",
    "\n",
    "## Concrete Example\n",
    "Notice, that if we were to take the most general form, where the state at time $t$ depends on all of the previous states, it would be really hard to measure these probability distributions. For example, think of a wikipedia article where we try to predict the next word. Let's say it is a 1000 word wikipedia article. Now, we have to get the distribution of the 1000th word given the last 999 words:\n",
    "\n",
    "### $$p(w_{1000} \\; | \\; w_{999}, ...,w_1)$$\n",
    "\n",
    "However, we can imagine that this is the only wikipedia article with that exact same sequence of 999 words. So our probability measure is 1 out of 1. That is a 1 sample measurement and not a great language model. \n",
    "\n",
    "Conversely, if you are thinking of the begining of the article, and you only have 1 previous word, say that the word is \"the\", then you have an enormous number of possible next words. So, you may want to do something like train on only sequences of 3 or 4 words. In this case, your current word would depend only on the two or 3 previous words.\n",
    "\n",
    "### $$p(w(t) \\; | \\; w(t-1), w(t-2))$$\n",
    "\n",
    "## Generalize\n",
    "To generalize the above concept, we have:\n",
    "\n",
    "\n",
    "#### $$First \\; order \\; Markov \\rightarrow p\\Big(s(t) \\; | \\; s(t-1)\\Big)$$\n",
    "#### $$Second \\; order \\; Markov \\rightarrow p\\Big(s(t) \\; | \\; s(t-1), s(t-2)\\Big)$$\n",
    "#### $$Third \\; order \\; Markov \\rightarrow p\\Big(s(t) \\; | \\; s(t-1), s(t-2), s(t-3)\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# Markov Models \n",
    "\n",
    "<img src=\"images/state-transitions.png\">\n",
    "\n",
    "We can now return to our weather example. Let's say that we are trying to model the three states: rain, sun, and cloud. We can represent this model as a graph, where each node is a state, and each edge is the probability of going from one state to the next state. You can already see that this is a first order markov model, because the weights only depend on the current state, and it only effects the next state. Once we go to the next state, that becomes the current state, and the state after that only depends on the current state. Notice, that it is possible for the next state to be the same as the current state; i.e. we can have two rainy days in a row. \n",
    "\n",
    "So, this means that we have 3 weights for each of the 3 states. How many weights are there in total for M states? Well, since each state can go to each state (including itself), there are always $M^2$ weights. These weights are generally stored in an $M x M$ matrix called $A$, which is called the state transition matrix, or simply the transition probabilities. \n",
    "\n",
    "Any element $A(i, j)$ represents the probability of going to state $j$ from state $i$:\n",
    "\n",
    "#### $$A(i, j) = p \\Big(s(t)=j \\; \\big\\vert \\; s(t-1)=i\\Big)$$\n",
    "\n",
    "So, what constraints does this place on $A$? Well, since whenever we are in state $i$ we must go to one of the three states, and there are no other possibilities. Therefore, the sum of row $A(i, :)$ must sum to 1. This applies for $i = 1..M$.\n",
    "\n",
    "## Starting Position \n",
    "Another important thing to keep in mind for markov models is where do you start? For example, we are trying to model the first word of a sentence, or how much money you can buy a house for. We must know how to model $p \\Big( s(0)\\Big)$ This is what is called the **initial state distribution**. It is represented by an $M$ dimensional vector called $\\pi$, and we usually represent it by a $1 x M$ dimensional row vector (unlike what we usually are working with, column vectors). Of course, all of the values must sum to one, since it is the probability that we start in each of the $M$ states. \n",
    "\n",
    "## Simple Examples\n",
    "With these concepts in hand, we are ready to start using the model. We can ask questions like \"*what is the probability of this sequence*\"?:\n",
    "> \"Sun, sun, rain, cloud\"\n",
    "\n",
    "So, the probability of that is just equal to: \n",
    "\n",
    "#### $$p(sun, sun, rain, cloud) = p(cloud \\; | \\; rain)p(rain \\; | \\; sun)p(sun \\; | \\; sun)p(sun)$$\n",
    "\n",
    "#### $$p(sun, sun, rain, cloud) = 0.2 * 0.05 * 0.8 * p(sun)$$\n",
    "\n",
    "Note that $p(sun)$ is not given in the diagram. In general this looks like:\n",
    "\n",
    "#### $$p\\big(s(0),...,s(T)\\big) = \\pi\\big(s(0)\\big))\\prod_{t=1..T}p\\big(s(t) \\; | \\; s(t-1)\\big)$$\n",
    "\n",
    "Which in english just means the probability of state 0, times the product of the rest of the state transition probabilities. This allows us to ask questions like:\n",
    "\n",
    "> \"Which sequence is more probable in this model?\"\n",
    "\n",
    "Meaning, if we are given two sequences, we can determine which one is more likely to happen. \n",
    "\n",
    "## Training a Markov Model\n",
    "One question that you make have is: how do we train a markov model? This is very simple if we use **maximum likelihood**. For example, suppose we have the following 3 sentences as training data:\n",
    "> \"I like dogs\"<br>\n",
    "\"I like cats\"<br>\n",
    "\"I love kangaroos\"<br>\n",
    "\n",
    "We can treat each word as a state, so we have 6 states: \n",
    "> \"0 = I\"<br>\n",
    "\"1 = like\"<br>\n",
    "\"2 = love\"<br>\n",
    "\"3 = dogs\"<br>\n",
    "\"4 = cats\"<br>\n",
    "\"5 = kangaroos\"<br>\n",
    "\n",
    "We can give these each indexes in a vector, so they take the numbers 0-5 inclusive. If we use maximum likelihood, then our initial state distribution is just 100% probability for starting with the word \"I\", since all sentences state with \"I\". This means $\\pi$ is:\n",
    "\n",
    "#### $$\\pi = [1, 0, 0, 0, 0, 0]$$\n",
    "#### $$\\pi(\"I\") = 1$$\n",
    "\n",
    "Next, if the current word is \"I\" then there are two possibilities for the second word: \"like\" and \"love\". So:\n",
    "\n",
    "#### $$p\\big(like \\; | \\; I\\big) = \\frac{2}{3}$$\n",
    "#### $$p\\big(love \\; | \\; I\\big) = \\frac{1}{3}$$\n",
    " \n",
    "Finally, we have: \n",
    "#### $$p\\big(dogs \\; | \\; like\\big) = \\big(cats \\; | \\; like\\big) = \\frac{1}{2}$$\n",
    "#### $$p\\big(kangaroos \\; | \\; love\\big) = 1$$\n",
    "\n",
    "All other state transition probabilities are 0.\n",
    "\n",
    "## Smoothing\n",
    "Realize that in the english language we have over 1 million words. So, that is going to be a very large vocabulary. Even with lots of data, we may not be able to capture every possible sentence. This means that some things will have a probability of 0 that should have a chance of occuring. So, sometimes instead of maximum likelihood we use **smoothed estimates**. \n",
    "\n",
    "Our non-smoothed scenario would look like: \n",
    "#### $$A(i, j) = \\frac{count(i \\rightarrow j)}{count(i)}$$\n",
    "\n",
    "In the case of **epsilon smoothing**, we have:\n",
    "#### $$A(i, j) = \\frac{c(i \\rightarrow j) + \\epsilon}{c(i) + \\epsilon V}$$\n",
    "\n",
    "In the above, V is the vocabulary size. When epsilon is 1, we called this **add 1 smoothing**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# The Math of Markov Chains\n",
    "We are now going to look at markov chains, which are really just markov models. When we are talking about markov chains, we are generally just talking about a **discrete time stochastic process**. \n",
    "\n",
    "Think about the following question:\n",
    "> \"*What is the probability of a sunny day 5 days from now?*\"\n",
    "\n",
    "Well, we can calculate this simply using the probabilities. First, we can consider the probability of it being sunny tomorrow. That is just $p(sun(1)$, or p(sun) at time t = 1. Now, that is just:\n",
    "\n",
    "#### $$p(sun(1)) = \\frac{p(sun(1), sun(0)) + p(sun(1), rain(0)) + p(sun(1), cloud(0))}{marginalizer}$$\n",
    "\n",
    "We can then use **Bayes rule** which would give us the probability in terms of the conditionals, and the initial state distribution. This is just the transition matrix $A$ and $\\pi$ which defines the markov model. \n",
    "\n",
    "#### $$p(sun(1)) = p(sun(1) | sun(0))\\pi(sun) + p(sun(1) | rain(0))\\pi(rain) + p(sun(1) | cloud(0))\\pi(cloud)$$\n",
    "\n",
    "In general, you multiply $\\pi$ by the matrix $A$, giving you another row vector, with the new state distribution at time $t=1$. \n",
    "\n",
    "#### $$p(s(1)) = \\pi A$$\n",
    "\n",
    "Next you multiply by the new state distribution at time $t=2$:\n",
    "\n",
    "#### $$p(s(2)) = \\pi AA = \\pi A^2$$\n",
    "\n",
    "And so on:\n",
    "\n",
    "#### $$p(s(t)) = \\pi A^t$$\n",
    "\n",
    "## Stationary Distribution\n",
    "Suppose we start off in the state distribution of $p(s)$, and then multiply by $A$ and still end up with a state distribution of $p(s)$. This is known as a stationary distribution because no matter how many times we transition form this state distribution, we still end up with the same state distribution. \n",
    "\n",
    "How can we find a stationary distribution? You may recognize this as the eigenvector/value problem-specifically the case where the eigenvalue is 1. Recall that eigenvector are not unique, so after finding the eigenvector for which the eigenvalue is 1, you must normalize it so that it sums to 1. \n",
    "\n",
    "Keep in mind that this is not the normal eigenvalue problem that you are used to seeing because the $\\pi$ vector is by convention a row vector. Generally it looks like:\n",
    "\n",
    "#### $$Av = \\lambda v$$\n",
    "\n",
    "However, in our case we need to solve the eigenvalue problem for the transpose of $A$, not $A$ itself. \n",
    "\n",
    "## Limiting Distribution\n",
    "Another thing we can ask is: \n",
    "> \"*What state do we expect to end up in?*\"\n",
    "\n",
    "We can think of this as the final state distribution, or the state distribution at time $t= \\infty$:\n",
    "\n",
    "#### $$\\pi_{\\infty} = \\pi A^{\\infty}$$\n",
    "\n",
    "This is essentially just taking $\\pi$ and multiplying it by $A$ an infinite number of times. Another way of thinking of this is that no matter how many times I transition after this point, I still expect to have the same state probability distribution, known as the **equilibrium distribution** or **limiting distribution**, because it is the state distribution that you settle into after a very long time. \n",
    "\n",
    "#### $$A^{\\infty}A = A^{\\infty}$$\n",
    "#### $$\\pi_{\\infty} = \\pi_{\\infty}A$$\n",
    "\n",
    "By definition, if we take the equilibrium distribution, multiple by $A$ and get the same distribution, then it is a **stationary distribution**. So, all equilibrium distributions are state distributions. However, it should be known that all state distributions are not equilibrium distributions. \n",
    "\n",
    "So, what does the equilibrium distribution tell us? Suppose we have:\n",
    "\n",
    "#### $$\\pi_{\\infty}(sun, rain, cloud) = (\\frac{5}{10}, \\frac{1}{10}, \\frac{4}{10})$$\n",
    "\n",
    "This means that in the long run, if we measured 1000 days, we would expect 500 to be sunny, 100 to be rainy, and 400 to be cloudy. This allows us to gather statistics over time, even though a random process is time variant. This could not be done with stock prices! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
