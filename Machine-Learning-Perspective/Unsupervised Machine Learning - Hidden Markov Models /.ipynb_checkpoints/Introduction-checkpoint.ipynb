{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook is going to cover **hidden markov models**, which are used for modeling sequences of data. Sequences appear everywhere, from stock prices, to language, credit scoring, webpage visits.\n",
    "\n",
    "Often, we may be dealing with sequences in machine learning and we don't even realize it; or we may even ignore the fact that it came from a sequence. For instance, the following sentence: \n",
    "\n",
    "> \"Like and cats dogs I\"\n",
    "\n",
    "Clearly, this model does not make any sense. This is what happens when you use a model such as bag of words. The fact that it becomes much harder to tell what a sentence means when you take away the time aspect, tells you that there is a lot of information carried there. The original sentence was:\n",
    "\n",
    "> \"I like cats and dogs\"\n",
    "\n",
    "This may be relatively easy to decode on your own, but you can imagine that this gets much harder as the sentence gets longer. \n",
    "\n",
    "## 1.1 Outline\n",
    "\n",
    "1. We will start by looking at the most **basic markov model**, with no hidden portion. These are very useful for modeling sequences as we will see. We will talk about the mathematical properties of the markov model, and go through a ton of examples so we can see how they are used. Google's PageRank algorithm is based on markov models. So, despite being based on old technology, markov models are still very useful and relevant today. \n",
    "\n",
    "2. We will also talk about how to model language, and how to analyze web visitor data, so you can fix problems like high bounce rate. \n",
    "\n",
    "3. Next, we will look at the **hidden markov model**. This will be very complex mathematically, but the first section should prepare you. We will look at the three basic problems in hidden markov modeling:\n",
    "    * Predicting the probability of a sequence\n",
    "    * Predicting the most likely sequence of hidden states given an observed sequence\n",
    "    * How to train a hidden markov model \n",
    "    * We will even go further and look at how this relates to deep learning by using gradient descent to train our HMM. Typically, the expectation maximization algorithm is used. We will do this too, but we will see how gradient descent makes this much easier. \n",
    "4. We will finally look at Hidden Markov Models for real-valued data, this is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
