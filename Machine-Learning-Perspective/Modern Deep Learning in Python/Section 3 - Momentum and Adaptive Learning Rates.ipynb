{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Momentum to Speed Up Training\n",
    "We will now take a look at one of the most effective methods at improving plain gradient descent, called **momentum**. This can be thought of as the 80% factor to improve your learning procedure. \n",
    "\n",
    "A way to think of this is as follows: Gradient descent without momentum requires a *force* or *push* each time we want to get the weights to move. In other words, each time we want to move, there has to a be a gradient so that we can move in the direction of the gradient. If we had **momentum**, we can imagine that our update could keep moving, even without the gradient being present. \n",
    "\n",
    "This can be thought of as pushing a box on ice vs. pushing a box on gravel. If we are pushing the box on gravel, the minute we stop applying force, the box will also stop moving. This is analogous to gradient descent without momentum. However, if we were pushing the box on ice we could and then let go and it would continue moving for a period of time, before stopping. This is analogous to gradient descent with momentum. Another way to phrase this is as follows:\n",
    "> With momentum included in our update, our weight vector will build up a velocity in any direction that has a *consistent gradient*.\n",
    "\n",
    "Let's put this into math. \n",
    "\n",
    "## 1.1 Gradient Descent, *without* Momentum\n",
    "Our update for $\\theta$ can be described as:\n",
    "#### $$\\theta_t \\leftarrow \\theta_{t-1} - \\eta g_{t-1}$$\n",
    "This says that $\\theta_t$ is equal to the previous value of $theta$, minus the learning rate, times the gradient $g_t$. From this we can see that if the gradient is 0, nothing will happen to $\\theta_t$. It just gets updated to it's old value and doesn't change. \n",
    "\n",
    "## 1.2 Gradient Descent, *with* Momentum\n",
    "Now let's say that we add in **momentum**. Note that the term momentum is used very loosely here, since it has nothing to do with actual physical momentum. What we do is create a new variable, $v$, which stands for the velocity. It is equal to $\\mu$ (the momentum term) times its old velocity, minus the learning rate times the gradient. Notice that now, the gradient only directly influences the velocity, which in turn has an effect on the position (our weight vector), $\\theta$. \n",
    "\n",
    "#### $$v_t \\leftarrow \\mu v_{t-1} - \\eta g_{t-1}$$\n",
    "\n",
    "This new term, $\\mu v_{t-1}$, gives us the ability to \"slide on ice\" if you will. In other words, it allows us to continue to move in the same direction that we were going before. Now, we talked about how if a box is sliding on ice, it will still stop eventually. That means that we are going to want our updated $v$ to be a fraction of the prior $v$, and hence $\\mu$ should be a fraction. Typical values of $\\mu$ are 0.9, 0.95, 0.99, etc. This means that without any $g$, the equation will still eventually \"slow down\". Our update rule for $\\theta_t$ then becomes:\n",
    "\n",
    "#### $$\\theta_t \\leftarrow \\theta_{t-1} + v_t$$\n",
    "\n",
    "Now, if we combine these two equations we can see that our total update rule is:\n",
    "\n",
    "#### $$\\theta_t \\leftarrow \\theta_{t-1} + \\mu v_{t-1} -\\eta g_{t-1} $$\n",
    "\n",
    "And we can see that if we set the momentum term, $\\mu$, equal to zero, we end up with the same update rule we originally had for gradient descent. \n",
    "\n",
    "## 1.3 The Effect of Momentum\n",
    "You may be wondering, what is the effect of using momentum? Well we can see below that by using momentum, the cost converges to its minimum value much faster. This significantly speeds up training! \n",
    "\n",
    "<img src=\"images/momentum.png\">\n",
    "\n",
    "From another perspective, we can think of a situation where we have unequal gradients in different directions. In the image below, we have a very large gradient that creates the valley (each side is very steep), and then in the other direction (the stream flowing down), it is a very small gradient. \n",
    "\n",
    "<img src=\"images/large-small-gradient.png\">\n",
    "\n",
    "For visualization purposes, lets assume we have 2 parameters to optimize: the vertical and horizontal parameter. The gradient in one direction is very steep, and the gradient in the other direction is very shallow. The idea is that if you don't have momentum, then you rely purely on the gradient, which points more in the steep direction than in the shallow direction-this is just a property of the gradient, it is the direction of steepest descent. Since this gradient vector points more in the steep direction, we are going to zigzag back and forth across the valley. That is a very inefficient way of reaching the minimum. \n",
    "\n",
    "<img src=\"images/contours-momentum.png\">\n",
    "\n",
    "Once we add momentum, however, things change. Because in the shallow direction, we move in the same direction every time, those velocities are going to accumulate, so we will have a portion of our old velocity, added to our new velocity to help us along in that direction. The result is that we get there faster by taking bigger steps in the shallow direction of the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "# 2. Nesterov Momentum\n",
    "Nesterov momentum was coined by **Y Nesterov** in 1983. It is described as:\n",
    "> \"A method for unconstrained convex minimization problem with rate of convergence O(1/$k^2$)\"\n",
    "\n",
    "The core idea is that when the current weight vector is at some position, lets say $w$, then looking at original momentum update from earlier, we know that the momentum term alone (ignoring the term with the gradient), is about to nudge the parameter vector by $\\mu v_{t-1}$. Therefore, if we are about to compute the gradient, we can treat the future approximate position of $w$, $w + \\mu v_{t-1}$, as a \"lookahead\" - as in this is a point in the vicinity of where we are going to end up. Hence, it makes sense to compute the gradient at the $w + \\mu v_{t-1}$, instead of the old/stale position $w$. \n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/nesterov-vs-normal.png\">\n",
    "\n",
    "The image above makes it clear that instead of evaluating the gradient at the current position of $w$, (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this \"looked-ahead\" position. Also, keep in mind that in the image above the blue vector is referring to our update to the velocity, $v_t$, and not to the update of $w_t$. \n",
    "\n",
    "Okay, so we have a basic idea of **nesterov momentum** now, but let's just try and reiterate from a few different perspectives, to help is sink in. So, instead of just using momentum to blindly keep going in the direction that we were already going, let's instead peak ahead, by taking a big jump in the direction of the previous velocity, and calculate the gradient from there. We can think of it as though you are gambling, and if you are going to gamble it is better to take a big jump and then make a correction, than to make a correction and then gamble. \n",
    "\n",
    "<img src=\"images/nesterov1.png\">\n",
    "\n",
    "So first we peak ahead, jumping in the direction of the previous velocity (accumulated gradient): \n",
    "\n",
    "<img src=\"images/nesterov2.png\">\n",
    "\n",
    "We then measure the gradient, and go downhill in the direction of the gradient. We use that gradient to update our velocity (accumulated gradient). In other words, we combine the big jump with our gradient to get the accumulated gradient. So in a way, its peaking ahead and then course correcting based on where we would have ended up. \n",
    "\n",
    "<img src=\"images/nesterov3.png\">\n",
    "\n",
    "We then take that accumulated gradient (first green vector), multiply by some momentum constant, $\\mu$, and then we take the next big jump in the direction of that accumulated gradient. Again, at the place where we end up (head of second brown vector), we measure the gradient, we go downhill (second red vector) to correct any errors we have made, and we get a new accumulated gradient (second green vector)\n",
    "\n",
    "We can see that the blue vectors represent where we would go if we were using standard momentum, where we first measure the gradient where it currently is (small blue vector), and it adds that to the brown vector, and ends up making a jump by the big blue vector (first brown vector plus small blue vector, i.e. the current gradient). The brown vector represents our peak ahead value. Notice that it is in the same direction as the blue vector. The red vector is the gradient at the peak ahead value. The green vector is just the vector of the brown vector and the red vector. \n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## 2.1 Nesterov Equations\n",
    "So, with the visuals discussed, what do the equations look like? First, we are going to use $w$ to represent our weights instead of $\\theta$. Also, the majority if this is looking at how we will update $v_t$, and the last step covers $w_t$. Now, lets start with the vector that represents the previous value of our weights, $w_{t-1}$, and the previous velocity, $v_{t-1}$:\n",
    "\n",
    "<img src=\"images/nesterov-eq-1.png\">\n",
    "\n",
    "Now, we have this jump ahead, which we can call $w'_{t-1}$. We can also think of it as just our previous weight position, plus the momentum step. It is in the same direction of our previous velocity vector (because remember, the first part of updating $v_t$ was the term $\\mu v_{t-1}$, but it is slightly smaller since the jump is scaled by $\\mu$:\n",
    "\n",
    "<img src=\"images/nesterov-eq-2.png\">\n",
    "\n",
    "#### $$look \\; ahead\\; value: \\; w'_{t-1} = w_{t-1} +\\mu v_{t-1}$$\n",
    "#### $$look \\; ahead\\; value: \\; w'_{t-1} = v_{t-1} +\\mu v_{t-1}$$\n",
    "\n",
    "The above equations are equivalent because both $w_{t-1}$ and $v_{t-1}$ both have the same position (head each vector). Also, note that as seen in the image above, the jump ahead is just the previous value of the velocity (or previous weight position, $w_{t-1}$), plus the momentum term multiplied by the previous velocity. Next, we calculate the gradient at this jump ahead point, and then use that to update $v$:\n",
    "\n",
    "<img src=\"images/nesterov-eq-3.png\">\n",
    "\n",
    "<img src=\"images/nesterov-eq-4.png\">\n",
    "\n",
    "#### $$v_t \\leftarrow \\mu v_{t-1} - \\eta \\nabla J(w'_{t-1})$$\n",
    "\n",
    "Which is equal to:\n",
    "\n",
    "#### $$v_t \\leftarrow \\mu v_{t-1} - \\eta \\nabla J(w_{t-1} +\\mu v_{t-1})$$\n",
    "\n",
    "And then the last step is to update $w_t$, the accumulated gradient, which is the same as it was for standard momentum:\n",
    "\n",
    "<img src=\"images/nesterov-eq-5.png\">\n",
    "\n",
    "#### $$w_t \\leftarrow w_{t-1} + v_t$$\n",
    "\n",
    "The main difference to note is that in the standard method we are taking the gradient from the current position of $w$, and also making our momentum step from the current position of $w$, whereas in the Nesterov method we first take our momentum step from the current position of $w$, then correct by taking the gradient from that position. For a great link that goes over this topic in more detail, checkout out the follow: http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## 2.3 Reformulation\n",
    "However, in practice, this is not how nesterov momentum is usually implemented. Instead, we will reformulate the equations. Let's try and express everything only in terms of $w'$, our lookahead value of $w$, and it is where we want to calculate the gradient from. So, we can define $w'_t$ and $w'_{t-1}$ using the same definition:\n",
    "### 2.3.1 Redefine in terms of $w'$\n",
    "#### $$w'_t = w_t + \\mu v_t$$\n",
    "#### $$w'_{t-1} = w_{t-1} + \\mu v_{t-1}$$\n",
    "\n",
    "In other words, these are the lookahead values of $w$ at two consecutive steps. The second equation is seen in the below: \n",
    "\n",
    "<img src=\"images/nesterov-eq-6.png\">\n",
    "\n",
    "So, remember, the look ahead value can just be thought of as the step in the direction of the previous velocity, from the current weight vector postion. Next lets recall our updates for $v$ and $w$.\n",
    "\n",
    "### 2.3.2 Recall updates for $v$ and $w$\n",
    "#### $$v_t = \\mu v_{t-1} - \\eta \\nabla J(w'_{t-1})$$\n",
    "#### $$w_t = w_{t-1} + v_t$$\n",
    "\n",
    "From here we can substitute $w'$ for $w$\n",
    "\n",
    "### 2.3.3 Substitute $w'$ for $w$\n",
    "#### $$w'_t - \\mu v_t = w'_{t-1} - \\mu v_{t-1} + v_t$$\n",
    "\n",
    "### 2.3.4 Combine like terms on the right\n",
    "#### $$w'_t = w'_{t-1} -\\mu v_{t-1} + (1 + \\mu)v_t$$\n",
    "\n",
    "### 2.3.5 Get rid of $v_t$ term \n",
    "We can get ride of the $v_t$ term on the right by replacing it with an expression in terms of $v_{t-1}$. \n",
    "#### $$w'_t = w'_{t-1} - \\mu v_{t-1} + (1+\\mu) \\Big[\\mu v_{t-1} - \\eta \\nabla J (w'_{t-1})\\Big]$$\n",
    "\n",
    "### 2.3.6 Combine like terms, arrive at Nesterov momentum update\n",
    "#### $$w'_t = w'_{t-1} + \\mu^2 v_{t-1} - (1+\\mu)\\eta \\nabla J (w'_{t-1})$$\n",
    "This equation is the one that you will most often see when people are discussing nesterov momentum. \n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## 2.4 Regular vs. Nesterov Momentum\n",
    "So, to quickly recap, the equation for **regular momentum** is: \n",
    "\n",
    "#### $$w_t = w_{t-1} + \\mu v_{t-1} - \\eta \\nabla J (w_{t-1})$$\n",
    "\n",
    "And **Nesterov Momentum** is defined as:\n",
    "\n",
    "#### $$w'_t = w'_{t-1} + \\mu^2 v_{t-1} - (1+ \\mu) \\eta \\nabla (w'_{t-1})$$\n",
    "\n",
    "We can see that they each have a similar form. There is a previous $w$ term, a previous velocity term, and a previous gradient term. And if we take the nesterov equation, and plug phrase our $w'_t$ update in terms of $v_t$, we can see that the equations have an even greater resemblance:\n",
    "\n",
    "#### $$v_t = \\mu v_{t-1} - \\eta \\nabla J (w'_{t-1})$$\n",
    "#### $$w'_t = w'_{t-1} + \\mu v_t - \\eta \\nabla J (w'_{t-1})$$\n",
    "\n",
    "The question may arise, why is this useful? What is the point of all of this algebraic manuipulation? Well, we have expressed the nesterov momentum update entirely in terms of all the look ahead $w'$s, so the actual non look ahead $w$s are never needed. We can just treat $w$, whatever it may be, as if they were the lookahead values. In other words, lets just drop the prime symbols!\n",
    "\n",
    "#### $$v_t = \\mu v_{t-1} - \\eta \\nabla J (w_{t-1})$$\n",
    "#### $$w_t = w_{t-1} + \\mu v_t - \\eta \\nabla J (w_{t-1})$$\n",
    "\n",
    "So we can see that similar to regular momentum, we just proceed in two steps. \n",
    "1. First we calculate the new $v$ from the old $v$ and the gradient\n",
    "2. Then we update $w$ using the new $v$\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## 2.5 Conclusion \n",
    "The final updates for **regular momentum** are:\n",
    "#### $$v_t \\leftarrow \\mu v_{t-1} - \\eta \\nabla J(w_{t-1})$$\n",
    "#### $$w_t \\leftarrow w_{t-1} + v_t$$\n",
    "\n",
    "And for **nesterov momentum**:\n",
    "#### $$v_t = \\mu v_{t-1} - \\eta \\nabla J (w_{t-1})$$\n",
    "#### $$w_t = w_{t-1} + \\mu v_t - \\eta \\nabla J (w_{t-1})$$\n",
    "\n",
    "So the main difference be clearly distinguished: The update rule for $w_t$ is slightly different. However, in practice the lost per iteration tends to be rather similar, so choosing between the two is not a huge deal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<br></br>\n",
    "# 3. Momentum in Code\n",
    "Let's now implement 3 different scenarios:\n",
    "1. Batch Gradient descent, no momentum\n",
    "2. Batch Gradient descent, with momentum\n",
    "3. Batch Gradient descent, with Nesterov Momentum\n",
    "\n",
    "We can start with our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import get_normalized_data, error_rate, cost, y2indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets quickly define our `forward` and derivative functions. Note that I have commented out the sigmoid code, but they can be performed with the sigmoid instead of the ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    # sigmoid\n",
    "    # Z = 1 / (1 + np.exp(-( X.dot(W1) + b1 )))\n",
    "\n",
    "    # relu\n",
    "    Z = X.dot(W1) + b1\n",
    "    Z[Z < 0] = 0\n",
    "\n",
    "    A = Z.dot(W2) + b2\n",
    "    expA = np.exp(A)\n",
    "    Y = expA / expA.sum(axis=1, keepdims=True)\n",
    "    return Y, Z\n",
    "\n",
    "def derivative_w2(Z, T, Y):\n",
    "    return Z.T.dot(Y - T)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    return (Y - T).sum(axis=0)\n",
    "\n",
    "def derivative_w1(X, Z, T, Y, W2):\n",
    "    # return X.T.dot( ( ( Y-T ).dot(W2.T) * ( Z*(1 - Z) ) ) ) # for sigmoid\n",
    "    return X.T.dot( ( ( Y-T ).dot(W2.T) * (Z > 0) ) ) # for relu\n",
    "\n",
    "def derivative_b1(Z, T, Y, W2):\n",
    "    # return (( Y-T ).dot(W2.T) * ( Z*(1 - Z) )).sum(axis=0) # for sigmoid\n",
    "    return (( Y-T ).dot(W2.T) * (Z > 0)).sum(axis=0) # for relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we have imported `get_normalized_data`, which means that we will be using the full 784 dimensionality data set. Now lets define our setup function, where the goal is to prep our data for the 3 scenarios discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20         # 20 iterations, 1 batch per iteration \n",
    "print_period = 10 \n",
    "\n",
    "X, Y = get_normalized_data()      # grab our normalized X and Y data\n",
    "lr = 0.00004                      # precomputed learning and reg rates\n",
    "reg = 0.01            \n",
    "\n",
    "# create train and test set (data is already shuffled), as well as one hot matrix\n",
    "Xtrain = X[:-1000,]          # grabbing everything up until the last 100\n",
    "Ytrain = Y[:-1000]           # grabbing last 1000, making it test set\n",
    "Xtest = X[-1000:,]\n",
    "Ytest = Y[-1000:]\n",
    "Ytrain_ind = y2indicator(Ytrain)      # turn targets into one hot encoded matrices\n",
    "Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "N, D = Xtrain.shape \n",
    "batch_sz = 500                        # set batch size to 500\n",
    "n_batches = N // batch_sz              # get number of batches\n",
    "\n",
    "M = 300                               # number hidden units, we found this in prev demo\n",
    "K = Ytrain_ind.shape[1]              # number of output classes \n",
    "\n",
    "# intialize weights to random small values \n",
    "W1 = np.random.randn(D, M) / np.sqrt(D)\n",
    "b1 = np.zeros(M)\n",
    "W2 = np.random.randn(M, K) / np.sqrt(M)\n",
    "b2 = np.zeros(K)\n",
    "\n",
    "# and let's save initial weights so we can compare all methods!\n",
    "W1_0 = W1.copy()\n",
    "b1_0 = b1.copy()\n",
    "W2_0 = W2.copy()\n",
    "b2_0 = b2.copy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Batch Gradient Descent, no Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=0, j=0: 2294.498623\n",
      "Error rate: 0.847\n",
      "Cost at iteration i=0, j=10: 1748.315398\n",
      "Error rate: 0.477\n",
      "Cost at iteration i=0, j=20: 1413.028825\n",
      "Error rate: 0.328\n",
      "Cost at iteration i=0, j=30: 1195.417416\n",
      "Error rate: 0.271\n",
      "Cost at iteration i=0, j=40: 1043.688699\n",
      "Error rate: 0.227\n",
      "Cost at iteration i=0, j=50: 929.306309\n",
      "Error rate: 0.204\n",
      "Cost at iteration i=0, j=60: 843.777746\n",
      "Error rate: 0.188\n",
      "Cost at iteration i=0, j=70: 775.493602\n",
      "Error rate: 0.175\n",
      "Cost at iteration i=0, j=80: 724.584081\n",
      "Error rate: 0.165\n",
      "Cost at iteration i=1, j=0: 714.305526\n",
      "Error rate: 0.159\n",
      "Cost at iteration i=1, j=10: 674.306474\n",
      "Error rate: 0.157\n",
      "Cost at iteration i=1, j=20: 639.139676\n",
      "Error rate: 0.148\n",
      "Cost at iteration i=1, j=30: 609.880513\n",
      "Error rate: 0.141\n",
      "Cost at iteration i=1, j=40: 583.694787\n",
      "Error rate: 0.141\n",
      "Cost at iteration i=1, j=50: 562.353288\n",
      "Error rate: 0.139\n",
      "Cost at iteration i=1, j=60: 542.412863\n",
      "Error rate: 0.14\n",
      "Cost at iteration i=1, j=70: 523.907600\n",
      "Error rate: 0.138\n",
      "Cost at iteration i=1, j=80: 510.060361\n",
      "Error rate: 0.133\n",
      "Cost at iteration i=2, j=0: 506.514873\n",
      "Error rate: 0.133\n",
      "Cost at iteration i=2, j=10: 494.730608\n",
      "Error rate: 0.13\n",
      "Cost at iteration i=2, j=20: 482.830822\n",
      "Error rate: 0.129\n",
      "Cost at iteration i=2, j=30: 472.193933\n",
      "Error rate: 0.124\n",
      "Cost at iteration i=2, j=40: 461.486455\n",
      "Error rate: 0.125\n",
      "Cost at iteration i=2, j=50: 453.264057\n",
      "Error rate: 0.12\n",
      "Cost at iteration i=2, j=60: 444.250866\n",
      "Error rate: 0.124\n",
      "Cost at iteration i=2, j=70: 435.142751\n",
      "Error rate: 0.122\n",
      "Cost at iteration i=2, j=80: 428.687355\n",
      "Error rate: 0.122\n",
      "Cost at iteration i=3, j=0: 426.670708\n",
      "Error rate: 0.122\n",
      "Cost at iteration i=3, j=10: 421.246961\n",
      "Error rate: 0.119\n",
      "Cost at iteration i=3, j=20: 415.589304\n",
      "Error rate: 0.12\n",
      "Cost at iteration i=3, j=30: 409.832241\n",
      "Error rate: 0.118\n",
      "Cost at iteration i=3, j=40: 403.899142\n",
      "Error rate: 0.117\n",
      "Cost at iteration i=3, j=50: 399.483518\n",
      "Error rate: 0.116\n",
      "Cost at iteration i=3, j=60: 394.138004\n",
      "Error rate: 0.116\n",
      "Cost at iteration i=3, j=70: 388.184084\n",
      "Error rate: 0.115\n",
      "Cost at iteration i=3, j=80: 384.305989\n",
      "Error rate: 0.116\n",
      "Cost at iteration i=4, j=0: 382.873856\n",
      "Error rate: 0.112\n",
      "Cost at iteration i=4, j=10: 379.742041\n",
      "Error rate: 0.114\n",
      "Cost at iteration i=4, j=20: 376.406819\n",
      "Error rate: 0.112\n",
      "Cost at iteration i=4, j=30: 372.637030\n",
      "Error rate: 0.11\n",
      "Cost at iteration i=4, j=40: 368.745917\n",
      "Error rate: 0.11\n",
      "Cost at iteration i=4, j=50: 365.938474\n",
      "Error rate: 0.11\n",
      "Cost at iteration i=4, j=60: 362.158688\n",
      "Error rate: 0.11\n",
      "Cost at iteration i=4, j=70: 357.710836\n",
      "Error rate: 0.111\n",
      "Cost at iteration i=4, j=80: 355.119271\n",
      "Error rate: 0.109\n",
      "Cost at iteration i=5, j=0: 353.980401\n",
      "Error rate: 0.108\n",
      "Cost at iteration i=5, j=10: 351.951338\n",
      "Error rate: 0.108\n",
      "Cost at iteration i=5, j=20: 349.757515\n",
      "Error rate: 0.108\n",
      "Cost at iteration i=5, j=30: 346.998172\n",
      "Error rate: 0.106\n",
      "Cost at iteration i=5, j=40: 344.213993\n",
      "Error rate: 0.104\n",
      "Cost at iteration i=5, j=50: 342.139193\n",
      "Error rate: 0.104\n",
      "Cost at iteration i=5, j=60: 339.239063\n",
      "Error rate: 0.103\n",
      "Cost at iteration i=5, j=70: 335.647992\n",
      "Error rate: 0.102\n",
      "Cost at iteration i=5, j=80: 333.818310\n",
      "Error rate: 0.102\n",
      "Cost at iteration i=6, j=0: 332.879434\n",
      "Error rate: 0.102\n",
      "Cost at iteration i=6, j=10: 331.469451\n",
      "Error rate: 0.103\n",
      "Cost at iteration i=6, j=20: 329.895787\n",
      "Error rate: 0.102\n",
      "Cost at iteration i=6, j=30: 327.734988\n",
      "Error rate: 0.101\n",
      "Cost at iteration i=6, j=40: 325.620488\n",
      "Error rate: 0.099\n",
      "Cost at iteration i=6, j=50: 323.949852\n",
      "Error rate: 0.101\n",
      "Cost at iteration i=6, j=60: 321.583520\n",
      "Error rate: 0.099\n",
      "Cost at iteration i=6, j=70: 318.550808\n",
      "Error rate: 0.099\n",
      "Cost at iteration i=6, j=80: 317.209915\n",
      "Error rate: 0.099\n",
      "Cost at iteration i=7, j=0: 316.413944\n",
      "Error rate: 0.1\n",
      "Cost at iteration i=7, j=10: 315.396264\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=7, j=20: 314.202422\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=7, j=30: 312.477507\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=7, j=40: 310.823557\n",
      "Error rate: 0.096\n",
      "Cost at iteration i=7, j=50: 309.443699\n",
      "Error rate: 0.095\n",
      "Cost at iteration i=7, j=60: 307.459584\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=7, j=70: 304.799722\n",
      "Error rate: 0.096\n",
      "Cost at iteration i=7, j=80: 303.793748\n",
      "Error rate: 0.092\n",
      "Cost at iteration i=8, j=0: 303.118136\n",
      "Error rate: 0.093\n",
      "Cost at iteration i=8, j=10: 302.368723\n",
      "Error rate: 0.091\n",
      "Cost at iteration i=8, j=20: 301.437907\n",
      "Error rate: 0.092\n",
      "Cost at iteration i=8, j=30: 300.003154\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=8, j=40: 298.690985\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=8, j=50: 297.538466\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=8, j=60: 295.810979\n",
      "Error rate: 0.091\n",
      "Cost at iteration i=8, j=70: 293.401512\n",
      "Error rate: 0.092\n",
      "Cost at iteration i=8, j=80: 292.630160\n",
      "Error rate: 0.091\n",
      "Cost at iteration i=9, j=0: 292.045939\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=9, j=10: 291.457304\n",
      "Error rate: 0.088\n",
      "Cost at iteration i=9, j=20: 290.702251\n",
      "Error rate: 0.088\n",
      "Cost at iteration i=9, j=30: 289.469744\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=9, j=40: 288.397900\n",
      "Error rate: 0.088\n",
      "Cost at iteration i=9, j=50: 287.409196\n",
      "Error rate: 0.089\n",
      "Cost at iteration i=9, j=60: 285.874979\n",
      "Error rate: 0.088\n",
      "Cost at iteration i=9, j=70: 283.649959\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=9, j=80: 283.069862\n",
      "Error rate: 0.086\n",
      "Cost at iteration i=10, j=0: 282.564524\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=10, j=10: 282.082464\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=10, j=20: 281.464673\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=10, j=30: 280.370414\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=10, j=40: 279.485706\n",
      "Error rate: 0.086\n",
      "Cost at iteration i=10, j=50: 278.614792\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=10, j=60: 277.221256\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=10, j=70: 275.136229\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=10, j=80: 274.710954\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=11, j=0: 274.272787\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=11, j=10: 273.864310\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=11, j=20: 273.335701\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=11, j=30: 272.341751\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=11, j=40: 271.588671\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=11, j=50: 270.826068\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=11, j=60: 269.550931\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=11, j=70: 267.587394\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=11, j=80: 267.296750\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=12, j=0: 266.915129\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=12, j=10: 266.563794\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=12, j=20: 266.106323\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=12, j=30: 265.181038\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=12, j=40: 264.532211\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=12, j=50: 263.846774\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=12, j=60: 262.667844\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=12, j=70: 260.814911\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=12, j=80: 260.638255\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=13, j=0: 260.304485\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=13, j=10: 260.005460\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=13, j=20: 259.615953\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=13, j=30: 258.755464\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=13, j=40: 258.204888\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=13, j=50: 257.585813\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=13, j=60: 256.499543\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=13, j=70: 254.733234\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=13, j=80: 254.646397\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=14, j=0: 254.356807\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=14, j=10: 254.100883\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=14, j=20: 253.756823\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=14, j=30: 252.943672\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=14, j=40: 252.467412\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=14, j=50: 251.898807\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=14, j=60: 250.890331\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=14, j=70: 249.201262\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=14, j=80: 249.188929\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=15, j=0: 248.935038\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=15, j=10: 248.716597\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=15, j=20: 248.412996\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=15, j=30: 247.644521\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=15, j=40: 247.220732\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=15, j=50: 246.689451\n",
      "Error rate: 0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=15, j=60: 245.748023\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=15, j=70: 244.127368\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=15, j=80: 244.178761\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=16, j=0: 243.959838\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=16, j=10: 243.778729\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=16, j=20: 243.514679\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=16, j=30: 242.778004\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=16, j=40: 242.398592\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=16, j=50: 241.901591\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=16, j=60: 241.012269\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=16, j=70: 239.450682\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=16, j=80: 239.567791\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=17, j=0: 239.371169\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=17, j=10: 239.219683\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=17, j=20: 238.991029\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=17, j=30: 238.290843\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=17, j=40: 237.944941\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=17, j=50: 237.474454\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=17, j=60: 236.637822\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=17, j=70: 235.130439\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=17, j=80: 235.298709\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=18, j=0: 235.125934\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=18, j=10: 234.991112\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=18, j=20: 234.784267\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=18, j=30: 234.115488\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=18, j=40: 233.799042\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=18, j=50: 233.359268\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=18, j=60: 232.563155\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=18, j=70: 231.112353\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=18, j=80: 231.317384\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=19, j=0: 231.165892\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=19, j=10: 231.059416\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=19, j=20: 230.878997\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=19, j=30: 230.226334\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=19, j=40: 229.932804\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=19, j=50: 229.523499\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=19, j=60: 228.770219\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=19, j=70: 227.370266\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=19, j=80: 227.603146\n",
      "Error rate: 0.076\n",
      "Final error rate: 0.076\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ------------------ test number 1, batch GD without momentum ------------------\n",
    "losses_batch = []\n",
    "errors_batch = []\n",
    "for i in range(max_iter):                  # iterate through all batches\n",
    "    for j in range(n_batches):             # iterate through each specific batch\n",
    "\n",
    "        # get batch size, make predictions \n",
    "        Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "        Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]    \n",
    "        pYbatch, Z = forward(Xbatch, W1, b1, W2, b2)\n",
    "\n",
    "        # perform updates\n",
    "        W2 -= lr * (derivative_w2(Z, Ybatch, pYbatch) + reg*W2)\n",
    "        b2 -= lr * (derivative_b2(Ybatch, pYbatch) + reg*b2)\n",
    "        W1 -= lr * (derivative_w1(Xbatch, Z, Ybatch, pYbatch, W2) + reg*W1)\n",
    "        b1 -= lr * (derivative_b1(Z, Ybatch, pYbatch, W2) + reg*b1)\n",
    "\n",
    "        # if print period:\n",
    "        if j % print_period == 0:\n",
    "            pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "            l = cost(pY, Ytest_ind)\n",
    "            losses_batch.append(l)\n",
    "            print(\"Cost at iteration i=%d, j=%d: %.6f\" % (i, j, l))\n",
    "\n",
    "            e = error_rate(pY, Ytest)\n",
    "            errors_batch.append(e)\n",
    "            print(\"Error rate:\", e)\n",
    "\n",
    "# print final error rate \n",
    "pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "print(\"Final error rate:\", error_rate(pY, Ytest))\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Batch Gradient Descent, Regular Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=0, j=0: 2294.463477\n",
      "Error rate: 0.847\n",
      "Cost at iteration i=0, j=10: 865.252969\n",
      "Error rate: 0.222\n",
      "Cost at iteration i=0, j=20: 514.710432\n",
      "Error rate: 0.146\n",
      "Cost at iteration i=0, j=30: 420.871860\n",
      "Error rate: 0.134\n",
      "Cost at iteration i=0, j=40: 372.069763\n",
      "Error rate: 0.119\n",
      "Cost at iteration i=0, j=50: 347.274838\n",
      "Error rate: 0.111\n",
      "Cost at iteration i=0, j=60: 322.067323\n",
      "Error rate: 0.105\n",
      "Cost at iteration i=0, j=70: 302.239365\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=0, j=80: 291.042415\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=1, j=0: 289.843262\n",
      "Error rate: 0.091\n",
      "Cost at iteration i=1, j=10: 281.756864\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=1, j=20: 269.635717\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=1, j=30: 260.857432\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=1, j=40: 254.453650\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=1, j=50: 250.460195\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=1, j=60: 242.755731\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=1, j=70: 234.680575\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=1, j=80: 232.651611\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=2, j=0: 232.756673\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=10: 229.339235\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=20: 223.906050\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=30: 218.371216\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=40: 214.979387\n",
      "Error rate: 0.073\n",
      "Cost at iteration i=2, j=50: 213.889161\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=2, j=60: 208.121230\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=2, j=70: 203.783485\n",
      "Error rate: 0.069\n",
      "Cost at iteration i=2, j=80: 204.360654\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=3, j=0: 205.397930\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=3, j=10: 203.456968\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=3, j=20: 198.402004\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=3, j=30: 194.459301\n",
      "Error rate: 0.063\n",
      "Cost at iteration i=3, j=40: 192.017781\n",
      "Error rate: 0.06\n",
      "Cost at iteration i=3, j=50: 192.639637\n",
      "Error rate: 0.06\n",
      "Cost at iteration i=3, j=60: 187.803318\n",
      "Error rate: 0.056\n",
      "Cost at iteration i=3, j=70: 184.544311\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=3, j=80: 185.618355\n",
      "Error rate: 0.057\n",
      "Cost at iteration i=4, j=0: 186.898771\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=4, j=10: 186.257543\n",
      "Error rate: 0.058\n",
      "Cost at iteration i=4, j=20: 181.395281\n",
      "Error rate: 0.054\n",
      "Cost at iteration i=4, j=30: 179.122582\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=4, j=40: 176.886379\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=4, j=50: 177.995178\n",
      "Error rate: 0.051\n",
      "Cost at iteration i=4, j=60: 174.325164\n",
      "Error rate: 0.052\n",
      "Cost at iteration i=4, j=70: 171.843706\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=4, j=80: 172.903220\n",
      "Error rate: 0.051\n",
      "Cost at iteration i=5, j=0: 174.209561\n",
      "Error rate: 0.049\n",
      "Cost at iteration i=5, j=10: 174.911499\n",
      "Error rate: 0.049\n",
      "Cost at iteration i=5, j=20: 169.946056\n",
      "Error rate: 0.052\n",
      "Cost at iteration i=5, j=30: 168.466478\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=5, j=40: 166.441036\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=5, j=50: 167.491783\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=5, j=60: 164.715102\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=5, j=70: 162.715753\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=5, j=80: 163.511962\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=6, j=0: 164.733075\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=6, j=10: 166.370886\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=6, j=20: 161.498060\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=6, j=30: 160.185122\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=6, j=40: 158.611106\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=6, j=50: 159.586986\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=6, j=60: 157.324814\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=6, j=70: 155.690671\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=6, j=80: 156.295186\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=7, j=0: 157.423026\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=7, j=10: 159.884154\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=7, j=20: 155.233033\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=7, j=30: 153.637556\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=7, j=40: 152.568268\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=7, j=50: 153.664414\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=7, j=60: 151.572935\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=7, j=70: 150.312521\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=7, j=80: 150.803089\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=8, j=0: 151.786655\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=8, j=10: 154.779688\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=8, j=20: 150.598785\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=8, j=30: 148.897202\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=8, j=40: 147.942186\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=8, j=50: 148.883908\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=8, j=60: 146.995310\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=8, j=70: 145.937349\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=8, j=80: 146.364137\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=9, j=0: 147.219289\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=9, j=10: 150.579226\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=9, j=20: 146.695020\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=9, j=30: 144.749338\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=9, j=40: 144.057661\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=9, j=50: 145.199304\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=9, j=60: 143.365842\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=9, j=70: 142.458309\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=9, j=80: 142.885027\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=10, j=0: 143.618734\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=10, j=10: 147.188463\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=10, j=20: 144.059668\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=10, j=30: 141.583048\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=10, j=40: 140.863160\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=10, j=50: 142.207033\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=10, j=60: 140.454386\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=10, j=70: 139.626737\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=10, j=80: 140.045902\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=11, j=0: 140.665768\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=10: 144.299901\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=11, j=20: 141.496906\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=30: 139.100956\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=11, j=40: 138.439906\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=11, j=50: 139.727892\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=11, j=60: 138.111822\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=70: 137.290816\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=80: 137.771676\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=12, j=0: 138.323045\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=12, j=10: 141.986588\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=12, j=20: 139.565257\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=12, j=30: 137.164118\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=12, j=40: 136.574107\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=12, j=50: 137.865597\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=12, j=60: 136.411676\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=12, j=70: 135.631505\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=12, j=80: 136.152213\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=13, j=0: 136.607838\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=13, j=10: 140.195623\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=13, j=20: 138.133038\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=30: 135.707678\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=40: 135.095462\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=13, j=50: 136.274613\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=13, j=60: 134.913738\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=13, j=70: 134.157789\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=13, j=80: 134.744493\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=14, j=0: 135.137019\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=14, j=10: 138.658361\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=20: 136.923513\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=30: 134.487652\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=14, j=40: 133.845144\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=14, j=50: 134.974525\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=14, j=60: 133.754173\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=14, j=70: 132.994976\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=14, j=80: 133.603243\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=15, j=0: 133.944373\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=15, j=10: 137.412295\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=15, j=20: 135.904162\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=15, j=30: 133.449860\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=15, j=40: 132.740421\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=15, j=50: 133.784747\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=15, j=60: 132.689521\n",
      "Error rate: 0.038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=15, j=70: 131.887183\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=15, j=80: 132.529317\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=16, j=0: 132.832339\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=16, j=10: 136.270885\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=16, j=20: 135.127153\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=16, j=30: 132.702527\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=40: 131.891508\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=50: 132.858625\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=16, j=60: 131.886307\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=70: 131.089756\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=80: 131.719684\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=17, j=0: 131.978548\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=17, j=10: 135.295026\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=17, j=20: 134.411995\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=17, j=30: 131.942410\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=17, j=40: 131.048723\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=17, j=50: 131.930850\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=17, j=60: 131.115385\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=17, j=70: 130.323413\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=17, j=80: 130.970258\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=0: 131.197745\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=18, j=10: 134.385540\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=18, j=20: 133.818903\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=30: 131.365831\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=40: 130.389380\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=18, j=50: 131.183361\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=18, j=60: 130.523110\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=70: 129.737984\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=80: 130.354473\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=0: 130.561262\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=10: 133.645807\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=19, j=20: 133.334656\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=30: 130.874764\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=40: 129.766372\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=19, j=50: 130.442477\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=19, j=60: 129.900068\n",
      "Error rate: 0.036\n",
      "Cost at iteration i=19, j=70: 129.156125\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=80: 129.751700\n",
      "Error rate: 0.037\n",
      "Final error rate: 0.037\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------- test number 2, batch GD with regular momentum ------------------\n",
    "W1 = W1_0.copy()\n",
    "b1 = b1_0.copy()\n",
    "W2 = W2_0.copy()\n",
    "b2 = b2_0.copy()\n",
    "losses_momentum = []\n",
    "errors_momentum = []\n",
    "\n",
    "mu = 0.9                    # momentum parameter, think viscosity \n",
    "dW2 = 0                     # need to keep track of the previous weight changes (gradients)\n",
    "db2 = 0                     # think of these as velocity\n",
    "dW1 = 0\n",
    "db1 = 0\n",
    "\n",
    "for i in range(max_iter):                  # iterate through all batches\n",
    "    for j in range(n_batches):             # iterate through each specific batch\n",
    "\n",
    "        # get batch size, make predictions \n",
    "        Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "        Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]    \n",
    "        pYbatch, Z = forward(Xbatch, W1, b1, W2, b2)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        gW2 = derivative_w2(Z, Ybatch, pYbatch) + reg*W2\n",
    "        gb2 = derivative_b2(Ybatch, pYbatch) + reg*b2\n",
    "        gW1 = derivative_w1(Xbatch, Z, Ybatch, pYbatch, W2) + reg*W1\n",
    "        gb1 = derivative_b1(Z, Ybatch, pYbatch, W2) + reg*b1\n",
    "        \n",
    "        # update our velocities - based on regular momentum equation\n",
    "        dW2 = mu*dW2 - lr*gW2\n",
    "        db2 = mu*db2 - lr*gb2\n",
    "        dW1 = mu*dW1 - lr*gW1\n",
    "        db1 = mu*db1 - lr*gb1\n",
    "        \n",
    "        # update weights\n",
    "        W2 += dW2\n",
    "        b2 += db2\n",
    "        W1 += dW1 \n",
    "        b1 += db1 \n",
    "        \n",
    "        # if print period:\n",
    "        if j % print_period == 0:\n",
    "            pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "            l = cost(pY, Ytest_ind)\n",
    "            losses_momentum.append(l)\n",
    "            print(\"Cost at iteration i=%d, j=%d: %.6f\" % (i, j, l))\n",
    "\n",
    "            e = error_rate(pY, Ytest)\n",
    "            errors_momentum.append(e)\n",
    "            print(\"Error rate:\", e)\n",
    "\n",
    "# print final error rate \n",
    "pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "print(\"Final error rate:\", error_rate(pY, Ytest))\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Gradient Descent, Nesterov Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=0, j=0: 2225.324882\n",
      "Error rate: 0.803\n",
      "Cost at iteration i=0, j=10: 817.535177\n",
      "Error rate: 0.207\n",
      "Cost at iteration i=0, j=20: 501.828737\n",
      "Error rate: 0.145\n",
      "Cost at iteration i=0, j=30: 413.995516\n",
      "Error rate: 0.133\n",
      "Cost at iteration i=0, j=40: 368.104063\n",
      "Error rate: 0.117\n",
      "Cost at iteration i=0, j=50: 343.756647\n",
      "Error rate: 0.109\n",
      "Cost at iteration i=0, j=60: 320.605857\n",
      "Error rate: 0.102\n",
      "Cost at iteration i=0, j=70: 301.133948\n",
      "Error rate: 0.096\n",
      "Cost at iteration i=0, j=80: 291.138533\n",
      "Error rate: 0.091\n",
      "Cost at iteration i=1, j=0: 289.069163\n",
      "Error rate: 0.089\n",
      "Cost at iteration i=1, j=10: 281.060472\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=1, j=20: 268.627669\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=1, j=30: 260.722780\n",
      "Error rate: 0.086\n",
      "Cost at iteration i=1, j=40: 253.952531\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=1, j=50: 249.614301\n",
      "Error rate: 0.081\n",
      "Cost at iteration i=1, j=60: 242.165274\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=1, j=70: 233.798733\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=1, j=80: 232.519981\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=2, j=0: 232.036405\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=10: 228.417215\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=20: 222.879016\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=30: 218.372066\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=2, j=40: 214.547546\n",
      "Error rate: 0.073\n",
      "Cost at iteration i=2, j=50: 212.881313\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=2, j=60: 207.855819\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=2, j=70: 202.940087\n",
      "Error rate: 0.068\n",
      "Cost at iteration i=2, j=80: 204.453240\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=3, j=0: 204.917701\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=3, j=10: 202.335243\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=3, j=20: 197.646483\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=3, j=30: 194.159823\n",
      "Error rate: 0.061\n",
      "Cost at iteration i=3, j=40: 191.892331\n",
      "Error rate: 0.06\n",
      "Cost at iteration i=3, j=50: 191.764647\n",
      "Error rate: 0.059\n",
      "Cost at iteration i=3, j=60: 187.518909\n",
      "Error rate: 0.058\n",
      "Cost at iteration i=3, j=70: 183.856081\n",
      "Error rate: 0.054\n",
      "Cost at iteration i=3, j=80: 185.827990\n",
      "Error rate: 0.057\n",
      "Cost at iteration i=4, j=0: 186.611915\n",
      "Error rate: 0.056\n",
      "Cost at iteration i=4, j=10: 185.143590\n",
      "Error rate: 0.057\n",
      "Cost at iteration i=4, j=20: 181.702772\n",
      "Error rate: 0.054\n",
      "Cost at iteration i=4, j=30: 178.838470\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=4, j=40: 176.816683\n",
      "Error rate: 0.053\n",
      "Cost at iteration i=4, j=50: 177.279017\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=4, j=60: 173.949117\n",
      "Error rate: 0.051\n",
      "Cost at iteration i=4, j=70: 171.150254\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=4, j=80: 173.053074\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=5, j=0: 173.972126\n",
      "Error rate: 0.049\n",
      "Cost at iteration i=5, j=10: 173.491961\n",
      "Error rate: 0.051\n",
      "Cost at iteration i=5, j=20: 169.677426\n",
      "Error rate: 0.052\n",
      "Cost at iteration i=5, j=30: 167.979540\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=5, j=40: 165.889754\n",
      "Error rate: 0.049\n",
      "Cost at iteration i=5, j=50: 166.508027\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=5, j=60: 164.107502\n",
      "Error rate: 0.049\n",
      "Cost at iteration i=5, j=70: 161.925519\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=5, j=80: 163.612034\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=6, j=0: 164.558516\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=6, j=10: 164.898268\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=6, j=20: 161.198498\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=6, j=30: 159.650884\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=6, j=40: 157.956178\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=6, j=50: 158.650450\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=6, j=60: 156.740796\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=6, j=70: 154.951690\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=6, j=80: 156.367878\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=7, j=0: 157.290690\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=7, j=10: 158.372902\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=7, j=20: 154.833744\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=7, j=30: 152.914444\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=7, j=40: 151.762363\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=7, j=50: 152.588622\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=7, j=60: 150.915117\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=7, j=70: 149.570353\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=7, j=80: 150.987285\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=8, j=0: 151.866964\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=8, j=10: 153.475380\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=8, j=20: 150.421081\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=8, j=30: 148.044978\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=8, j=40: 147.132767\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=8, j=50: 147.985176\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=8, j=60: 146.448733\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=8, j=70: 145.260680\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=8, j=80: 146.493425\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=9, j=0: 147.283359\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=9, j=10: 149.336939\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=9, j=20: 146.389021\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=9, j=30: 144.498096\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=9, j=40: 143.470271\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=9, j=50: 144.192656\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=9, j=60: 142.905083\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=9, j=70: 141.834509\n",
      "Error rate: 0.043\n",
      "Cost at iteration i=9, j=80: 142.926750\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=10, j=0: 143.638362\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=10, j=10: 146.029762\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=10, j=20: 143.226869\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=10, j=30: 141.236877\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=10, j=40: 140.421789\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=10, j=50: 141.177429\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=10, j=60: 140.096190\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=10, j=70: 139.283535\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=10, j=80: 140.338803\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=0: 140.946784\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=10: 143.457274\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=11, j=20: 140.920617\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=11, j=30: 139.050442\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=11, j=40: 138.216552\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=11, j=50: 138.837304\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=11, j=60: 137.870537\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=70: 137.104796\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=11, j=80: 138.140660\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=12, j=0: 138.678342\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=12, j=10: 141.301927\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=12, j=20: 139.006746\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=12, j=30: 137.066991\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=12, j=40: 136.367430\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=12, j=50: 136.974232\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=12, j=60: 136.067767\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=12, j=70: 135.340758\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=12, j=80: 136.373423\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=0: 136.851994\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=10: 139.522226\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=20: 137.462860\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=30: 135.552908\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=40: 134.871457\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=13, j=50: 135.380370\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=13, j=60: 134.504048\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=70: 133.812702\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=13, j=80: 134.889240\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=0: 135.317941\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=10: 138.019039\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=20: 136.230115\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=14, j=30: 134.407181\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=40: 133.752430\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=14, j=50: 134.168845\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=14, j=60: 133.344408\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=14, j=70: 132.669475\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=14, j=80: 133.744856\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=15, j=0: 134.135925\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=15, j=10: 136.838087\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=15, j=20: 135.267965\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=15, j=30: 133.170648\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=15, j=40: 132.596681\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=15, j=50: 133.147637\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=15, j=60: 132.269559\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=15, j=70: 131.593269\n",
      "Error rate: 0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=15, j=80: 132.715975\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=16, j=0: 133.087654\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=16, j=10: 135.850484\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=16, j=20: 134.805926\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=16, j=30: 132.552716\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=16, j=40: 131.883689\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=50: 132.343432\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=60: 131.490671\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=16, j=70: 130.733164\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=16, j=80: 131.851890\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=17, j=0: 132.188373\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=17, j=10: 134.886652\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=17, j=20: 133.795982\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=17, j=30: 132.037789\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=17, j=40: 131.261915\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=17, j=50: 131.516445\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=17, j=60: 130.822865\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=17, j=70: 130.156586\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=17, j=80: 131.226337\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=18, j=0: 131.522779\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=18, j=10: 134.177735\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=18, j=20: 133.295807\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=30: 131.533501\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=18, j=40: 130.702263\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=18, j=50: 130.859539\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=60: 130.214921\n",
      "Error rate: 0.036\n",
      "Cost at iteration i=18, j=70: 129.575643\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=18, j=80: 130.617522\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=19, j=0: 130.893625\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=19, j=10: 133.514414\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=19, j=20: 132.841475\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=30: 130.766507\n",
      "Error rate: 0.034\n",
      "Cost at iteration i=19, j=40: 129.890522\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=19, j=50: 130.213727\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=19, j=60: 129.572342\n",
      "Error rate: 0.036\n",
      "Cost at iteration i=19, j=70: 128.927865\n",
      "Error rate: 0.036\n",
      "Cost at iteration i=19, j=80: 130.002239\n",
      "Error rate: 0.038\n",
      "Final error rate: 0.038\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --------------- test number 3, batch GD with regular momentum ------------------\n",
    "W1 = W1_0.copy()\n",
    "b1 = b1_0.copy()\n",
    "W2 = W2_0.copy()\n",
    "b2 = b2_0.copy()\n",
    "\n",
    "losses_nesterov = []\n",
    "errors_nesterov = []\n",
    "\n",
    "mu = 0.9                    # momentum parameter, think viscosity \n",
    "vW2 = 0                     # need to keep track of the previous weight changes (gradients)\n",
    "vb2 = 0                     # think of these as velocity\n",
    "vW1 = 0\n",
    "vb1 = 0\n",
    "\n",
    "for i in range(max_iter):                  # iterate through all batches\n",
    "    for j in range(n_batches):             # iterate through each specific batch\n",
    "\n",
    "        # get batch size, make predictions \n",
    "        Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "        Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]    \n",
    "        pYbatch, Z = forward(Xbatch, W1, b1, W2, b2)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        gW2 = derivative_w2(Z, Ybatch, pYbatch) + reg*W2\n",
    "        gb2 = derivative_b2(Ybatch, pYbatch) + reg*b2\n",
    "        gW1 = derivative_w1(Xbatch, Z, Ybatch, pYbatch, W2) + reg*W1\n",
    "        gb1 = derivative_b1(Z, Ybatch, pYbatch, W2) + reg*b1\n",
    "        \n",
    "        # update our velocites - based on nesterov momentum equations \n",
    "        vW2 = mu*vW2 - lr*gW2\n",
    "        vb2 = mu*vb2 - lr*gb2\n",
    "        vW1 = mu*vW1 - lr*gW1\n",
    "        vb1 = mu*vb1 - lr*gb1\n",
    "        \n",
    "        # update our weights - based on nesterov momentum equations \n",
    "        W2 += mu*vW2 - lr*gW2\n",
    "        b2 += mu*vb2 - lr*gb2\n",
    "        W1 += mu*vW1 - lr*gW1\n",
    "        b1 += mu*vb1 - lr*gb1\n",
    "        \n",
    "        # if print period:\n",
    "        if j % print_period == 0:\n",
    "            pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "            l = cost(pY, Ytest_ind)\n",
    "            losses_nesterov.append(l)\n",
    "            print(\"Cost at iteration i=%d, j=%d: %.6f\" % (i, j, l))\n",
    "\n",
    "            e = error_rate(pY, Ytest)\n",
    "            errors_nesterov.append(e)\n",
    "            print(\"Error rate:\", e)\n",
    "\n",
    "# print final error rate \n",
    "pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "print(\"Final error rate:\", error_rate(pY, Ytest))\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot all likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(losses_batch, label=\"batch\")\n",
    "plt.plot(losses_momentum, label=\"momentum\")\n",
    "plt.plot(losses_nesterov, label=\"nesterov\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<br></br>\n",
    "# 4. Variable and Adaptive Learning Rates \n",
    "Momentum is generally looked at as the most impactful method when it comes to speeding up training. If you compare the loss per iteration between standard gradient descent, and gradient descent with momentum, the difference is huge! \n",
    "\n",
    "Additionally, momentum is nice because you don't have to play with the hyper parameters very much. You can just set it to 0.9 and it is usually fine, resulting in huge performance gains. However, some adaptive learning techniques are very powerful, so let's take a look! \n",
    "\n",
    "## 4.1 Variable Learning Rates \n",
    "The first new learning rate we will look at is a **variable learning rate**, e.g. a learning rate that is a function of time, $\\eta(t)$. This is sometimes referred to as \"Learning rate scheduling\". The first one worth mentioning is:\n",
    "\n",
    "### 4.1.1 Step Decay \n",
    "Periodically, say ever 100 iterations, we will reduce the learning rate by a constant factor. For example, if we constantly divide by 2, we will half it each time. <br>\n",
    "\n",
    "### 4.1.2 Exponential Decay\n",
    "In this method the learning rate follows and exponential curve:\n",
    "\n",
    "#### $$\\eta(t) = A * e^{-kt}$$\n",
    "\n",
    "<img src=\"images/exp-decay.png\">\n",
    "\n",
    "### 4.1.3 $\\frac{1}{t}$ decay\n",
    "Here the learning rate will decay proportionally to $\\frac{1}{t}$:\n",
    "#### $$\\eta(t) = \\frac{A}{kt + 1 }$$\n",
    "In this case the dropoff is slower than exponential decay. \n",
    "\n",
    "<img src=\"images/1t-decay.png\">\n",
    "\n",
    "\n",
    "### 4.1.4 What do all of these methods have in common?\n",
    "Well, each of these methods will decrease the learning rate as time goes on (i.e. the number of iterations increases). The question comes up, why would be want to do that? Well, generally speaking, when we initialize the weights of a neural network, they are going to be very far from the optimal weights, so it is good to start with a large learning rate so that we can take bigger steps towards the goal. This is the motivation behind momentum as well: We want to pick up speed by accumulating past gradients, because we know that if we are very far away from our goal, then those gradients should be large. However, as we get close to the goal, the gradient is going to shrink. In fact, by definition, the minimum of a function means the gradient must be 0-that is how we solve for the minimum of a function in calculus.\n",
    "\n",
    "But why may we want to slow down as we get close to the minimum? Well, when you are too close to the minimum and you take too big of a step, you are going to overshoot the minimum. So what ends up happening is that you just bounce back and forth. In fact, if you learning rate is too large then you will just bounce right of the valley and your loss may increase! So, in order to reduce all of this bouncing around, we would like to take smaller steps. \n",
    "\n",
    "One other things to think about is the following caveat: all of these methods mean that there are more **hyperparameters** to optimize. This adds more work to your plate as a data scientist. \n",
    "\n",
    "---\n",
    "## 4.2 Adaptive Learning Rates \n",
    "<br>\n",
    "\n",
    "<img src=\"images/opt1.gif\">\n",
    "\n",
    "<br>\n",
    "### 4.2.1 AdaGrad\n",
    "The first adaptive learning technique we will go over is **AdaGrad**. The main idea behind it is this: we cannot expect the dependence of the cost on each of the parameters to be the same. In other words, in one direction the gradient may be very steep, but in another direction the gradient may be very flat. So, perhaps it may be beneficial to adapt the learning rate for each parameter individually, based on how much it has changed in the past. \n",
    "\n",
    "So, in AdaGrad what we do is introduce a variable called the **cache**. \n",
    "\n",
    "#### $$cache = cache + gradient^2$$\n",
    "\n",
    "#### $$w \\leftarrow w - \\eta \\frac{\\nabla J}{\\sqrt{cache + \\epsilon }}$$\n",
    "\n",
    "Each parameter of the neural network has its own cache, so for example if you have one weight matrix of size (3 x 4), you will also have a cache matrix of size (3 x 4). The same thing applies to the bias vectors. \n",
    "\n",
    "The idea behind the cache is that it is going to accumulate the squared gradients. Because we are squaring the gradients, the cache is always going to be positive. And because each parameter has its own cache, then if one parameter has had a lot of large gradients in the past, then its cache will be very large, and its effective learning rate will be very small, and it will change more slowly in the future. On the other hand, if a parameter has had a lot of small gradients in the past then it's cache will be small, so its respective learning rate will remain large, and it will have more opportunity to change in the future. \n",
    "\n",
    "One small thing to keep in mind is that we usually add a small number $\\epsilon$ to the denominator in order to prevent dividing by 0. This is generally set to $10^{-8}$ or $10^{-10}$. \n",
    "\n",
    "One important thing to stress about AdaGrad is that everything we are doing is an element-wise operation. In other words, each scalar parameter and its learning rate is effectively updated independently of the others. So you can look at the rules that we presented as scalar updates that apply to all of the parameters, or you can think of one huge parameter vector that contains all of the neural network parameters, and that each of the operations is an element wise operation. \n",
    "\n",
    "### 4.2.2 RMSProp\n",
    "This next technique builds on the fact that researchers have found that AdaGrad decreases the learning rate too aggresively. In this case, the learning rate would approach 0 too quickly, when in fact there was still learning to be done. The method introduced to fix this is called **RMSProp**, and it was introduced by Geoff Hinton and team. It works as follows:\n",
    "\n",
    "The reason that AdaGrad decrease the learning rate too quickly is because the cache is growing too fast. So in order to make the cache grow more slowly, we actually decrease it each time we update it. This is done by taking a weighted average of the old cache and the new squared gradient. We call this weight the decay weight, and we can see that the two weights add up to 1. \n",
    "\n",
    "#### $$cache = decay*cache +(1 - decay)*gradient^2$$\n",
    "\n",
    "Typical values for the decay are 0.99, 0.999, etc. The intuition for these choices will be discussed in later lectures. By doing this, we say that we are making the cache \"leaky\". The reasoning it is leaking is because we can imagine that if we had 0 gradient for a long time, eventually the cache would shrink back down to 0, because it would be decreased by the decay rate on each round \n",
    "\n",
    "Note that there is some ambiguity in the RMSProp and AdaGrad algorithms. Specifically, this can be seen in the context of RMSProp. The ambiguity arises from the initial value of the cache. You may automatically assume that 0 is a good value, but this actually has a very strange effect. We can let:\n",
    "#### $$decay = 0.99$$ \n",
    "And that means our initial update for the cache is:\n",
    "\n",
    "#### $$0.001g^2$$\n",
    "And hence our initial update is (ignoring epsilon): \n",
    "#### $$\\frac{\\eta g}{\\sqrt{0.001g^2}}$$\n",
    "Which ends up being very large, because the denominator is very small. What you would have to do is compensate by making you initial learning rate smaller than usual. One solution to this however, is just to set the cache to 1 instead. By manipulating the RMSProp update, we can show that this is what we would get if we were not doing RMSProp at all:\n",
    "\n",
    "#### $$\\nabla w = \\eta \\frac{g}{\\sqrt{1 + 0.001 g^2}} \\approx \\eta g$$\n",
    "\n",
    "You may still be wondering which way is the correct way to go about things? Well, in **TensorFlow** the cache is initialized to 1. In **Keras** the cache is initialized to 0. \n",
    "\n",
    "## 4.3 Summary Pseudocode\n",
    "> **AdaGrad**\n",
    "```\n",
    "# at every batch\n",
    "cache += gradient ** 2\n",
    "param = param - learning_rate * gradient / sqrt(cache + epsilon)\n",
    "```\n",
    "---\n",
    "**RMSProp**\n",
    "```\n",
    "# at every batch\n",
    "cache = decay * cache + (1 - decay) * gradient **2\n",
    "param = param - learning_rate * gradient / sqrt(cache + epsilon)\n",
    "```\n",
    "**Note:**\n",
    "Epsilon is generally 10e-8, 10e-9, 10e-10, and decay is generally 0.9, 0.99, 0.999 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# 5. Constant Learning Rate vs. RMSProp in Code\n",
    "Here we are going to be comparing RMSProp against a constant learning rate. We can start by defining our standard multi-layer perceptron functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z = X.dot(W1) + b1\n",
    "    Z[Z < 0] = 0               # using ReLU function\n",
    "    A = Z.dot(W2) + b2\n",
    "    expA = np.exp(A)\n",
    "    Y = expA / expA.sum(axis=1, keepdims=True)\n",
    "    return Y, Z\n",
    "\n",
    "def derivative_w2(Z, T, Y):\n",
    "    return Z.T.dot(Y - T)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    return (Y - T).sum(axis=0)\n",
    "\n",
    "def derivative_w1(X, Z, T, Y, W2):\n",
    "    return X.T.dot( ( (Y - T).dot(W2.T) * (Z > 0) ) )      # for relu\n",
    "\n",
    "def derivative_b1(Z, T, Y, W2):\n",
    "    return ((Y - T).dot(W2.T) * (Z > 0)).sum(axis=0)       # for relu\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from util import get_normalized_data, error_rate, cost, y2indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start our main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n",
      "Cost at iteration i=0, j=0: 2407.482202\n",
      "Error rate: 0.867\n",
      "Cost at iteration i=0, j=10: 1826.017811\n",
      "Error rate: 0.544\n",
      "Cost at iteration i=0, j=20: 1473.440433\n",
      "Error rate: 0.359\n",
      "Cost at iteration i=0, j=30: 1244.315492\n",
      "Error rate: 0.279\n",
      "Cost at iteration i=0, j=40: 1089.593470\n",
      "Error rate: 0.245\n",
      "Cost at iteration i=0, j=50: 975.277438\n",
      "Error rate: 0.225\n",
      "Cost at iteration i=0, j=60: 889.805773\n",
      "Error rate: 0.206\n",
      "Cost at iteration i=0, j=70: 820.510821\n",
      "Error rate: 0.188\n",
      "Cost at iteration i=0, j=80: 767.020651\n",
      "Error rate: 0.182\n",
      "Cost at iteration i=1, j=0: 757.122496\n",
      "Error rate: 0.178\n",
      "Cost at iteration i=1, j=10: 714.588696\n",
      "Error rate: 0.171\n",
      "Cost at iteration i=1, j=20: 678.051987\n",
      "Error rate: 0.164\n",
      "Cost at iteration i=1, j=30: 646.499934\n",
      "Error rate: 0.161\n",
      "Cost at iteration i=1, j=40: 621.282427\n",
      "Error rate: 0.158\n",
      "Cost at iteration i=1, j=50: 599.113331\n",
      "Error rate: 0.153\n",
      "Cost at iteration i=1, j=60: 580.663440\n",
      "Error rate: 0.148\n",
      "Cost at iteration i=1, j=70: 562.327224\n",
      "Error rate: 0.144\n",
      "Cost at iteration i=1, j=80: 547.540358\n",
      "Error rate: 0.143\n",
      "Cost at iteration i=2, j=0: 544.188619\n",
      "Error rate: 0.142\n",
      "Cost at iteration i=2, j=10: 530.052243\n",
      "Error rate: 0.139\n",
      "Cost at iteration i=2, j=20: 516.712672\n",
      "Error rate: 0.136\n",
      "Cost at iteration i=2, j=30: 504.302662\n",
      "Error rate: 0.132\n",
      "Cost at iteration i=2, j=40: 494.615760\n",
      "Error rate: 0.129\n",
      "Cost at iteration i=2, j=50: 485.286479\n",
      "Error rate: 0.128\n",
      "Cost at iteration i=2, j=60: 477.860823\n",
      "Error rate: 0.127\n",
      "Cost at iteration i=2, j=70: 469.377578\n",
      "Error rate: 0.124\n",
      "Cost at iteration i=2, j=80: 462.524524\n",
      "Error rate: 0.125\n",
      "Cost at iteration i=3, j=0: 460.659058\n",
      "Error rate: 0.123\n",
      "Cost at iteration i=3, j=10: 453.126694\n",
      "Error rate: 0.12\n",
      "Cost at iteration i=3, j=20: 445.787226\n",
      "Error rate: 0.12\n",
      "Cost at iteration i=3, j=30: 438.687056\n",
      "Error rate: 0.117\n",
      "Cost at iteration i=3, j=40: 433.478593\n",
      "Error rate: 0.116\n",
      "Cost at iteration i=3, j=50: 428.100893\n",
      "Error rate: 0.115\n",
      "Cost at iteration i=3, j=60: 424.163532\n",
      "Error rate: 0.115\n",
      "Cost at iteration i=3, j=70: 419.017374\n",
      "Error rate: 0.112\n",
      "Cost at iteration i=3, j=80: 415.145552\n",
      "Error rate: 0.111\n",
      "Cost at iteration i=4, j=0: 413.877615\n",
      "Error rate: 0.109\n",
      "Cost at iteration i=4, j=10: 408.906733\n",
      "Error rate: 0.114\n",
      "Cost at iteration i=4, j=20: 404.166610\n",
      "Error rate: 0.109\n",
      "Cost at iteration i=4, j=30: 399.236957\n",
      "Error rate: 0.111\n",
      "Cost at iteration i=4, j=40: 395.910730\n",
      "Error rate: 0.108\n",
      "Cost at iteration i=4, j=50: 392.243526\n",
      "Error rate: 0.108\n",
      "Cost at iteration i=4, j=60: 389.787780\n",
      "Error rate: 0.107\n",
      "Cost at iteration i=4, j=70: 386.315518\n",
      "Error rate: 0.106\n",
      "Cost at iteration i=4, j=80: 383.862091\n",
      "Error rate: 0.106\n",
      "Cost at iteration i=5, j=0: 382.900765\n",
      "Error rate: 0.106\n",
      "Cost at iteration i=5, j=10: 379.174655\n",
      "Error rate: 0.103\n",
      "Cost at iteration i=5, j=20: 375.703785\n",
      "Error rate: 0.104\n",
      "Cost at iteration i=5, j=30: 371.780762\n",
      "Error rate: 0.104\n",
      "Cost at iteration i=5, j=40: 369.447790\n",
      "Error rate: 0.102\n",
      "Cost at iteration i=5, j=50: 366.704571\n",
      "Error rate: 0.1\n",
      "Cost at iteration i=5, j=60: 365.056025\n",
      "Error rate: 0.101\n",
      "Cost at iteration i=5, j=70: 362.530597\n",
      "Error rate: 0.101\n",
      "Cost at iteration i=5, j=80: 360.885679\n",
      "Error rate: 0.1\n",
      "Cost at iteration i=6, j=0: 360.117170\n",
      "Error rate: 0.099\n",
      "Cost at iteration i=6, j=10: 357.096566\n",
      "Error rate: 0.1\n",
      "Cost at iteration i=6, j=20: 354.341750\n",
      "Error rate: 0.098\n",
      "Cost at iteration i=6, j=30: 351.047424\n",
      "Error rate: 0.1\n",
      "Cost at iteration i=6, j=40: 349.263379\n",
      "Error rate: 0.095\n",
      "Cost at iteration i=6, j=50: 347.098420\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=6, j=60: 345.972166\n",
      "Error rate: 0.096\n",
      "Cost at iteration i=6, j=70: 344.056486\n",
      "Error rate: 0.097\n",
      "Cost at iteration i=6, j=80: 342.913872\n",
      "Error rate: 0.095\n",
      "Cost at iteration i=7, j=0: 342.274637\n",
      "Error rate: 0.095\n",
      "Cost at iteration i=7, j=10: 339.718626\n",
      "Error rate: 0.096\n",
      "Cost at iteration i=7, j=20: 337.405853\n",
      "Error rate: 0.093\n",
      "Cost at iteration i=7, j=30: 334.551713\n",
      "Error rate: 0.091\n",
      "Cost at iteration i=7, j=40: 333.081451\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=7, j=50: 331.294861\n",
      "Error rate: 0.09\n",
      "Cost at iteration i=7, j=60: 330.493247\n",
      "Error rate: 0.089\n",
      "Cost at iteration i=7, j=70: 328.996975\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=7, j=80: 328.193347\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=8, j=0: 327.641001\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=8, j=10: 325.390317\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=8, j=20: 323.399621\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=8, j=30: 320.824862\n",
      "Error rate: 0.087\n",
      "Cost at iteration i=8, j=40: 319.574646\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=8, j=50: 318.041694\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=8, j=60: 317.454417\n",
      "Error rate: 0.086\n",
      "Cost at iteration i=8, j=70: 316.258699\n",
      "Error rate: 0.086\n",
      "Cost at iteration i=8, j=80: 315.699934\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=9, j=0: 315.211171\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=9, j=10: 313.203338\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=9, j=20: 311.486171\n",
      "Error rate: 0.085\n",
      "Cost at iteration i=9, j=30: 309.137469\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=9, j=40: 308.051906\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=9, j=50: 306.714807\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=9, j=60: 306.299519\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=9, j=70: 305.325017\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=9, j=80: 304.942806\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=10, j=0: 304.509797\n",
      "Error rate: 0.083\n",
      "Cost at iteration i=10, j=10: 302.687232\n",
      "Error rate: 0.082\n",
      "Cost at iteration i=10, j=20: 301.157869\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=10, j=30: 298.983892\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=10, j=40: 297.999651\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=10, j=50: 296.838428\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=10, j=60: 296.561877\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=10, j=70: 295.768675\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=10, j=80: 295.530903\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=11, j=0: 295.147084\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=11, j=10: 293.483066\n",
      "Error rate: 0.08\n",
      "Cost at iteration i=11, j=20: 292.105952\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=11, j=30: 290.076669\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=11, j=40: 289.172488\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=11, j=50: 288.133344\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=11, j=60: 287.959199\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=11, j=70: 287.313068\n",
      "Error rate: 0.078\n",
      "Cost at iteration i=11, j=80: 287.200241\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=12, j=0: 286.854436\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=12, j=10: 285.315126\n",
      "Error rate: 0.079\n",
      "Cost at iteration i=12, j=20: 284.075540\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=12, j=30: 282.161613\n",
      "Error rate: 0.075\n",
      "Cost at iteration i=12, j=40: 281.308571\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=12, j=50: 280.376055\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=12, j=60: 280.289207\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=12, j=70: 279.764275\n",
      "Error rate: 0.077\n",
      "Cost at iteration i=12, j=80: 279.746039\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=13, j=0: 279.428021\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=13, j=10: 277.986781\n",
      "Error rate: 0.075\n",
      "Cost at iteration i=13, j=20: 276.849644\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=13, j=30: 275.023760\n",
      "Error rate: 0.075\n",
      "Cost at iteration i=13, j=40: 274.219092\n",
      "Error rate: 0.074\n",
      "Cost at iteration i=13, j=50: 273.389523\n",
      "Error rate: 0.074\n",
      "Cost at iteration i=13, j=60: 273.379458\n",
      "Error rate: 0.075\n",
      "Cost at iteration i=13, j=70: 272.951342\n",
      "Error rate: 0.074\n",
      "Cost at iteration i=13, j=80: 273.021928\n",
      "Error rate: 0.076\n",
      "Cost at iteration i=14, j=0: 272.723275\n",
      "Error rate: 0.075\n",
      "Cost at iteration i=14, j=10: 271.371892\n",
      "Error rate: 0.073\n",
      "Cost at iteration i=14, j=20: 270.308303\n",
      "Error rate: 0.072\n",
      "Cost at iteration i=14, j=30: 268.564415\n",
      "Error rate: 0.071\n",
      "Cost at iteration i=14, j=40: 267.789687\n",
      "Error rate: 0.071\n",
      "Cost at iteration i=14, j=50: 267.034406\n",
      "Error rate: 0.072\n",
      "Cost at iteration i=14, j=60: 267.084341\n",
      "Error rate: 0.072\n",
      "Cost at iteration i=14, j=70: 266.738824\n",
      "Error rate: 0.072\n",
      "Cost at iteration i=14, j=80: 266.867908\n",
      "Error rate: 0.072\n",
      "Cost at iteration i=15, j=0: 266.586202\n",
      "Error rate: 0.072\n",
      "Cost at iteration i=15, j=10: 265.324583\n",
      "Error rate: 0.073\n",
      "Cost at iteration i=15, j=20: 264.320850\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=15, j=30: 262.651905\n",
      "Error rate: 0.069\n",
      "Cost at iteration i=15, j=40: 261.899716\n",
      "Error rate: 0.069\n",
      "Cost at iteration i=15, j=50: 261.222475\n",
      "Error rate: 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=15, j=60: 261.331208\n",
      "Error rate: 0.071\n",
      "Cost at iteration i=15, j=70: 261.051562\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=15, j=80: 261.249319\n",
      "Error rate: 0.069\n",
      "Cost at iteration i=16, j=0: 260.981141\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=16, j=10: 259.783343\n",
      "Error rate: 0.07\n",
      "Cost at iteration i=16, j=20: 258.832490\n",
      "Error rate: 0.069\n",
      "Cost at iteration i=16, j=30: 257.226516\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=16, j=40: 256.488256\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=16, j=50: 255.862730\n",
      "Error rate: 0.068\n",
      "Cost at iteration i=16, j=60: 256.024618\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=16, j=70: 255.811565\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=16, j=80: 256.070337\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=17, j=0: 255.811756\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=17, j=10: 254.670914\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=17, j=20: 253.769707\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=17, j=30: 252.220836\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=17, j=40: 251.491022\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=17, j=50: 250.916364\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=17, j=60: 251.104907\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=17, j=70: 250.938222\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=17, j=80: 251.247446\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=18, j=0: 250.999108\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=18, j=10: 249.921678\n",
      "Error rate: 0.066\n",
      "Cost at iteration i=18, j=20: 249.067338\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=18, j=30: 247.566771\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=18, j=40: 246.841821\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=18, j=50: 246.312475\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=18, j=60: 246.531803\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=18, j=70: 246.429465\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=18, j=80: 246.780833\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=19, j=0: 246.542556\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=19, j=10: 245.513146\n",
      "Error rate: 0.063\n",
      "Cost at iteration i=19, j=20: 244.702525\n",
      "Error rate: 0.063\n",
      "Cost at iteration i=19, j=30: 243.242995\n",
      "Error rate: 0.063\n",
      "Cost at iteration i=19, j=40: 242.525352\n",
      "Error rate: 0.064\n",
      "Cost at iteration i=19, j=50: 242.041988\n",
      "Error rate: 0.063\n",
      "Cost at iteration i=19, j=60: 242.283109\n",
      "Error rate: 0.062\n",
      "Cost at iteration i=19, j=70: 242.221064\n",
      "Error rate: 0.062\n",
      "Cost at iteration i=19, j=80: 242.613655\n",
      "Error rate: 0.063\n",
      "Final error rate: 0.063\n",
      "-------------------- END --------------------------\n",
      "Cost at iteration i=0, j=0: 1429.831377\n",
      "Error rate: 0.481\n",
      "Cost at iteration i=0, j=10: 454.908593\n",
      "Error rate: 0.124\n",
      "Cost at iteration i=0, j=20: 368.433712\n",
      "Error rate: 0.106\n",
      "Cost at iteration i=0, j=30: 335.060221\n",
      "Error rate: 0.094\n",
      "Cost at iteration i=0, j=40: 317.354932\n",
      "Error rate: 0.084\n",
      "Cost at iteration i=0, j=50: 285.195300\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=0, j=60: 278.112930\n",
      "Error rate: 0.065\n",
      "Cost at iteration i=0, j=70: 267.913781\n",
      "Error rate: 0.067\n",
      "Cost at iteration i=0, j=80: 257.421832\n",
      "Error rate: 0.063\n",
      "Cost at iteration i=1, j=0: 250.972946\n",
      "Error rate: 0.061\n",
      "Cost at iteration i=1, j=10: 242.109657\n",
      "Error rate: 0.06\n",
      "Cost at iteration i=1, j=20: 217.968138\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=1, j=30: 216.282807\n",
      "Error rate: 0.059\n",
      "Cost at iteration i=1, j=40: 210.880195\n",
      "Error rate: 0.057\n",
      "Cost at iteration i=1, j=50: 197.931513\n",
      "Error rate: 0.052\n",
      "Cost at iteration i=1, j=60: 198.152167\n",
      "Error rate: 0.056\n",
      "Cost at iteration i=1, j=70: 194.639469\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=1, j=80: 195.088528\n",
      "Error rate: 0.055\n",
      "Cost at iteration i=2, j=0: 196.931471\n",
      "Error rate: 0.053\n",
      "Cost at iteration i=2, j=10: 187.496847\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=2, j=20: 178.766093\n",
      "Error rate: 0.051\n",
      "Cost at iteration i=2, j=30: 179.750454\n",
      "Error rate: 0.052\n",
      "Cost at iteration i=2, j=40: 174.499068\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=2, j=50: 165.515506\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=2, j=60: 169.074676\n",
      "Error rate: 0.05\n",
      "Cost at iteration i=2, j=70: 165.947329\n",
      "Error rate: 0.052\n",
      "Cost at iteration i=2, j=80: 165.225039\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=3, j=0: 171.558103\n",
      "Error rate: 0.047\n",
      "Cost at iteration i=3, j=10: 163.160454\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=3, j=20: 157.038011\n",
      "Error rate: 0.046\n",
      "Cost at iteration i=3, j=30: 156.892500\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=3, j=40: 152.753240\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=3, j=50: 146.179653\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=3, j=60: 150.537265\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=3, j=70: 149.163336\n",
      "Error rate: 0.048\n",
      "Cost at iteration i=3, j=80: 147.216821\n",
      "Error rate: 0.045\n",
      "Cost at iteration i=4, j=0: 146.257317\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=4, j=10: 142.539154\n",
      "Error rate: 0.041\n",
      "Cost at iteration i=4, j=20: 137.187261\n",
      "Error rate: 0.042\n",
      "Cost at iteration i=4, j=30: 137.748352\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=4, j=40: 134.098645\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=4, j=50: 128.467946\n",
      "Error rate: 0.034\n",
      "Cost at iteration i=4, j=60: 133.984431\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=4, j=70: 136.973935\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=4, j=80: 134.721474\n",
      "Error rate: 0.044\n",
      "Cost at iteration i=5, j=0: 131.556042\n",
      "Error rate: 0.04\n",
      "Cost at iteration i=5, j=10: 131.077307\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=5, j=20: 127.131527\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=5, j=30: 127.285301\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=5, j=40: 124.048063\n",
      "Error rate: 0.036\n",
      "Cost at iteration i=5, j=50: 119.117145\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=5, j=60: 125.321177\n",
      "Error rate: 0.039\n",
      "Cost at iteration i=5, j=70: 128.311458\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=5, j=80: 126.579618\n",
      "Error rate: 0.038\n",
      "Cost at iteration i=6, j=0: 123.453278\n",
      "Error rate: 0.037\n",
      "Cost at iteration i=6, j=10: 124.263812\n",
      "Error rate: 0.036\n",
      "Cost at iteration i=6, j=20: 120.574262\n",
      "Error rate: 0.035\n",
      "Cost at iteration i=6, j=30: 120.063870\n",
      "Error rate: 0.029\n",
      "Cost at iteration i=6, j=40: 117.772522\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=6, j=50: 112.770333\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=6, j=60: 119.594800\n",
      "Error rate: 0.034\n",
      "Cost at iteration i=6, j=70: 122.178600\n",
      "Error rate: 0.034\n",
      "Cost at iteration i=6, j=80: 120.058653\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=7, j=0: 116.763430\n",
      "Error rate: 0.035\n",
      "Cost at iteration i=7, j=10: 118.637394\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=7, j=20: 115.499167\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=7, j=30: 114.055756\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=7, j=40: 112.957530\n",
      "Error rate: 0.03\n",
      "Cost at iteration i=7, j=50: 108.240589\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=7, j=60: 115.594634\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=7, j=70: 117.531628\n",
      "Error rate: 0.031\n",
      "Cost at iteration i=7, j=80: 115.025904\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=8, j=0: 111.770445\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=8, j=10: 114.691682\n",
      "Error rate: 0.03\n",
      "Cost at iteration i=8, j=20: 111.889390\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=8, j=30: 110.112787\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=8, j=40: 109.765102\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=8, j=50: 105.497554\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=8, j=60: 112.986851\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=8, j=70: 114.666603\n",
      "Error rate: 0.029\n",
      "Cost at iteration i=8, j=80: 111.367872\n",
      "Error rate: 0.03\n",
      "Cost at iteration i=9, j=0: 107.945927\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=9, j=10: 111.354067\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=9, j=20: 109.072213\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=9, j=30: 106.807143\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=9, j=40: 107.571506\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=9, j=50: 103.349432\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=9, j=60: 111.013331\n",
      "Error rate: 0.032\n",
      "Cost at iteration i=9, j=70: 112.191261\n",
      "Error rate: 0.03\n",
      "Cost at iteration i=9, j=80: 108.745286\n",
      "Error rate: 0.03\n",
      "Cost at iteration i=10, j=0: 105.361627\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=10, j=10: 109.149417\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=10, j=20: 107.016831\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=10, j=30: 104.574305\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=10, j=40: 106.333513\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=10, j=50: 101.565126\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=10, j=60: 109.189592\n",
      "Error rate: 0.029\n",
      "Cost at iteration i=10, j=70: 109.793487\n",
      "Error rate: 0.029\n",
      "Cost at iteration i=10, j=80: 106.183713\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=11, j=0: 103.140477\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=11, j=10: 106.939493\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=11, j=20: 105.466925\n",
      "Error rate: 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration i=11, j=30: 103.199691\n",
      "Error rate: 0.022\n",
      "Cost at iteration i=11, j=40: 105.352864\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=11, j=50: 100.892007\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=11, j=60: 108.047685\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=11, j=70: 108.382836\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=11, j=80: 104.671830\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=12, j=0: 101.821889\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=12, j=10: 105.530988\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=12, j=20: 104.361315\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=12, j=30: 102.162426\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=12, j=40: 104.502238\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=12, j=50: 100.564040\n",
      "Error rate: 0.022\n",
      "Cost at iteration i=12, j=60: 106.990028\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=12, j=70: 107.347621\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=12, j=80: 103.665560\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=13, j=0: 101.063960\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=13, j=10: 104.394032\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=13, j=20: 103.625851\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=13, j=30: 101.374606\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=13, j=40: 104.094880\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=13, j=50: 100.455384\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=13, j=60: 106.083151\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=13, j=70: 106.145867\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=13, j=80: 102.808616\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=14, j=0: 100.279541\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=14, j=10: 103.440940\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=14, j=20: 103.004027\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=14, j=30: 100.978731\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=14, j=40: 103.401630\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=14, j=50: 100.503567\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=14, j=60: 105.412700\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=14, j=70: 105.559400\n",
      "Error rate: 0.028\n",
      "Cost at iteration i=14, j=80: 102.288629\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=15, j=0: 99.861687\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=15, j=10: 103.028915\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=15, j=20: 102.648328\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=15, j=30: 100.550293\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=15, j=40: 102.756817\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=15, j=50: 100.567890\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=15, j=60: 104.695999\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=15, j=70: 104.824004\n",
      "Error rate: 0.027\n",
      "Cost at iteration i=15, j=80: 101.996842\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=16, j=0: 99.622131\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=16, j=10: 102.848118\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=16, j=20: 102.467089\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=16, j=30: 100.610957\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=16, j=40: 102.232551\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=16, j=50: 100.772664\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=16, j=60: 104.194316\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=16, j=70: 104.101539\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=16, j=80: 101.837781\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=17, j=0: 99.568919\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=17, j=10: 102.602494\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=17, j=20: 102.351542\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=17, j=30: 100.541668\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=17, j=40: 102.112419\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=17, j=50: 100.918178\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=17, j=60: 104.311982\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=17, j=70: 104.031731\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=17, j=80: 101.917995\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=18, j=0: 99.733512\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=18, j=10: 102.493752\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=18, j=20: 102.359240\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=18, j=30: 100.844271\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=18, j=40: 102.099821\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=18, j=50: 101.079690\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=18, j=60: 104.000797\n",
      "Error rate: 0.025\n",
      "Cost at iteration i=18, j=70: 103.685905\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=18, j=80: 101.993724\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=19, j=0: 99.755071\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=19, j=10: 102.584047\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=19, j=20: 102.519738\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=19, j=30: 100.922513\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=19, j=40: 102.145615\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=19, j=50: 101.415875\n",
      "Error rate: 0.023\n",
      "Cost at iteration i=19, j=60: 103.763164\n",
      "Error rate: 0.026\n",
      "Cost at iteration i=19, j=70: 103.441646\n",
      "Error rate: 0.024\n",
      "Cost at iteration i=19, j=80: 102.079884\n",
      "Error rate: 0.024\n",
      "Final error rate: 0.023\n",
      "-------------------- END --------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120845cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    max_iter = 20    \n",
    "    print_period = 10\n",
    "    \n",
    "    X, Y = get_normalized_data()\n",
    "    lr = 0.00004\n",
    "    reg = 0.01\n",
    "    \n",
    "    # get train/test data\n",
    "    Xtrain = X[:-1000,]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest  = X[-1000:,]\n",
    "    Ytest  = Y[-1000:]\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "    Ytest_ind = y2indicator(Ytest)\n",
    "    \n",
    "    N, D = Xtrain.shape\n",
    "    batch_sz = 500\n",
    "    n_batches = N // batch_sz\n",
    "    \n",
    "    # 300 hidden units, 10 output classes, then initialize our weights\n",
    "    M = 300\n",
    "    K = 10\n",
    "    W1 = np.random.randn(D, M) / 28\n",
    "    b1 = np.zeros(M)\n",
    "    W2 = np.random.randn(M, K) / np.sqrt(M)\n",
    "    b2 = np.zeros(K)\n",
    "    \n",
    "    # --------------------- 1. Constant Learning Rate -----------------------------\n",
    "    # cost = -16\n",
    "    LL_batch = []\n",
    "    CR_batch = []\n",
    "    for i in range(max_iter):         # looping through total iterations\n",
    "        for j in range(n_batches):    # looping through number of batches \n",
    "            Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "            Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "            pYbatch, Z = forward(Xbatch, W1, b1, W2, b2)\n",
    "\n",
    "            # weight and bias updates\n",
    "            W2 -= lr*(derivative_w2(Z, Ybatch, pYbatch) + reg*W2)\n",
    "            b2 -= lr*(derivative_b2(Ybatch, pYbatch) + reg*b2)\n",
    "            W1 -= lr*(derivative_w1(Xbatch, Z, Ybatch, pYbatch, W2) + reg*W1)\n",
    "            b1 -= lr*(derivative_b1(Z, Ybatch, pYbatch, W2) + reg*b1)\n",
    "            \n",
    "            # print/debugging step\n",
    "            if j % print_period == 0:\n",
    "                # calculate just for LL\n",
    "                pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "                # print \"pY:\", pY\n",
    "                ll = cost(pY, Ytest_ind)\n",
    "                LL_batch.append(ll)\n",
    "                print(\"Cost at iteration i=%d, j=%d: %.6f\" % (i, j, ll))\n",
    "\n",
    "                err = error_rate(pY, Ytest)\n",
    "                CR_batch.append(err)\n",
    "                print(\"Error rate:\", err)\n",
    "\n",
    "    pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "    print(\"Final error rate:\", error_rate(pY, Ytest))\n",
    "    print('-------------------- END --------------------------')\n",
    "\n",
    "\n",
    "    # --------------------- 2. RMSProp -----------------------------\n",
    "    W1 = np.random.randn(D, M) / 28\n",
    "    b1 = np.zeros(M)\n",
    "    W2 = np.random.randn(M, K) / np.sqrt(M)\n",
    "    b2 = np.zeros(K)\n",
    "    LL_rms = []\n",
    "    CR_rms = []\n",
    "    lr0 = 0.001         # Initial Learning Rate. If you set this too high you'll get NaN!\n",
    "    cache_W2 = 1        # storing cache for each of our weights\n",
    "    cache_b2 = 1\n",
    "    cache_W1 = 1\n",
    "    cache_b1 = 1  \n",
    "    decay_rate = 0.999        # our decay rate parameter and epsilon\n",
    "    eps = 1e-10\n",
    "    \n",
    "    # calculate batches in same way \n",
    "    for i in range(max_iter):\n",
    "        for j in range(n_batches):\n",
    "            Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "            Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "            pYbatch, Z = forward(Xbatch, W1, b1, W2, b2)              # forward pass\n",
    "            \n",
    "            # weight and bias updates, WITH RMSProp\n",
    "            gW2 = derivative_w2(Z, Ybatch, pYbatch) + reg*W2\n",
    "            cache_W2 = decay_rate*cache_W2 + (1 - decay_rate)*gW2*gW2 # elem by elem mult\n",
    "            W2 -= lr0 * gW2 / (np.sqrt(cache_W2) + eps)\n",
    "    \n",
    "            gb2 = derivative_b2(Ybatch, pYbatch) + reg*b2\n",
    "            cache_b2 = decay_rate*cache_b2 + (1 - decay_rate)*gb2*gb2\n",
    "            b2 -= lr0 * gb2 / (np.sqrt(cache_b2) + eps)\n",
    "\n",
    "            gW1 = derivative_w1(Xbatch, Z, Ybatch, pYbatch, W2) + reg*W1\n",
    "            cache_W1 = decay_rate*cache_W1 + (1 - decay_rate)*gW1*gW1\n",
    "            W1 -= lr0 * gW1 / (np.sqrt(cache_W1) + eps)\n",
    "\n",
    "            gb1 = derivative_b1(Z, Ybatch, pYbatch, W2) + reg*b1\n",
    "            cache_b1 = decay_rate*cache_b1 + (1 - decay_rate)*gb1*gb1\n",
    "            b1 -= lr0 * gb1 / (np.sqrt(cache_b1) + eps)\n",
    "            \n",
    "            if j % print_period == 0:\n",
    "                # calculate just for LL\n",
    "                pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "                # print \"pY:\", pY\n",
    "                ll = cost(pY, Ytest_ind)\n",
    "                LL_rms.append(ll)\n",
    "                print(\"Cost at iteration i=%d, j=%d: %.6f\" % (i, j, ll))\n",
    "\n",
    "                err = error_rate(pY, Ytest)\n",
    "                CR_rms.append(err)\n",
    "                print(\"Error rate:\", err)\n",
    "\n",
    "    pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "    print(\"Final error rate:\", error_rate(pY, Ytest))\n",
    "    print('-------------------- END --------------------------')\n",
    "\n",
    "    plt.plot(LL_batch, label='const')\n",
    "    plt.plot(LL_rms, label='rms')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "# 6. Adam Optimizer \n",
    "One of the newest and most common optimization techniques these days is known as the **Adam Optimizer**. It is going to be given it's own section all together, since as of 2017 it is often considered the go to optimizer for those doing deep learning. Sometimes, people will talk about **Adam** by saying it is like RMSprop, but *with* momentum. This can be slightly confusing though, considering that TensorFlow has RMSprop, and allows you to add momentum to it, which is *not* Adam. In other words, you can have RMSprop with momentum, but that doesn't give you Adam-it just gives you RMSprop with momentum. Adam does, in a sense, add something like momentum to RMSprop, but in a very specific way. In order to fully understand **Adam**, we first need to look at part of the theory that it is dependent on: **Exponentially-Smoothed Averages**. \n",
    "\n",
    "<br>\n",
    "## 6.1 Exponentially Smoothed Averages \n",
    "We are going to take a minute to dig into something that may seem straight forward: How to calculate an average. The first thought we all have when being asked to do this is: Why not just add all of the sample data points, and then divide by the number of data points, resulting in the sample mean:\n",
    "\n",
    "#### $$\\bar{X}_N = \\frac{1}{N}\\sum_{n=1}^NX_n$$\n",
    "\n",
    "But now let's suppose that you have a large amount of data-so much so that all of your X's cannot fit into memory at the same time. Is it still possible to calculate the sample mean? Yes it is! We can read in one data point at a time, and then delete each data point after we've looked at it. It is shown below that the current sample mean can actually be expressed in terms of the previous sample mean and the current data point.\n",
    "\n",
    "#### $$\\bar{X}_N =  \\frac{1}{N}\\sum_{n=1}^NX_n = \\frac{1}{N}\\Big((N-1)\\bar{X}_{N-1} + X_N \\Big) = (1 - \\frac{1}{N})\\bar{X}_{N-1}+\\frac{1}{N}X_N$$\n",
    "\n",
    "We can then express this using simpler symbols. We can call $Y$ our output, and we can use $t$ to represent the current time step:\n",
    "\n",
    "#### $$Y_t = (1 - \\frac{1}{t})Y_{t-1} + \\frac{1}{t}X_t$$\n",
    "\n",
    "Great, so we have solved our problem of how to calculate the sample mean when we can't fit all of the data into memory, but we can see that there is this $\\frac{1}{t}$ term. This says that as $t$ grows larger, the current sample has less and less of an effect on the total mean. Clearly this makes sense, because as $t$ grows that means the total number of $X$'s we've seen has grown. We also decrease the influence of the previous $Y$ by $1 - \\frac{1}{t}$. This means that each new $Y$ is just part of the old $Y$, plus part of the newest $X$. But in the end, it balances out to give us exactly the sample mean of $X$. \n",
    "\n",
    "For convenience we can call this $\\frac{1}{t}$ term $\\alpha_t$. What if we were to say that we did not want $\\alpha$ to be $\\frac{1}{t}$? What if we said that we wanted each data point to matter equally at the time that we see it, so that we can set alpha to be a constant? Of course, $\\alpha$ needs to be less than 1 so that we don't end up negating the previous mean. \n",
    "\n",
    "#### $$0 < \\alpha_t = constant < 1 $$\n",
    "#### $$Y_t = (1 - \\alpha)Y_{t-1} + \\alpha X_t$$\n",
    "\n",
    "So what does this give us? \n",
    "\n",
    "### 6.1.1 The Exponentially-smoothed average\n",
    "This gives us what is called the exponentially smoothed average! We can see why it is called exponential when we express it in terms of only $X$'s. \n",
    "\n",
    "#### $$Y_t = (1 - \\alpha)^tY_0 + \\alpha \\sum_{\\tau = 0}^{t - 1}(1- \\alpha)^\\tau X(t- \\tau)$$\n",
    "\n",
    "If the equation above is not clear, the expansion below should clear up where everything is coming from and *why* this is called exponential. Let's say we are looking at $Y_{100}$:\n",
    "\n",
    "#### $$Y_{100} = (1-\\alpha)^{100}Y_0 + \\alpha * X_{100} + \\alpha * (1 - \\alpha)^1*X_{99} + \\alpha * (1 - \\alpha)^2 * X_{98}+ ...$$\n",
    "\n",
    "We can see the exponential term start to accumulate along the $(1 - alpha)$! Now, does this still give us the mean, aka the expected value of $X$? Well, if you take the expected value of everything, we can see that we arrive at the expected value of $X$:\n",
    "\n",
    "#### $$(1 - \\alpha)E[y(t-1)] + \\alpha E[X(t)] = (1-\\alpha)E(X) + \\alpha E(X) = E(X)$$\n",
    "\n",
    "We do arrive at the expected value of $X$, so we can see that the math does checkout! Of course, this is assuming that the distribution of $X$ does not change over time. Note that if you have come from a signal processing background, you may recognize this as a **low-pass filter**. Another way to think about this is that you are saying *current values matter more*, and *past values matter less* in an exponentially decreasing way. So, if $X$ is not stationary (meaning it's distribution changes over time), then this is actually a better way to estimate the mean (average) then weighting all data points equally over all time.\n",
    "\n",
    "## 6.2 Exponentially Smoothed Average's and Adam\n",
    "Now, how does this apply to the **Adam Optimizer**? Well, if we call the sample mean of $T$ samples, $M_T$, we write $M_T$ as:\n",
    "#### $$M_T = \\frac{1}{T} \\sum_{t=1}^TX_t = \\frac{1}{T}\\sum_{t=1}^{T-1}X_t+\\frac{1}{T}X_T = (1 - \\frac{1}{T})M_{T-1}+\\frac{1}{T}X_T$$\n",
    "\n",
    "Where $X_T$ is the new sample, and $M_{T-1}$ is the previous mean. This means that instead of needing to store all of the $X$'s, all we need to store is the current $X$ and the previous $M$, and then we calculate the current $M$. \n",
    "\n",
    "Now, you may notice that there is this $\\frac{1}{T}$ term that occurs in the equation. It is reasonable to ask: \"What is we set this term to be a constant?\" Then, we get back **precisely** what looks like the **cache update** from **RMSProp**!\n",
    "\n",
    "#### $$M_T = decay*M_{T-1} + (1 - decay)X_T$$\n",
    "\n",
    "## 6.3 What is the cache of RMSProp estimating?\n",
    "At this point, it is a very reasonable thing to ask: \"What is the cache of RMSProp really estimating?\" Well, it is really estimating the mean of the square of the gradient! \n",
    "\n",
    "<img src=\"images/exp-av-mean.png\">\n",
    "\n",
    "<img src=\"images/exp-av-cache.png\">\n",
    "\n",
    "#### $$v_t = decay * v_{t-1} + (1 - decay)*g^2 \\approx mean(g^2)$$\n",
    "\n",
    "And because we eventually take the square root of the cache, now you can see where RMSProp gets its name (RMS= \"root mean square\").\n",
    "\n",
    "## 6.4 Second Moment\n",
    "Also, it is worth remembering that the **mean** of a **random variable** can also be phrased as an **expected value**, aka: $mean(X) = E(X)$. In particular, when we take the expected value of the square of a random variable, we get the **second moment**:\n",
    "#### $$E(X^2) = 2nd \\;moment\\;of\\;X$$\n",
    "\n",
    "So, $E(g^2)$ is the second moment of the gradient, and we calculate it using this exponentially smoothed average. We are going to refer to it as $v$ from now on, since the **2nd central moment** of a random variable is the definition of variance. \n",
    "\n",
    "#### $$v \\approx E(g^2)$$\n",
    "\n",
    "#### $$M_T = (1 - \\frac{1}{T})M_{T-1}+\\frac{1}{T}X_T$$\n",
    "\n",
    "#### $$v_t = decay * v_{t-1} + (1 - decay)*g^2$$\n",
    "\n",
    "## 6.5 First Moment\n",
    "Now, since there is a second moment, it is reasonable to ask: is there a **first moment**? Yes there is! It is just the expected value of $g$.\n",
    "\n",
    "#### $$m_t = \\mu m_{t-1} + (1 - \\mu)g_t$$\n",
    "#### $$m \\approx E(g)$$\n",
    "\n",
    "We will call this $m$ because the first moment of a random variable is the definition of the **mean**. Because the **Adam Update** makes use of the 1st and 2nd moments of $g$ using exponentially smoothed estimates, this also explains the name of the algorithm: **Adaptive Moment Estimation**. \n",
    "\n",
    "Also, note that the update for $m$ looks a lot like momentum! Hence, why some people refer to this as **RMSProp** with momentum. Usually momentum is just implemented by adding a momentum parameter to the velocity rather than doing this weighted average, as a reminder it usually looks like:\n",
    "\n",
    "#### $$m_t = \\mu m_{t-1} + g_t$$\n",
    "\n",
    "## 6.6 One final problem\n",
    "So to get to our final destination, which is the **Adam Update**, we are going to combine these two pieces: the first and second moments of $g$, or as we have been calling them, $m$ and $v$ (mean and variance). However, before we do that, there is one last important concept to discuss. The exponentially smoothed average acts very much like a smoothing function, ignoring much of the variation. It will ignore the rapid very random spikes, but it will follow the main trend of the signal, the original sine wave. In the field of signal processing this is known as a low pass filter, because the high frequency changes are being filtered out. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/low-pass-filter.png\">\n",
    "\n",
    "The problem with exponential smoothing is just like RMSProp, what is the initial value? The output is supposed to depend on the current value of the input plus the last value of the output. But since there is no last value of the output for $t = 0 $, we typically just set that to 0. But in doing so, we make the first value of the output small:\n",
    "#### $$Y(0)= 0$$\n",
    "#### $$Y(1) = 0.99 * Y(0) + 0.01*X(1) = 0.01*X(1)$$\n",
    "\n",
    "This means that initially your output will be biased towards 0! \n",
    "\n",
    "<img src=\"images/bias-zero.png\">\n",
    "\n",
    "## 6.7 Bias Correction\n",
    "Is there anything that we can do about the issue presented above? Well, there is a technique, **bias correction**, that makes up for this. \n",
    "\n",
    "#### $$\\hat{Y}(t) = \\frac{Y(t)}{1 - decay^t}$$\n",
    "\n",
    "We divide by $(1 - decay)^t$. As you can see, when $t$ is small we are dividing by a very small number, which makes the initial value bigger. This is good because the initial value was too small. \n",
    "\n",
    "Let's go through an example to make things very clear. Given a decay of 0.99, our original output would be:\n",
    "\n",
    "#### $$Y(1) = 0.01*X(1)$$\n",
    "\n",
    "And $Y(2)$ would be:\n",
    "\n",
    "#### $$Y(2) = 0.0099*X(1) + 0.01*X(2)$$\n",
    "\n",
    "But, if we correct it, then we have:\n",
    "\n",
    "#### $$\\hat{Y}(1) = \\frac{0.01}{1 - 0.99^1}*X(1) = X(1)$$\n",
    "\n",
    "And our corrected $\\hat{Y}(2)$:\n",
    "\n",
    "#### $$\\hat{Y}(2) = \\frac{Y(2)}{0.0199} = 0.497*X(1) + 0.503*X(2)$$\n",
    "\n",
    "This also makes sense because it is about half of $X(1)$ plus about half of $X(2)$, with a bit more weight on the current value. We can also see that as $t$ approaches infinity, the bias correction term approaches 1, meaning that we quickly converge to the standard exponentially smooth average. The important part is that our output now has the correct range of values when compared with the input, for all values of the input, including the initial ones. \n",
    "\n",
    "## 6.8 Back to Adam\n",
    "Now that we have discussed bias correction for exponentially smoothed averages, you might guess that we are going to apply it to the exponentially smoothed averages that we have been talking about, in particular $m$ and $v$. So, if we call $\\hat{m}$ the bias corrected version of $m$, and $\\hat{v}$ the bias corrected version of $v$, then we can formulate our **Adam Update**. \n",
    "\n",
    "#### $$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "\n",
    "#### $$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "Note, there are two different decay rates for $m$ and $v$, which we call $\\beta_1$ and $\\beta_2$. Typical values are in the range of 0.9 to 0.99. $\\epsilon$ again has values in the range of $10^{-8}$ to $10^{-10}$. Our final update for $w$ is:\n",
    "\n",
    "#### $$w_{t+1} \\leftarrow w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t+\\epsilon}}$$\n",
    "\n",
    "## 6.9 Recap\n",
    "Now that was a ton of stuff, so lets recap! **Adam** is a new, modern adaptive learning rate technique that is the default go to for many deep learning practioners. It combines 2 proven techniques: \n",
    "> 1. **RMSProp** which is a **cache mechanism** (leaky cache)\n",
    "2. **Momentum** which speeds up training just by continuing to go in the same direction it was going before (keeping track of old gradients) \n",
    "\n",
    "We saw that these two methods can be generalized, by observing that they estimate the first and second moments of the gradient. **Adam** also adds bias correction, which makes up for the fact that the exponentially smoothed average starts at zero, and hence the initial estimates are biased towards 0. \n",
    "\n",
    "#### $$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "\n",
    "#### $$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "#### $$w_{t+1} \\leftarrow w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t+\\epsilon}}$$\n",
    "\n",
    "## 6.10 Summary Pseudocode\n",
    "At every batch (typical $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$):\n",
    "\n",
    "#### $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$$\n",
    "#### $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2$$\n",
    "#### $$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "#### $$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "#### $$w_{t+1} \\leftarrow w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t+\\epsilon}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
