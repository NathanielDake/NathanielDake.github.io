{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Batch Normalization\n",
    "At this point we should know that normalizing our data for machine learning models is typically a good idea. That is when we make all of the input features have mean of 0 and variance of 1. \n",
    "\n",
    "We accomplish this by subtracting by the mean and dividing by the standard deviation of the data. \n",
    "\n",
    "#### $$X_{normalized} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Recall, the reason we want to do this is because this is the region where our nonlinear activation functions are the most active/dynamic - aka that is where they change the most. \n",
    "\n",
    "So, how can we think of batch normalization? Well, instead of making normalization part of the preprocessing stage, we make it part of the neural network itself. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/old-norm.png\">\n",
    "\n",
    "<img src=\"images/new-norm.png\">\n",
    "\n",
    "However, what makes this useful is that we perform normalization at every layer! Recall, that each layer of a neural network is like a little logisitic regression. So rather than just normalize the data once, we will normalize it before we do every little logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 2. Exponentially Smoothed Averages\n",
    "We are going to take a minute to dig into something that may seem straight forward: How to calculate an average. The first thought we all have when being asked to do this is: Why not just add all of the sample data points, and then divide by the number of data points, resulting in the sample mean:\n",
    "\n",
    "#### $$\\bar{X}_N = \\frac{1}{N}\\sum_{n=1}^NX_n$$\n",
    "\n",
    "But now let's suppose that you have a large amount of data-so much so that all of your X's cannot fit into memory at the same time. Is it still possible to calculate the sample mean? Yes it is! We can read in one data point at a time, and then delete each data point after we've looked at it. It is shown below that the current sample mean can actually be expressed in terms of the previous sample mean and the current data point.\n",
    "\n",
    "#### $$\\bar{X}_N =  \\frac{1}{N}\\sum_{n=1}^NX_n = \\frac{1}{N}\\Big((N-1)\\bar{X}_{N-1} + X_N \\Big) = (1 - \\frac{1}{N})\\bar{X}_{N-1}+\\frac{1}{N}X_N$$\n",
    "\n",
    "We can then express this using simpler symbols. We can call $Y$ our output, and we can use $t$ to represent the current time step:\n",
    "\n",
    "#### $$Y_t = (1 - \\frac{1}{t})Y_{t-1} + \\frac{1}{t}X_t$$\n",
    "\n",
    "Great, so we have solved our problem of how to calculate the sample mean when we can't fit all of the data into memory, but we can see that there is this $\\frac{1}{t}$ term. This says that as $t$ grows larger, the current sample has less and less of an effect on the total mean. Clearly this makes sense, because as $t$ grows that means the total number of $X$'s we've seen has grown. We also decrease the influence of the previous $Y$ by $1 - \\frac{1}{t}$. This means that each new $Y$ is just part of the old $Y$, plus part of the newest $X$. But in the end, it balances out to give us exactly the sample mean of $X$. \n",
    "\n",
    "For convenience we can call this $\\frac{1}{t}$ term $\\alpha_t$. What if we were to say that we did not want $\\alpha$ to be $\\frac{1}{t}$? What if we said that we wanted each data point to matter equally at the time that we see it, so that we can set alpha to be a constant? Of course, $\\alpha$ needs to be less than 1 so that we don't end up negating the previous mean. \n",
    "\n",
    "#### $$0 < \\alpha_t = constant < 1 $$\n",
    "#### $$Y_t = (1 - \\alpha)Y_{t-1} + \\alpha X_t$$\n",
    "\n",
    "So what does this give us? \n",
    "\n",
    "### 2.1 The Exponentially-smoothed average\n",
    "This gives us what is called the exponentially smoothed average! We can see why it is called exponential when we express it in terms of only $X$'s. \n",
    "\n",
    "#### $$Y_t = (1 - \\alpha)^tY_0 + \\alpha \\sum_{\\tau = 0}^{t - 1}(1- \\alpha)^\\tau X(t- \\tau)$$\n",
    "\n",
    "If the equation above is not clear, the expansion below should clear up where everything is coming from and *why* this is called exponential. Let's say we are looking at $Y_{100}$:\n",
    "\n",
    "#### $$Y_{100} = (1-\\alpha)^{100}Y_0 + \\alpha * X_{100} + \\alpha * (1 - \\alpha)^1*X_{99} + \\alpha * (1 - \\alpha)^2 * X_{98}+ ...$$\n",
    "\n",
    "We can see the exponential term start to accumulate along the $(1 - alpha)$! Now, does this still give us the mean, aka the expected value of $X$? Well, if you take the expected value of everything, we can see that we arrive at the expected value of $X$:\n",
    "\n",
    "#### $$(1 - \\alpha)E[y(t-1)] + \\alpha E[X(t)] = (1-\\alpha)E(X) + \\alpha E(X) = E(X)$$\n",
    "\n",
    "We do arrive at the expected value of $X$, so we can see that the math does checkout! Of course, this is assuming that the distribution of $X$ does not change over time. Note that if you have come from a signal processing background, you may recognize this as a **low-pass filter**. Another way to think about this is that you are saying *current values matter more*, and *past values matter less* in an exponentially decreasing way. So, if $X$ is not stationary (meaning it's distribution changes over time), then this is actually a better way to estimate the mean (average) then weighting all data points equally over all time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
