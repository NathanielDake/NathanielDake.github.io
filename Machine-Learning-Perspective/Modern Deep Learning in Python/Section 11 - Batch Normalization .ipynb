{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Batch Normalization\n",
    "At this point we should know that normalizing our data for machine learning models is typically a good idea. That is when we make all of the input features have mean of 0 and variance of 1. \n",
    "\n",
    "We accomplish this by subtracting by the mean and dividing by the standard deviation of the data. \n",
    "\n",
    "#### $$X_{normalized} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Recall, the reason we want to do this is because this is the region where our nonlinear activation functions are the most active/dynamic - aka that is where they change the most. \n",
    "\n",
    "So, how can we think of batch normalization? Well, instead of making normalization part of the preprocessing stage, we make it part of the neural network itself. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/old-norm.png\">\n",
    "\n",
    "<img src=\"images/new-norm.png\">\n",
    "\n",
    "However, what makes this useful is that we perform normalization at every layer! Recall, that each layer of a neural network is like a little logisitic regression. So rather than just normalize the data once, we will normalize it before we do every little logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 2. Exponentially Smoothed Averages\n",
    "We are going to take a minute to dig into something that may seem straight forward: How to calculate an average. The first thought we all have when being asked to do this is: Why not just add all of the sample data points, and then divide by the number of data points, resulting in the sample mean:\n",
    "\n",
    "#### $$\\bar{X}_N = \\frac{1}{N}\\sum_{n=1}^NX_n$$\n",
    "\n",
    "But now let's suppose that you have a large amount of data-so much so that all of your X's cannot fit into memory at the same time. Is it still possible to calculate the sample mean? Yes it is! We can read in one data point at a time, and then delete each data point after we've looked at it. It is shown below that the current sample mean can actually be expressed in terms of the previous sample mean and the current data point.\n",
    "\n",
    "#### $$\\bar{X}_N =  \\frac{1}{N}\\sum_{n=1}^NX_n = \\frac{1}{N}\\Big((N-1)\\bar{X}_{N-1} + X_N \\Big) = (1 - \\frac{1}{N})\\bar{X}_{N-1}+\\frac{1}{N}X_N$$\n",
    "\n",
    "We can then express this using simpler symbols. We can call $Y$ our output, and we can use $t$ to represent the current time step:\n",
    "\n",
    "#### $$Y_t = (1 - \\frac{1}{t})Y_{t-1} + \\frac{1}{t}X_t$$\n",
    "\n",
    "Great, so we have solved our problem of how to calculate the sample mean when we can't fit all of the data into memory, but we can see that there is this $\\frac{1}{t}$ term. This says that as $t$ grows larger, the current sample has less and less of an effect on the total mean. Clearly this makes sense, because as $t$ grows that means the total number of $X$'s we've seen has grown. We also decrease the influence of the previous $Y$ by $1 - \\frac{1}{t}$. This means that each new $Y$ is just part of the old $Y$, plus part of the newest $X$. But in the end, it balances out to give us exactly the sample mean of $X$. \n",
    "\n",
    "For convenience we can call this $\\frac{1}{t}$ term $\\alpha_t$. What if we were to say that we did not want $\\alpha$ to be $\\frac{1}{t}$? What if we said that we wanted each data point to matter equally at the time that we see it, so that we can set alpha to be a constant? Of course, $\\alpha$ needs to be less than 1 so that we don't end up negating the previous mean. \n",
    "\n",
    "#### $$0 < \\alpha_t = constant < 1 $$\n",
    "#### $$Y_t = (1 - \\alpha)Y_{t-1} + \\alpha X_t$$\n",
    "\n",
    "So what does this give us? \n",
    "\n",
    "### 2.1 The Exponentially-smoothed average\n",
    "This gives us what is called the exponentially smoothed average! We can see why it is called exponential when we express it in terms of only $X$'s. \n",
    "\n",
    "#### $$Y_t = (1 - \\alpha)^tY_0 + \\alpha \\sum_{\\tau = 0}^{t - 1}(1- \\alpha)^\\tau X(t- \\tau)$$\n",
    "\n",
    "If the equation above is not clear, the expansion below should clear up where everything is coming from and *why* this is called exponential. Let's say we are looking at $Y_{100}$:\n",
    "\n",
    "#### $$Y_{100} = (1-\\alpha)^{100}Y_0 + \\alpha * X_{100} + \\alpha * (1 - \\alpha)^1*X_{99} + \\alpha * (1 - \\alpha)^2 * X_{98}+ ...$$\n",
    "\n",
    "We can see the exponential term start to accumulate along the $(1 - alpha)$! Now, does this still give us the mean, aka the expected value of $X$? Well, if you take the expected value of everything, we can see that we arrive at the expected value of $X$:\n",
    "\n",
    "#### $$(1 - \\alpha)E[y(t-1)] + \\alpha E[X(t)] = (1-\\alpha)E(X) + \\alpha E(X) = E(X)$$\n",
    "\n",
    "We do arrive at the expected value of $X$, so we can see that the math does checkout! Of course, this is assuming that the distribution of $X$ does not change over time. Note that if you have come from a signal processing background, you may recognize this as a **low-pass filter**. Another way to think about this is that you are saying *current values matter more*, and *past values matter less* in an exponentially decreasing way. So, if $X$ is not stationary (meaning it's distribution changes over time), then this is actually a better way to estimate the mean (average) then weighting all data points equally over all time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 3. Batch Normalization - Theory\n",
    "Recall, before we input any of our data in ML algorithms, we like to normalize the data first. Normalization means subtracting the mean, and dividing by the standard deviation. This forces our data to have a mean of 0 and a variance of 1. We like to do this because it keeps our inputs in a specific range, and we know that our sigmoid and tanh functions are most saturated when the inputs are small. \n",
    "\n",
    "What **batch normalization** does, is that instead of you normalizing the data manually, before putting it into the neural network, normalization will occur at every layer of the neural network. You can think of it as building normalization into the neural net vs. doing the calculation yourself before it is input into the neural net. So, the normalization occurs at the neural network level, and not the data level. \n",
    "\n",
    "<br>\n",
    "## 3.1 How does it really work?\n",
    "First and foremost, it should be known that the name batch normalization comes from the fact that we will be doing batch gradient descent. In other words, during training we will be looking at a small batch of data and doing one gradient descent step on that, looking at the next batch, and so on. \n",
    "\n",
    "So, it is called batch normalization because the mean and standard deviation that we calculate are the sample mean and sample standard deviation of the batch. Keep in mind that this only applies during training, because it is only during training that we will have batches of data. \n",
    "\n",
    "```\n",
    "X_B = next batch of data\n",
    "mu_B = mean(X_B)\n",
    "sigma_B = std(X_B)\n",
    "Y_B = (X_B - mu_B) / sigma_B\n",
    "```\n",
    "\n",
    "You may still wonder, \"when does batch normalization actually happen?\" Well, batch normalization happens right before we pass it through the activation function. So, before we had two steps: \n",
    "1. Do the linear transformation\n",
    "2. Then pass it through the activation function. \n",
    "\n",
    "<img src=\"images/regular.png\">\n",
    "\n",
    "With batch gradient descent, we have 3 steps:\n",
    "1. Do the linear transformation\n",
    "2. Perform batch normalization \n",
    "3. Then pass it through the activation function\n",
    "\n",
    "<img src=\"images/batch-gradient.png\">\n",
    "\n",
    "<br>\n",
    "### 3.1.1 Naming Convention\n",
    "One important thing to remember is that we are going to call the input to batch normalization $X$, and the output $Y$. This may be confusing at first since generall the input to the neural network is referred to as $X$ and the output is referred to as $Y$. So, for this lecture only does $X$ refer to the inputs of batch normalization and $Y$ refers to the outputs of batch normalization. \n",
    "\n",
    "<br>\n",
    "## 3.2 Missing Step\n",
    "Now, so far this may seem very simple. You have a batch of activations, you calculate their mean and standard deviation, you standardize the activations, and then pass it through the activation function as normal. Note, we often add a small number to denominator in order to prevent dividing by 0 if the variance of the batch is 0.\n",
    "\n",
    "```\n",
    "X_B = next batch of data (Note: X refers to activation here)\n",
    "mu_B = mean(X_B)\n",
    "sigma_B_squared = var(X_B)\n",
    "Y_B = (X_B - mu_B)/ sqrt(sigma_B_squared + epsilon)\n",
    "```\n",
    "\n",
    "However, at this point we are missing one crucial and slightly counter intuitive step! This step comes right before we pass through the activation function. So, what was shown previously was actually missing a step. So, right after normalize the data, we actually scale it back. Right after standardizing the data, we actually scale it to something else, giving it a different mean and different standard deviation. We call this second scale parameter $\\gamma$, and the second location parameter $\\beta$. \n",
    "\n",
    "#### $$\\hat{X}_B = \\frac{X_B - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "#### $$Y = \\gamma\\hat{X}_B + \\beta$$\n",
    "\n",
    "This may seem very counterintuitive, because this entire concept was motivated with the idea that standardization is good. So, why are we now unstandardizing our data by multiplying by $\\gamma$ and adding $\\beta$? The reason is that standardization may not be good, but we do not know. So, **we let the neural network decide by using gradient descent**. In other words, we will optimize $\\gamma$ and $\\beta$ to do whatever is best. \n",
    "\n",
    "Note, these parameters will be updated using back propagation, just like all of the other weights in the neural network. Luckily, now that we know theano and tensorflow, we don't have to worry about taking any derivatives, and we can just focus on how the neural network was built. \n",
    "\n",
    "<br>\n",
    "## 3.3 Letting the Neural Network learn what is best\n",
    "So, lets suppose that standardization is good. Then, the neural network will learn that $\\gamma$ should be close to 1, and $\\beta$ should be close to 0. However, if standardization is bad and something else will lead to better results, then the neural network will learn a better $\\gamma$ and a better $\\beta$ - aka they will be whatever minimizes the cost! In other words, the neural network is learning through gradient descent what the best scale and shift of the data should be. This is not necessarily 1 and 0, it is whatever minimizes our cost function. \n",
    "\n",
    "## 3.4 Another Problem\n",
    "So, we now know how to train a neural network with batch gradient descent, but we still have another problem. Let's say it is now time to do prediction; we can say we have 1 sample input, and we would like to calculate a prediction for it. Well, we can't do what we have been doing! Because if we subtract the mean of one sample from the sample itself, we would just get a vector of 0s. So, clearly this is not what we want to do during test time. \n",
    "\n",
    "What would be nice is if we kept track of all the sample means and sample standard deviations we saw during training, we could calculate an overall (global) mean and overall (global) standard deviation, and use those during test time! This is exactly what we do; we keep a running mean and running variance! This will look similar to RMSprop and Adam Smoothing, since it is an exponentially smoothed average! We keep a running mean and running variance, and we do this using exponential smoothing. We will call our global mean $\\mu$ and our global variance $\\sigma^2$.\n",
    "\n",
    "```\n",
    "for each batch B:\n",
    "    mu = decay*mu + (1 - decay)*mu_B\n",
    "    sigma_squared = decay*sigma_squared + (1 - decay)*sigma_B_squared\n",
    "```\n",
    "\n",
    "Now, theoreticall you could just calculate the global mean and global variance to the be the mean and variance of all of your training data:\n",
    "#### $$\\mu = mean(X_{train})$$\n",
    "#### $$\\sigma^2 = var(X_{train})$$\n",
    "\n",
    "The problem with that is that it does not scale if your data is too large. At the same time, that may be a simpler way to think of $\\mu$ and $\\sigma$.\n",
    "\n",
    "## 3.5 Test/Prediction \n",
    "During test time, based on our above discussion:\n",
    "\n",
    "#### $$\\hat{x}_{test} = \\frac{x_{test} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "#### $$y_{test} = \\gamma \\hat{x}_{test} + \\beta$$\n",
    "\n",
    "Where $\\mu$ and $\\sigma$ are whatever we calculated during training. \n",
    "\n",
    "## 3.6 Implementation\n",
    "Now, in order to implement these, you could do it manually. However, there are functions that are built into both theano and tensorflow that can help us. For tensorflow that is:\n",
    "\n",
    "**TensorFlow Implementation**<br>\n",
    "```\n",
    "tf.nn.batch_normalization\n",
    "# or\n",
    "tf.contrib.layers.batch_norm\n",
    "```\n",
    "\n",
    "And for Theano:\n",
    "**Theano Implementation**<br>\n",
    "```\n",
    "from theano.tensor.nnet.bn import batch_normalization_train,\n",
    "batch_normalization_test\n",
    "```\n",
    "\n",
    "One thing to keep in mind is that these are all element wise operations. So, this applies equally to scalars and vectors. \n",
    "\n",
    "## 3.7 Theory\n",
    "So, after all of this discussion on the mechanics of batch normalization, you may still be wondering, how does batch normalization actually help? Well, the authors of the paper that introduced batch gradient descent mention **internal covariant shift**. That sounds very complicated, however, covariate is really just another term for input features, so we can call that X. By shift, they mean that as we perform batch gradient descent the distribution of X may change as we change the network. Aka, this is saying that the distribution of input features can change during training. \n",
    "\n",
    "What happens when the data shifts is that the weights then have to compensate, because what they expected the data to look like before is not what it looks like now. So, if you input data has changed then all of the weights in front of it in the neural network will then have to change. The authors claim that this increases training time, because it requires lowering the learning rate and careful weight initialization. \n",
    "\n",
    "But, lets say we do batch normalization, and the data in each layer is normalized. Let's assume the ideal case where all of the data is precisely of mean 0 and variance 1. Well, now the weights won't have to adapt to different looking data, because all of the data looks the same. \n",
    "\n",
    "This has two effects that have been shown by the authors of the original paper. \n",
    "1. First, it allows us to increase the learning rate, leading to faster training. \n",
    "2. Secondly, it acts like a regularizer. Because the inputs no longer have to take on extreme values, neither will the weights. It has been demonstrated that sometimes by including batch normalization in the network, you can eliminate the need for dropout regularization, since BN is already doing something like regularization. \n",
    "\n",
    "One final note: Usually a neural network has a weight matrix and bias vector as parameters. However, we no longer need the bias because the batch normalization already shifts the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
