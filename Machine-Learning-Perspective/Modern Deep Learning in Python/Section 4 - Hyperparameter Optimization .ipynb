{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Optimization \n",
    "We have just introduced a lot of new hyperparameters, and this is only going to get worse as our neural networks get more and more complex. This leads us in to a topic called **hyperparameter optimization**. So, what are the hyperparameters that we have learned about so far? \n",
    "\n",
    ">* **Learning Rate** or if it is adaptive, **initial learning rate** and **Decay rate** <br>\n",
    "* **Momentum** <br>\n",
    "* **Regularization weight**<br>\n",
    "* **Hidden Layer Size**<br>\n",
    "* **Number of hidden layers**<br>\n",
    "\n",
    "So, what are some approaches to choosing these parameters? \n",
    "\n",
    "## 1.1 K-Fold Cross-Validation\n",
    "Let's go over **K-Fold Cross Validation** to review. The idea is simple, we do the following:\n",
    "> 1. Split data into K parts\n",
    "2. Do a loop that iterates through K times\n",
    "3. Each time we take out one part, and use that as the validation set, and the rest of the data as the training set\n",
    "\n",
    "So in the first iteration of the loop, we validate with the first part, and train on the rest. \n",
    "\n",
    "<img src=\"images/k-folds-validation.png\">\n",
    "\n",
    "Here is some pseudocode that can do this:\n",
    "```\n",
    "def crossValidation(model, X, Y, K=5):\n",
    "    X, Y = shuffle (X, Y)\n",
    "    sz = len(Y) / K\n",
    "    scores = []\n",
    "    for k in range(K):\n",
    "        xtr = np.concatenate([ X[:k*sz, :], X[(k*sz + sz):, :] ])\n",
    "        ytr = np.concatenate([ Y[:k*sz], X[(k*sz + sz):] ])\n",
    "        xte = X[k*sz:(k*sz + sz), :]\n",
    "        yte = Y[k*sz:(k*sz + sz)]\n",
    "        \n",
    "        model.fit(xtr, ytr)\n",
    "        score = model.score(xte, yte)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores), np.std(scores)\n",
    "```\n",
    "\n",
    "Now, we can see that this algorithm contains **K** different scores. We can simply use the mean of these of these scores as a measurement for how good this particular hyperparameter setting is. Another thing we could do is a statistical test to determine if the difference between two hyperparameter settings is statistically significantly better than the other.\n",
    "\n",
    "## 1.2 Sci-Kit Learn K-Folds\n",
    "Sci-Kit learn has its own K-folds implementation that is great to use:\n",
    "\n",
    "```\n",
    "from sklearn import cross_validation\n",
    "scores = cross_validation.cross_val_score(model, X, Y, cv=K)\n",
    "```\n",
    "\n",
    "Note that the SKLearn implementation does require you to conform to certain aspects of the SKLearn API. For example, you must provide a class with at least the 3 methods `fit()`, `predict()`, and `score()`. \n",
    "\n",
    "## 1.3 Leave-One-Out Cross-Validation\n",
    "One special variation of K-Folds cross-validation is where we set K = N. We will talk more about this later. But the basic idea is:\n",
    "> 1. We do a loop N times \n",
    "2. Every iteration of the loop we train on everything but one point \n",
    "3. We test on the one point that was left out\n",
    "4. We do this N times for all points \n",
    "\n",
    "Now with all of that discussed, what are some of the different approaches to hyperparameter optimization? \n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "## 1.4 Grid Search\n",
    "Grid search is an exhaustive search. This means that you can choose a set of learning rates that you want to try, choose a set of momentums you want to try, and choose a set of regularizations that you want to try, at which point you try every combination of them. In code that may look like: \n",
    "\n",
    "```\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "momentums = [1, 0.1, 0.01, 0.001]\n",
    "regularizations = [1, 0.1, 0.01]\n",
    "\n",
    "for lr in learning_rates: \n",
    "    for mu in momentums:\n",
    "        for reg in regularizations:\n",
    "            score = cross_validation(lr, mu, reg, data)\n",
    "```\n",
    "\n",
    "As you can imagine, this is **very** slow! But, since each model is independent of the others, there is a great opportunity here for a **parallelization**! Frameworks like **hadoop** and **spark** are ideal for this type of problem. \n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "## 1.5 Random Search\n",
    "On the other hand we have **random search**, which instead of looking at every possibility, just moves in random directions until the score is improved. A basic algorithm could look like: \n",
    "\n",
    "```\n",
    "theta = random position in hyperparameter space\n",
    "score1 = cross_validation(theta, data)\n",
    "for i in range(max_iterations):\n",
    "    next_theta = sample from hypersphere around theta\n",
    "    score2 = cross_validation(next_theta, data)\n",
    "    if score2 is better than score1:\n",
    "        theta = next_theta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "# 2. Sampling Logarithmically \n",
    "Let's now talk about how to sample random numbers when performing random search. It is not quite as straight forward as you may assume. \n",
    "\n",
    "## 2.1 Main Problem\n",
    "Suppose we want to randomly sample the learning rate. We know that the difference between 0.001 and 0.0011 is not that significant. In general, we want to try different numbers on a log scale, such as $10^{-2}$,$10^{-3}$,$10^{-4}$, etc. So if you sample between $10^{-7}$ and $10^{-1}$ uniformally, what is going to happen? \n",
    "\n",
    "Well if we look at the image below, we can see that most of what we would sample is on the same scale as $10^-1$, while everything else is under represented!\n",
    "\n",
    "<img src=\"images/sampling-scale.png\">\n",
    "\n",
    "So, how can we fix this problem? Well, we will sample on a **log scale**! That way we will get an even distribution between every 10th power, which is exactly what we want! Algorithmically this may look like:\n",
    "\n",
    "```\n",
    "Sample uniformly from (-7, -1) from a uniform distribution # Or whatever limits you want \n",
    "R ~ U(-7, -1)\n",
    "Set your learning rate to be 10^R\n",
    "```\n",
    "\n",
    "This can also be used for hyper parameters like decay where we want to try numbers like 0.9, 0.99, 0.999, etc! \n",
    "\n",
    "It may not seem intuitive that these numbers are still on a log scale, but if we rewrite those numbers as:\n",
    "#### $$0.9 = 1 - 10^{-1}$$\n",
    "#### $$0.99 = 1 - 10^{-2}$$\n",
    "#### $$0.999 = 1 - 10^{-3}$$\n",
    "\n",
    "We can see that they are indeed in fact still on a log scale! These will give very different results, so being able to sample them effectively is very important. Algorithmically it may look like:\n",
    "\n",
    "```\n",
    "R ~ U(lower, upper)\n",
    "Decay Rate = 1 - 10^R\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
