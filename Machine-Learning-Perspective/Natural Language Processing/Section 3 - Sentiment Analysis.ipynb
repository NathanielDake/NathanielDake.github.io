{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Overview\n",
    "* sentiment is a measure of how positive or negative something is\n",
    "* we are going to build a very simple sentiment analyzer \n",
    "* These are amazon reviews, so they come with 5 star ratings, and we are going to look at the electronics category \n",
    "* These are XML files, so we will need an XML parser \n",
    "\n",
    "# NLP Terminology\n",
    "* **Corpus**: Collection of text\n",
    "* **Tokens**: Words and punctuation that make up the corpus. \n",
    "* **Type**: a distinct token. Ex. \"Run, Lola Run\" has four tokens (comma counts as one) and 3 types.\n",
    "* **Vocabulary**: The set of all types. \n",
    "* The google corpus (collection of text) has 1 trillion tokens, and only 13 million types. English only has 1 million dictionary words, but the google corpus includes types such as \"www.facebook.com\". \n",
    "\n",
    "# Outline \n",
    "* we are just going to look at the electronics category \n",
    "* we could use the 5 star targets to do regression, but lets just do classification since they are already marked \"positive\" and \"negative\"\n",
    "* XML parser (going to use BeautifulSoup)\n",
    "* only look at **review_test**\n",
    "* To create our feature vector, we will count up the number of occurences of each word, and divided it by the total number of words  \n",
    "* For that to work we will need two passes through the data\n",
    "    1. One to collect the total number of distinct words, so that we know the size of our feature vector, in other words the vocabulary size, and possibly remove stop words like \"this\", \"is\", \"I\", \"to\", etc, to decrease the vocabulary size. The goal here is to know the index of each token\n",
    "    2. On the second pass, we will be able to assign values to each data vector which index corresponds to which words, and one to create data vectors \n",
    "* once we have that, it is simply a matter of creating a classifier like the one we did for our spam detector \n",
    "* so we will use logistic regression, so we can intepret the weights! \n",
    "* Ex: if you see a word like horrible and it has a weight of minus 1, it is associated with negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Sentiment Analysis in Python using Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()                                # this turns words into their base form \n",
    "\n",
    "stopwords = set(w.rstrip() for w in open('data/stopwords.txt'))         # grab stop words \n",
    "\n",
    "positive_reviews = BeautifulSoup(open('data/electronics/positive.review').read(), \"lxml\")   # get pos reviews\n",
    "positive_reviews = positive_reviews.findAll('review_text')                                  # only want rev text\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('data/electronics/negative.review').read(), \"lxml\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance \n",
    "There are more positive than negative reviews, so we are going to shuffle the positive reviews and then cut off any extra that we may have so that they are both the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer function\n",
    "Lets now create a tokenizer function that can be used on our specific reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)                        # essentially string.split()\n",
    "    tokens = [t for t in tokens if len(t) > 2]                     # get rid of short words\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]     # get words to base form\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index each word\n",
    "We now need to create an index for each of the words, so that each word has an index in the final data vector. However, to able able to do that we need to know the size of the final data vector, and to be able to know that we need to know how big the vocabulary is. Remember, the **vocabulary** is just the set of all types!\n",
    "\n",
    "We are essentially going to look at every individual review, tokenize them, and then add those tokens 1 by 1 to the map if they do not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_map = {}                            # our vocabulary - dictionary that will map words to dictionaries\n",
    "current_index = 0                              # counter increases whenever we see a new word\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "# --------- loop through positive reviews ---------\n",
    "for review in positive_reviews:              \n",
    "    tokens = my_tokenizer(review.text)          # converts single review into array of tokens (split function)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:                        # loops through array of tokens for specific review\n",
    "        if token not in word_index_map:                        # if the token is not in the map, add it\n",
    "            word_index_map[token] = current_index          \n",
    "            current_index += 1                                 # increment current index\n",
    "                \n",
    "# --------- loop through negative reviews ---------\n",
    "for review in negative_reviews:              \n",
    "    tokens = my_tokenizer(review.text)          \n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:                       \n",
    "        if token not in word_index_map:                        \n",
    "            word_index_map[token] = current_index          \n",
    "            current_index += 1                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens into vector\n",
    "Now that we have our tokens and vocabulary, we need to convert our tokens into a vector. Because we are going to shuffle our train and test sets again, we are going to want to put labels and vector into same array for now since it makes it easier to shuffle. \n",
    "\n",
    "Note, this function operates on **one** review. So the +1 is creating our label, and this function is basically designed to take our input vector from an english form the a numeric vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    xy_data = np.zeros(len(word_index_map) + 1)          # equal to the vocab size + 1 for the label \n",
    "    for t in tokens:                                     # loop through every token\n",
    "        i = word_index_map[t]                            # get index from word index map\n",
    "        xy_data[i] += 1                                  # increment data at that index \n",
    "    xy_data = xy_data / xy_data.sum()                    # divide entire array by total, so they add to 1\n",
    "    xy_data[-1] = label                                  # set last element to label\n",
    "    return xy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to actually assign these tokens to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)               # total number of examples \n",
    "data = np.zeros((N, len(word_index_map) + 1))                       # N examples x vocab size + 1 for label\n",
    "i = 0                                                               # counter to keep track of sample\n",
    "\n",
    "for tokens in positive_tokenized:                                   # loop through postive tokenized reviews\n",
    "    xy = tokens_to_vector(tokens, 1)                                # passing in 1 because these are pos reviews\n",
    "    data[i,:] = xy                                                  # set data row to that of the input vector\n",
    "    i += 1                                                          # increment 1\n",
    "    \n",
    "for tokens in negative_tokenized:                                   \n",
    "    xy = tokens_to_vector(tokens, 0)                                \n",
    "    data[i,:] = xy                                                 \n",
    "    i += 1         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now 1000 rows of positively labeled reviews, followed by 1000 rows of negatively labeled reviews. Lets shuffle before getting our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Rate:  0.71\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification Rate: \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Rate\n",
    "We end up with a classification rate of 0.71, which is not so great, but it is better than random guessing. \n",
    "\n",
    "## Sentiment Analysis\n",
    "Something interesting that we can do is look at the weights of each word, to see if that word has positive or negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast 0.8415017020207988\n",
      "ha 0.8107250821654655\n",
      "you 1.1024048266623723\n",
      "time -0.7037068930179133\n",
      "little 0.9502620648440729\n",
      "'ve 0.7697421487982679\n",
      "sound 1.1335599776630716\n",
      "perfect 0.9800314164091442\n",
      "buy -0.9637652000132795\n",
      "quality 1.641902470563383\n",
      "easy 1.7455274934676788\n",
      "excellent 1.3188366996773964\n",
      "n't -2.1707529616105385\n",
      "money -1.09340826795223\n",
      "speaker 0.8644718069016352\n",
      "support -0.87503788272428\n",
      "item -1.010392795041761\n",
      "wa -1.6771987566294633\n",
      "price 2.833939363164663\n",
      "pretty 0.7857664954597734\n",
      "doe -1.2704316440170145\n",
      "highly 0.9777645969369407\n",
      "lot 0.7377844910861758\n",
      "waste -1.0187635956181265\n",
      "tried -0.8279985386367086\n",
      "then -1.0700240114561872\n",
      "memory 1.0238808992596147\n",
      "love 1.1682256082058238\n",
      "bad -0.7725815921230524\n",
      "poor -0.7958563591650633\n",
      "returned -0.7892276667851883\n",
      "return -1.2079105967144368\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7 \n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
