{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bootstrap Estimation\n",
    "We previously looked at the bias-variance tradeoff and if you were thinking critically you may have wondered: \"Could it be possible in some way to lower bias and variance simultaneously?\"\n",
    "\n",
    "In this section, we are going to take our first look into **model averaging**. The key tool that we need to do this is called **bootstrapping**, aka **resampling**. The fascinating result of this is that even though we are using the same data, we can get a better result. This should seem odd at first, since if we create a model from a set of samples, how can that be any different than taking the averages of different models trained on different subsets of those same samples again and again-it is the same set of samples after all. \n",
    "\n",
    "However, model averaging does work, even if it is true that they work on the same data that you would have if you only have 1 model. Before we talk about bootstraping for models, we are going to look at bootstrapping for simple parameter estimates like the mean. \n",
    "\n",
    "## 1.1 Bootstrap Estimation - Mean\n",
    "So, how does bootstrap estimation work? We are given a set of data points from $1...N$\n",
    "#### $$X = x_1,x_2,...,x_N$$\n",
    "We then draw a sample, with replacement, from this data set, $B$ times. For each of the $b$ subsample datasets, we calculate the parameter of interest-aka the mean, variance, or any other statistic. Once the loop is done we will have $B$ different estimates of the parameter. We can use this to find the mean of the parameter, and the variance of the parameter. Why do we care about the mean and variance? First, the mean tells us the most likely value of the parameter, in other words the expected value of the parameter. The variance then can tell us how accurate that estimate is! A **large variance** means not that accurate, and a **small variance** means more accurate. So, in pseudo code the algorithm could look like this:\n",
    "\n",
    "```\n",
    "X = x1, x2,...xN\n",
    "for b = 1..B:\n",
    "    Xb = sample_with_replacement(X)         # size of Xb is N\n",
    "    sample_mean[b] = sum(Xb)/N\n",
    "Calculate mean and variance of {sample_mean[1],...,sample_mean[B]}\n",
    "```\n",
    "\n",
    "As an example, lets just say that $X$ has a size N = 5 with the following values: \n",
    "#### $$X = 1,2,3,4,5$$\n",
    "\n",
    "Let's say that we decide to make $B = 4$, in order words we are going to have 4 iterations of sampling. The first time we sample (with replacement) we may end up with:\n",
    "\n",
    "#### $$X_{b1} = 1,2,5,5,2$$\n",
    "We have sampled five values from our original $X$. If we perform that 3 more times, we may end up with:\n",
    "#### $$X_{b2} = 4,3,3,1,5$$\n",
    "#### $$X_{b3} = 2,4,1,5,4$$\n",
    "#### $$X_{b4} = 3,1,3,2,4$$\n",
    "\n",
    "Now, let's say that our original goal was to be finding a certain parameter of $X$, in this case the mean. We can then calculate the mean of each of above samples:\n",
    "#### $$\\mu_{B1} = \\frac{15}{5} = 3$$\n",
    "#### $$\\mu_{B2} = \\frac{16}{5} = 3.2$$\n",
    "#### $$\\mu_{B3} = \\frac{16}{5} = 3.2$$\n",
    "#### $$\\mu_{B4} = \\frac{13}{5} = 2.6$$\n",
    "\n",
    "We now have $B$ different estimates of the mean of our samples, which were taken from $X$. What we may want to do is now try and find the **mean** of these means. In other words, we can try and find the parameters that describe this set of sampled data:\n",
    "\n",
    "#### $$mean(\\mu_{B1},\\mu_{B2},\\mu_{B3},\\mu_{B4}) = \\frac{\\mu_{B1} + \\mu_{B2}+\\mu_{B3}+\\mu_{B4}}{B}$$\n",
    "\n",
    "And the variance which can tell us how accurate our estimate is:\n",
    "#### $$var(\\mu_{B1},\\mu_{B2},\\mu_{B3},\\mu_{B4})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<br>\n",
    "## 1.2 Sampling with Replacement\n",
    "In case you have not come across sampling with replacement, let's quickly touch on it now. Suppose we have a dataset with the points 1,2,3,4,5.\n",
    "\n",
    "#### $$X = 1,2,3,4,5$$\n",
    "\n",
    "Suppose we then draw a sample and get 5. Sampling with replacements means that if we draw another sample, we can get 5 again. In fact, we could draw a sample with all 5s! \n",
    "\n",
    "#### $$sample = 5,5,5,5,5$$\n",
    "\n",
    "This is because we replace the sample after we take it from the dataset. This is the opposite of sampling without replacement. If we were to sample without replacement and we drew a number of samples equal to the dataset size, we would just draw the dataset itself. Hence, sampling with replacement is important to this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "## 1.3 Why does bootstrapping work?\n",
    "As you can see, bootstrapping is a very simple algorithm- you are just computing the parameter estimate multiple times from the same dataset. So, why does it work? Lets look at the results first and then we can derive them. Remember, we are interested in the mean and variance. \n",
    "\n",
    "<br>\n",
    "**Mean**<br>\n",
    "The mean of the bootstrap estimate is equal to the parameter itself:\n",
    "\n",
    "#### $$E(\\bar{\\theta_B}) = mean(\\bar{\\theta_B}) = \\theta$$\n",
    "\n",
    "And, as an example, if our parameter had been the mean, then what we find is that mean of our bootstrap estimate of $\\mu$, is equal to the actual value of $\\mu$. In other words, the mean of our bootstrap sampled means, is the actual mean of the original data.\n",
    "\n",
    "#### $$E(\\bar{\\mu_B}) = \\mu$$\n",
    "\n",
    "Or in the case of our example earlier:\n",
    "#### $$E(\\frac{\\mu_{B1}+\\mu_{B2}+\\mu_{B3}+\\mu_{B4}}{B}) = \\mu_X$$\n",
    " \n",
    "**Variance**<br>\n",
    "The variance is a bit more complicated. Let's suppose the correlation coefficient between two different estimates of the parameter, $\\hat{\\theta}_i, \\hat{\\theta}_j$ is $\\rho$, and the variance of each $\\hat{\\theta}$ is $\\sigma^2$:\n",
    "\n",
    "#### $$\\rho = corr(\\hat{\\theta}_i, \\hat{\\theta}_j), var(\\hat{\\theta}) = \\sigma^2$$\n",
    "\n",
    "Then, it can be derived that the variance of the bootstrap estimate is:\n",
    "\n",
    "#### $$var(\\bar{\\theta}_B) = \\frac{1 - \\rho}{B}\\sigma^2 + \\rho \\sigma^2$$\n",
    "\n",
    "Notice that if each bootstrap estimate is completely uncorrelated from the others, the variance would be the original variance divided by $B$. This means that for every bootstrap sample we take, we reduce the variance of our estimate. That is remarkable! Unfortunately, there will probably be correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "## 1.4 Confidence Interval\n",
    "One application of bootstrap estimation, is that we can also estimate the confidence interval of our estimate. We assume a gaussian approximation, so let's say we want a 95% confidence interval. That means that we want the lower and upper bound of $\\theta$ that covers 95% of the area under the probability distribution. This is approximately equal to the sample mean of the bootstrap $\\theta$, plus or minus 1.96 times the standard deviation of the bootstrap $\\theta$:\n",
    "\n",
    "#### $$95\\% CI \\approx \\bar{\\theta}_B \\;\\pm\\; 1.96 std(\\hat{\\theta}_B)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "## 1.5 Derivation of Mean and Variance \n",
    "Now that we know the main results of bootstrap estimation, how do we show that they are true?\n",
    "\n",
    "<br>\n",
    "### 1.5.1 Mean Derivation\n",
    "<br>\n",
    "Let's start with the mean. We want to be able to show that the mean value of our bootstrap estimated parameter is the value of the parameter itself. In other words, think back to our simple example at the start of lecture. Our data set was $X$, and the parameter we were looking at was $\\mu$, the mean of $X$. We want to be able to prove that after performing our bootstrap sampling, and that expected value of the mean of our samples (think $\\mu_{B1}$, $\\mu_{b2}$, and so on) are equal to actual mean of $X$, since this was the parameter we were originally trying to estimate! \n",
    "\n",
    "So, we know that expected value (based on its definiton: *expected value of a random variable, intuitively, is the long-run average value of repetitions of the experiment it represents*), is equivalent to the mean. Let's define the following: \n",
    "> * $\\bar{\\theta}_B$ = sample mean of resampled sample means\n",
    "* $\\hat{\\theta}_i$ = sample mean of bootstrap sample $i$\n",
    "* $\\theta$ = original parameter we're trying to estimate\n",
    "\n",
    "Okay, now we can start with looking at the expected value of our resampled sample means:\n",
    "\n",
    "#### $$E(\\bar{\\theta}_B)$$\n",
    "\n",
    "We can expand $\\bar{\\theta}_B$ based on its definition:\n",
    "\n",
    "#### $$E(\\bar{\\theta}_B) = E \\Big[ \\frac{1}{B} \\sum_{i=1}^B \\hat{\\theta}_i \\Big] = E\\Big[\\frac{1}{B}(\\hat{\\theta}_1 + ...+\\hat{\\theta}_B)\\Big]$$\n",
    "\n",
    "Because $\\frac{1}{B}$ is a constant, and the expected value of constant is just itself, we can pull it out:\n",
    "\n",
    "#### $$E(\\bar{\\theta}_B) = \\frac{1}{B}*E\\Big[(\\hat{\\theta}_1 + ...+\\hat{\\theta}_B)\\Big]$$\n",
    "\n",
    "And then we know the expected value of any $\\hat{\\theta}_i$ is going to be $\\theta$, the actual parameter. We also know that there are $B$ total $\\hat{\\theta}$s, so we can pull that out and end up with the final equation:\n",
    "\n",
    "#### $$E(\\bar{\\theta}_B) =  \\frac{1}{B}BE(\\hat{\\theta}) = \\theta$$\n",
    "\n",
    "\n",
    "\n",
    "We can see that the expected value of the bootstrap estimate of the parameter, is equal to the parameter, which is exactly what we were looking for. \n",
    "\n",
    "<br>\n",
    "### 1.5.2 Variance Derivation\n",
    "<br>\n",
    "<br>\n",
    "### 1.5.2.1 Variance Derivation - Definitions\n",
    "Next, let's look at the variance. We can start with some definitions. Let's suppose that the expected value of $\\hat{\\theta}$ (aka the expected value of the parameter that we calculate after resampling) is equal to $\\mu$. \n",
    "\n",
    "#### $$E(\\hat{\\theta}) = \\mu$$\n",
    "\n",
    "This is not necessarily equal to the original mean of data $X$. It is the mean of whatever parameter we are trying to estimate. For instance, say that the parameter we are trying to estimate is the mean (as in our simple example from earlier). We are stating that the expected value of any of the means we have sampled ($\\mu_{B1},\\mu_{B2}$, etc) is just equal to $\\mu$. So in this case $\\mu_{B1}$ and so on would be represented as $\\hat{\\theta}$ (the parameter we are trying to find, and $\\mu$ is the mean/expected value of that parameter. \n",
    "\n",
    "Let's also define the variance of $\\hat{\\theta}$ to be $\\sigma^2$:\n",
    "\n",
    "#### $$var(\\hat{\\theta}) = E \\Big[(\\hat{\\theta} - \\mu)^2\\Big] = \\sigma^2$$\n",
    "\n",
    "We can next define the correlation between two different $\\hat{\\theta}$s to be $\\rho$:\n",
    "#### $$\\rho = \\frac{E \\Big[(\\hat{\\theta}_i - \\mu)(\\hat{\\theta}_j - \\mu) \\Big]}{\\sigma^2}$$\n",
    "\n",
    "Note that correlation is scaled by standard deviation, so it always $[-1, 1]$. We then define the sum of all $\\hat{\\theta}$ to be $S_B$:\n",
    "#### $$S_B = \\sum_{i=1}^B \\hat{\\theta}_i$$\n",
    "\n",
    "And therefore the sample mean of the bootstrap estimates is:\n",
    "\n",
    "#### $$\\bar{\\theta}_B = \\frac{1}{B}S_B$$\n",
    "\n",
    "<br>\n",
    "### 1.5.2.2 Variance Derivation - Write out definition\n",
    "Let's start by writing out the definiton for variance of $\\bar{\\theta}_B$:\n",
    "\n",
    "#### $$var(\\bar{\\theta}_B) = E \\Big[(\\bar{\\theta}_B - \\mu)^2\\Big]= E \\Big[(\\frac{1}{B}S_B - \\mu)^2\\Big]$$\n",
    "\n",
    "Then we can perform several algebraic operations to the right side of the equation:\n",
    "#### $$E \\Big[(\\frac{1}{B}S_B - \\frac{1}{B} B\\mu)^2\\Big]$$\n",
    "#### $$E \\Big[\\Big((\\frac{1}{B})(S_B - B\\mu)\\Big)^2\\Big]$$\n",
    "#### $$E \\Big[\\frac{1}{B^2}(S_B - \\mu)^2\\Big]$$\n",
    "#### $$\\frac{1}{B^2}E \\Big[(S_B - \\mu)^2\\Big]$$\n",
    "#### $$\\frac{1}{B^2}E \\Big[S_B^2 - 2\\mu BS_B + \\mu^2 B^2\\Big]$$\n",
    "\n",
    "Now, if we look specifically at the term $- 2\\mu BS_B$, we can see that 2, $\\mu$, and $B$ are constant, so we can pull them outside of the expected value:\n",
    "\n",
    "#### $$E\\Big[-2 \\mu B S_B\\Big] = -2 \\mu B *E\\Big[S_B\\Big] $$\n",
    "\n",
    "We know that the expected value of $S_B$, based on its definition, can be rewritten as:\n",
    "\n",
    "#### $$-2 \\mu B *E\\Big[S_B\\Big] = -2 \\mu B *E\\Big[B\\hat{\\theta}\\Big] $$\n",
    "\n",
    "And since $B$ is constant, it can be pulled outside of the expected value, and we have defined the expected value of $\\hat{\\theta}$ to be $\\mu$:\n",
    "\n",
    "#### $$-2 \\mu B^2 *E\\Big[\\hat{\\theta}\\Big] = -2 \\mu^2 B^2 $$\n",
    "\n",
    "Now, back to the equation we branched off from: \n",
    "\n",
    "#### $$\\frac{1}{B^2}E \\Big[S_B^2 - 2\\mu BS_B + \\mu^2 B^2\\Big]$$\n",
    "\n",
    "We can replace the middle term with that which we just found above, and end up with: \n",
    "\n",
    "#### $$var(\\bar{\\theta}_B) = \\frac{1}{B^2}E \\Big[S_B^2 - \\mu^2 B^2\\Big]$$\n",
    "\n",
    "At this point, we can note that $\\mu$ and $B$ are **both constant**, so they can be pulled out of the expected value: \n",
    "\n",
    "#### $$var(\\bar{\\theta}_B) = \\frac{1}{B^2}\\Big(E \\Big[S_B^2\\Big]  - \\mu^2 B^2\\Big)$$\n",
    "\n",
    "Which if we then multiply the fraction through, we end up with: \n",
    "\n",
    "#### $$var(\\bar{\\theta}_B) = \\frac{1}{B^2}E \\Big[S_B^2\\Big]  - \\mu^2$$\n",
    "\n",
    "Our main focus now is to find $E\\Big[S_B^2\\Big]$.\n",
    "<br>\n",
    "### 1.5.2.3 Variance Derivation - Find $E\\Big[S_B^2\\Big]$\n",
    "We can start by using the definition of $S_B$, which is just the sum of the individual sample $\\hat{\\theta}$s:\n",
    "\n",
    "#### $$E\\Big[S_B^2\\Big] = E\\Big[(\\hat{\\theta}_1+\\hat{\\theta}_2+...+\\hat{\\theta}_B)(\\hat{\\theta}_1+\\hat{\\theta}_2+...+\\hat{\\theta}_B)\\Big]$$\n",
    "\n",
    "The important point to notice here is that we will end up with two types of terms here when we multiply this out. There will be the type where it is $\\hat{\\theta}_i*\\hat{\\theta}_i$ (aka the subscript is the same for both $\\hat{\\theta}$s in the expected value, it will look like $(\\hat{\\theta}_1*\\hat{\\theta}_1 +\\hat{\\theta}_2*\\hat{\\theta}_2)$ and so on). There will be $B$ of these terms. \n",
    "\n",
    "The other type of term that we will get is $\\hat{\\theta}_i*\\hat{\\theta}_j$, where $i \\neq j$. Since there will be $B^2$ terms in total, and there will be $B$ where $i = j$, then there will be $B(B-1)$ terms where $i \\neq j$. Both of these terms will be non zero.\n",
    "\n",
    "#### $$E\\Big[S_B^2\\Big] = BE\\Big[\\hat{\\theta}_i^2\\Big] + B(B-1)E_{i \\neq j}\\Big[\\hat{\\theta}_i\\hat{\\theta}_j\\Big]$$\n",
    "\n",
    "We can find these two expected values using our previous definitions! \n",
    "\n",
    "### 1.5.2.3 Variance Derivation - Rearange equation for Variance and Correlation\n",
    "If we rearange our equation for the variance and correlation of $\\hat{\\theta}$, we can find these two expected values. We can start with the variance and begin looking for $E\\Big[\\hat{\\theta}_i^2\\Big]$:\n",
    "\n",
    "#### $$var(\\hat{\\theta}_i) = \\sigma^2 = E\\Big[ (\\hat{\\theta}_i - \\mu)^2 \\Big]$$ \n",
    "\n",
    "And we can expand out the inside: \n",
    "\n",
    "#### $$E\\Big[ (\\hat{\\theta}_i - \\mu)^2 \\Big] = E\\Big[ (\\hat{\\theta}_i^2 - 2\\mu \\hat{\\theta}_i+ \\mu^2) \\Big]$$ \n",
    "\n",
    "Take the expected value of each term:\n",
    "\n",
    "#### $$E\\Big[\\hat{\\theta}_i^2\\Big] - E\\Big[ 2\\mu \\hat{\\theta}_i \\Big] + \\mu^2 $$ \n",
    "\n",
    "Focusing on the middle term (as we did earlier), we can see that the 2 and $\\mu$ just have an expected value of themselves, so they can be pulled out:\n",
    "\n",
    "#### $$E\\Big[\\hat{\\theta}_i^2\\Big] -  2\\mu E\\Big[\\hat{\\theta}_i \\Big] + \\mu^2 $$ \n",
    "\n",
    "And the expected value of $\\hat{\\theta}_i$ is just $\\mu$ (by definition!). So we end up with: \n",
    "\n",
    "#### $$\\sigma^2 = E\\Big[\\hat{\\theta}_i^2\\Big] - \\mu^2 $$ \n",
    "#### $$E\\Big[\\hat{\\theta}_i^2\\Big] = \\sigma^2 + \\mu^2 $$ \n",
    "\n",
    "Great! Now we can look a $\\rho$ and being solving for $E_{i \\neq j}\\Big[\\hat{\\theta}_i\\hat{\\theta}_j\\Big]$. Let's start with the definition of $\\rho$:\n",
    "\n",
    "#### $$\\rho = \\frac{E \\Big[(\\hat{\\theta}_i - \\mu)(\\hat{\\theta}_j - \\mu) \\Big]}{\\sigma^2}$$\n",
    "\n",
    "We can expand the top:\n",
    "\n",
    "#### $$\\rho = \\frac{E \\Big[(\\hat{\\theta}_i\\hat{\\theta}_j - \\mu \\hat{\\theta}_i - \\mu \\hat{\\theta}_j  +\\mu^2) \\Big]}{\\sigma^2}$$\n",
    "\n",
    "And again if we look at the middle terms, we can simplify them by taking out the $\\mu$, and knowing the the expected value of $\\hat{\\theta}$ is just $\\mu$. This allows us to simplify our equation to:\n",
    "\n",
    "#### $$\\rho = \\frac{E \\Big[\\hat{\\theta}_i\\hat{\\theta}_j\\Big] - \\mu^2}{\\sigma^2}$$\n",
    "\n",
    "And we can rearange that to find: \n",
    "\n",
    "#### $$E \\Big[\\hat{\\theta}_i\\hat{\\theta}_j\\Big] = \\rho\\sigma^2 + \\mu^2$$\n",
    "\n",
    "### 1.5.2.4 Variance Derivation - Plug in values to find $E\\Big[S_B^2\\Big]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
