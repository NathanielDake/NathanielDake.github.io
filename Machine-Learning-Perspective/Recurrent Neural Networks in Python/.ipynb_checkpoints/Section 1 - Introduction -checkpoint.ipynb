{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "As with the notebooks related to Hidden Markov Models, _**Recurrent Neural Networks**_ are all about learning sequences. But, where Markov Models are limited by the Markov Assumption, Recurrent Neural Networks are _not_. As a result, they are more expressive and powerful than anything we have seen and haven't made progress on in decades. \n",
    "\n",
    "## 1.1 Outline\n",
    "So, what will these notebooks contain, and how will they build on the previous notebooks surrounding Neural Networks and Hidden Markov Models? \n",
    "\n",
    "> * In the first section, we are going to add _time_ to our neural networks. This will introduce us to the _Simple Recurrent Unit_, also known as the _Elman Unit_.\n",
    "* We will then revisit the XOR problem, but will extend it so that it becomes the _parity_ problem. We will demonstrate that regular feed forward neural networks will have trouble solving this problem, but recurrent networks will work because the key is to treat the input as a sequence. \n",
    "* Next, we will revisit one of the most popular applications of RNN's, _Language Modeling_. In the Markov Models notebooks we have generated poetry, and discriminate from two different poets just from the sequence of parts of speech tags that they used. We will extend our language model so that it _no longer_ makes the markov assumption. \n",
    "* Another popular application for RNN's is word vectors or word embeddings. The most common technique for this is called _word-2-vec_, but we will go over how RNN's can also be used for creating word vectors. \n",
    "* We will then look at the very popular _LSTM_, _**Long-Short term memory unit**_, and the more modern and efficient _GRU_, _**Gated Recurrent Unit**_, which has been proven to yield comparable performance. \n",
    "* Finally we will apply these to more practical problems, such as learning a language model from wikipedia, and visualizing the word embeddings as a result.\n",
    "\n",
    "## 1.2 Tips \n",
    "I will offer a tip that helped me in understanding RNN's: Understand the mechanics first, and worry about the \"meaning\" later. When we talk about LSTM's, we are going to talk about the ability to remember and forget things. Keep in mind, these are just convenient names that are utilized by way of analogy. We are not actually building something that is remembering or forgetting. They are just mathematical formulas. So, worry about the math and let the meaning come naturally to you. \n",
    "\n",
    "What you most definitely do _not_ want to do is the opposite; try to understand the meaning without understanding the mechanics. When you do that, the result is usually a sensationalist media article, or a pop science book. This set of notebooks is the opposite of that; we want to understand on a technical level what is happening. Explaining things in layman terms, of thinking of real life analogies is icing on the cake, only if you understand the technicalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Review of Import Deep Learning Concepts\n",
    "\n",
    "## 2.1 Softmax Function\n",
    "Let's begin by talking about the Softmax function. The softmax function is what we use to classify _two or more_ classes. It is a bit more complicated to work with than the sigmoid-in particular it's derivative is harder to derive-but, we will let theano and tensorflow take care of that stuff for us. Remeber, all we do here is take an array of numbers, exponentiate them, divide by the sum, and that allows us to interpret the output of the softmax as a probability:\n",
    "\n",
    "\n",
    "$$y_k=\\frac{e^{a_k}}{\\sum_je^{a_j}}$$\n",
    "\n",
    "$$p\\big(y=k \\mid x \\big) =\\frac{e^{W_k^T x}}{\\sum_je^{W_j^Tx}}$$\n",
    "\n",
    "Where k represents the class $k$ in the output layer. In other words our $y$ output is going to be a  **(kx1)** vector for a single training example, and an **(Nxk)** matrix when computed for the entire training set. \n",
    "\n",
    "In code, it will look like:\n",
    "\n",
    "```\n",
    "def softmax(a):\n",
    "    expA = np.exp(a)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "## 2.2 Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
