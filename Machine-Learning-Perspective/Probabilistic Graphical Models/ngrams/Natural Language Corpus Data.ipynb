{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overview \n",
    "This post was inspired and based off of chapter fourteen of the book _Beautiful Data_, the chapter being written by Peter Norvig. The chapter can be found <a href=\"http://norvig.com/ngrams/ch14.pdf\">here</a>. The exercise will examining data consisting of the plainest of speech: 1 trillion words of English, taken from publically available webpages. \n",
    "\n",
    "This data set was published by Thorsten Brants and Alex Franz of Google in 2006, and is made publically available through the Linguistic Data Consortium <a href=\"https://catalog.ldc.upenn.edu/LDC2006T13\">here</a>.\n",
    "\n",
    "This data set summarizes the original texts by _counting_ the number of appearances of each words, and of each two-, three-, four-, and five-word sequence. For example, the word \"the\" appears 23 billion times (2.2% of the trillion words), making it the most common word. \n",
    "\n",
    "---\n",
    "\n",
    "# Technical Definitions\n",
    "Before we can dig into an analysis, we must learn a few pieces of technical terminology that will serve us well in the long run. \n",
    "\n",
    "> **Corpus**: A collection of text is called a _corpus_.\n",
    "\n",
    "> **Tokens**: We treat the corpus as a sequence of _tokens_-meaning words and punctuation. \n",
    "\n",
    "> **Types**: Each distinct token is called a _type_, so the text \"Run, Lola Run\" has 4 tokens (the comman counts as one) but only three types. \n",
    "\n",
    "> **Vocabulary**: The set of all types is called the _vocabulary_. \n",
    "\n",
    "> **Unigram**: A 1-token sequence is a _unigram_. \n",
    "\n",
    "> **Bigram**: A 2-token sequence is a _bigram_.\n",
    "\n",
    "> **n-gram**: An n-token sequence is an _n-gram_.\n",
    "\n",
    "> **Probability**: We will refer to _P_ as probability, as in P(_the_) = 0.022, which means that the probability of the toekn \"the\" is 0.022, or 2.2%. If _W_ is a sequence of tokens, then _W3_ is the third token, and _W1:3_ is the sequence of the first through third tokens. $P(Wi = the|Wi-1=of)$ is the _conditional probability_ of \"the\", given that \"of\" is the previous token. \n",
    "\n",
    "We are now ready to look at some tasks that can be accomplished using the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Word Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
