{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminaries\n",
    "So far we have defined graphical models primarily as a data structure for encoding probability distributions. So, we have talked about how we can take a probability distribution and using a set of parameters that are some how tied to the graph structure, one can go ahead and represent a probability distribution over a high dimensional space in a factored form. \n",
    "\n",
    "It turns out that we can view the graph structure in a graphical model using a  completely complimentary viewpoint. Which is, as a representation of the set  of independencies, that the probability distribution must satisfy. \n",
    "\n",
    "## 1.1 Independence \n",
    "We start by talking about the independence of $\\alpha$ and $\\beta$ within a probability distribution. So:\n",
    "\n",
    "> For events $\\alpha$, $\\beta$, P $\\models \\alpha \\perp \\beta$ if:\n",
    "\n",
    "Note: $\\models$ is the logical symbol for satisfies, and $\\perp$ is the symbol for independence. So we can read the statement above as *P satisfies that $\\alpha$ is independent of  $\\beta$.*\n",
    "\n",
    "There are 3 entirely equivalent definitions of the concept of independence.So, if we go back to our definition:\n",
    "> For events $\\alpha$, $\\beta$, P $\\models \\alpha \\perp \\beta$ if:\n",
    "* Definition 1: The probability of the conjunction (can be shown using intersection, $\\cap$, or a comma) of the two events is:\n",
    "#### $$P(\\alpha, \\beta) = P(\\alpha \\cap \\beta) = P(\\alpha)P(\\beta)$$\n",
    "* Definition 2: This definition concerns flow of influence. It says, if you tell me $\\beta$, it doesn't effect my probability of $\\alpha$. So, the probability of $\\alpha$, given the information about $\\beta$ is the same as the probability of $\\alpha$ if you don't give that information:\n",
    "#### $$P(\\alpha|\\beta) = P(\\alpha)$$\n",
    "Becasue probabilistic influence is symmetrical, we have the exact converse of that:\n",
    "#### $$P(\\beta|\\alpha) = P(\\beta)$$\n",
    "\n",
    "So this is independence of events, and we can take that exact same definition and generalize it to **independence of random variables**. \n",
    "\n",
    "> For random variables $X, Y, P \\models X \\perp Y$ if:\n",
    "* $P(X, Y) = P(X)P(Y)$\n",
    "* $P(X|Y) = P(X)$\n",
    "* $P(Y|X) = P(Y)$\n",
    "\n",
    "We can read these statements in two different but equivalent forms. The first is as a **universal statement**. So, for all $x, y$:\n",
    "\n",
    "#### $$\\forall x, y: P(x,y) = P(x)P(y)$$\n",
    "\n",
    "This means we can think of it as a conjunction of many independence statements involving x and y, which are in X and Y, of the form above. \n",
    "\n",
    "The second interpretation is an expression over **factors**. That is, we are told that the factor $P(X, Y)$ (which is the joint distribution of X and Y) is actually the product of two lower dimensional factor, one which is a factor whose scope is X, and one is a factor whose scope is Y. \n",
    "\n",
    "### 1.1.1 Example of Independence \n",
    "We can now look at an example of independence by looking at a fragment of our student network. It has 3 random variables: intelligence, difficulty, and course grade. And this is a probability distribution which has a scope over 3 variables. \n",
    "\n",
    "<img src=\"images/independence.png\" height=\"300\" width=\"300\">\n",
    "\n",
    "We can go ahead and marginalize that and get a probability distribution over the scope, which is a factor over the scope I, D. And it happens that this is the marginal distribution (recall, in order to get $i^0$ and $d^0$, we add up the first 3 rows). \n",
    "\n",
    "<img src=\"images/independence-1.png\" height=\"300\" width=\"300\">\n",
    "\n",
    "It is not difficult to test that if we go ahead and marginalize $P(I, D)$ to get $P(I)$ and $P(D)$, that:\n",
    "\n",
    "#### $$P(I,D) = P(I)P(D)$$\n",
    "\n",
    "<img src=\"images/independence-2.png\" height=\"300\" width=\"300\">\n",
    "\n",
    "When we look at the graphical model we can see that there are no direct connections between $I$ and $D$.\n",
    "\n",
    "<img src=\"images/independence-3.png\" height=\"300\" width=\"300\">\n",
    "\n",
    "## 1.2 Conditional Independence\n",
    "Now, by itself, independence is not a particularly powerful notion. This is because it only happens rarely. That is, only in very rare cases will you have random variables that are truly independent of each other. \n",
    "\n",
    "We will now talk about **conditional independence**, which is written as:\n",
    "\n",
    "> For (sets of) random variables $X, Y, Z: P \\models (X \\perp Y | Z)$\n",
    "\n",
    "This can be read as: *P satisfies X is independent of Y given Z*. We have 4 equivalent definitions of this property.\n",
    "\n",
    "> $P \\models (X \\perp Y | Z)$ if:\n",
    "* $P(X, Y|Z) = P(X|Z)P(Y|Z)$ (\n",
    "* $P(X|Y,Z) = P(X|Z)$ (given Z, Y gives us no information that changes our probability in X)\n",
    "* $P(X|X,Z) = P(Y|Z)$ (given Z, X gives us no information that changes our probability in Y)\n",
    "* $P(X, Y, Z) \\propto \\phi_1(X, Z) \\phi_2(Y,Z)$ (the joint distribution of X, Y, Z is proportional to a product of two factors; one factor over X and Z, and one factor over Y and Z. \n",
    "\n",
    "### 1.2.1 Conditional Independence Example\n",
    "Let's look at an example of conditional independence. Imagine that you are given two coins, and we are told that one coin is fair and one coin is biased and will come up heads 90% of the time. \n",
    "\n",
    "<img src=\"images/cond-ind-1.png\" height=\"300\" width=\"300\">\n",
    "\n",
    "Now, we have a process where we first pick a coin and then toss it twice. So in the image above, the coin is the coin that you pick, and $X_1$ and $X_2$ are the two tosses. Let's think about dependence and independence in this example. If you don't know which coin you picked, and you toss the coin and it comes out heads, what happens to the probability of heads in the second toss? It is higher! That is because there is a greater chance that we picked the biased coin. However, if we are told that we picked the fair coin, we don't care about the first toss. We know it will not effect the outcome of the second toss. \n",
    "\n",
    "So we have:\n",
    "\n",
    "#### $$P \\not\\models X_1 \\perp X_2$$\n",
    "#### $$P \\models (X_1 \\perp X_2 | C)$$\n",
    "\n",
    "### 1.2.2 Conditional Independence Example 2\n",
    "<img src=\"images/cond-ind-2.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "We will now go over another example that we have seen before. In this case there is still one common cause, the students intelligence. We then have two things that eminate from that: the students grade and their SAT scores. And once again you can generate the joint distribution:\n",
    "\n",
    "#### $$P(I, S, G)$$\n",
    "\n",
    "You can then look at the probability of S and G given $i^0$:\n",
    "\n",
    "#### $$P(S, G|i^0)$$\n",
    "\n",
    "And we can then ask, how does that decompose and is that independent when look at the probability of S given $i^0$ and G given $i^0$?\n",
    "\n",
    "#### $$P(S|i^0) \\; and \\; P(G|i^0)$$\n",
    "\n",
    "### 1.2.2 Conditional can lose Independencies\n",
    "<img src=\"images/cond-ind-3.png\" height=\"400\" width=\"400\">\n",
    "\n",
    "Now we can think about the case where intelligence and difficult are influencing the grade. We have seen that I and D are independent in the original distribution, they are **not independent** when we condition on grade. We can see that I and D are not independent in the conditional distribution, even though they were in the marginal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
