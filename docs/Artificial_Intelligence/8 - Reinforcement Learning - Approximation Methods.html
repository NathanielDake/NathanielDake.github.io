
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Artificial_Intelligence.html">Artificial Intelligence</a>
</li>
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="8.-Approximation-Methods">8. Approximation Methods<a class="anchor-link" href="#8.-Approximation-Methods">&#182;</a></h1><p>We are now going to look at approximation methods. Recall that in the last section, we discussed a major disadvantage to all of the methods we have studied so far. That is, they all require us to estimate the value function for each state, and in the case of the action-value function, we have to estimate it for each state and action pair. We learned early on that the state space can grow very large, very quickly. This makes <em>all</em> of the methods we have studied impractical.</p>
<blockquote><ul>
<li>$V$ - Need to estimate |S| values</li>
<li>$Q$ - Need to estimate |S|x|A| values</li>
<li>|S| and |A| can be very large</li>
</ul>
</blockquote>
<p>The solution to this is <em><strong>approximation</strong></em>.</p>
<h2 id="1.1-Approximation-Theory">1.1 Approximation Theory<a class="anchor-link" href="#1.1-Approximation-Theory">&#182;</a></h2><p>Recall from our earlier work concerning deep learning, that neural networks are universal function approximators. That means that given the right architecture, a neural network can approximate any function to an arbitrary degree of accuracy. In practice, they do not perform perfectly, but they do perform very well.</p>
<p>Mathematically, what we are trying to do is first do a feature extraction: so from the state $s$ we can extract a feature vector $x$:</p>
<p>$$x = \varphi (s)$$</p>
<p>Our goal is to then find a <em>function</em> that takes in a feature vector $x$, and a set of parameters $\theta$, that faithfully approximates the value function $V(s)$:</p>
<p>$$\hat{f}(x, \theta) \approx V(s)$$</p>
<h2 id="1.2-Linear-Approximation">1.2 Linear Approximation<a class="anchor-link" href="#1.2-Linear-Approximation">&#182;</a></h2><p>In this section, we are going to focus specifically on linear methods. We will see that function approximation methods require us to use models that are <em>differentiable</em>, hence we wouldn't be able to use something like a decision-tree or k-nearest neighbor. In the next set of notebooks (RL with deep learning) we will look at using deep learning methods, which are also differentiable. Unlike linear models, we won't need to do feature engineering before hand, although we could. Models like convolutional neural networks will allow us to use raw pixel data as the state, and the neural network will do its own automatic feature extraction and selection. However, those are harder to implement and take away from the fundamentals of RL, so we will hold off on them for now. For now, all we will need to know are linear regression and gradient descent.</p>
<h2 id="1.3-Section-8-Outline">1.3 Section 8 Outline<a class="anchor-link" href="#1.3-Section-8-Outline">&#182;</a></h2><p>We are going to proceed with the following outline for this section:</p>
<blockquote><ul>
<li>We are first going to apply approximation methods to Monte Carlo Prediction. That means we will be estimating the value function given a fixed policy. But instead of representing the value function as a dictionary indexed by state, we will use a linear function approximator. Recall that MC methods require us to play the entire episodes and calculate the returns before doing any updates. So next we will...</li>
<li>Apply approximation methods to <code>TD(0)</code> prediction. Remember, <code>TD(0)</code> takes aspects of both MC sampling and the bootstrap method of DP. </li>
<li>After working on the prediction problem, we will move to the control problem, and we will use SARSA for this. But we will of course be replacing $Q$ with a linear function approximator. </li>
</ul>
</blockquote>
<h2 id="1.4-Sanity-Checking">1.4 Sanity Checking<a class="anchor-link" href="#1.4-Sanity-Checking">&#182;</a></h2><p>One thing to keep in mind in this section, is that we can always sanity check our results by comparing to the non-approximated version. We expect our approximation to be close, but not perfect. One obstacle that we may encounter is that our algorithm may be implemented perfectly, but your model is bad. Remember, linear models are <em>not</em> very expressive. So, if we extract a poor set of features, the model won't be able to learn the value function well. In other words, the model will have a large error. To avoid this, we need to proactively think about what features are good for mapping states to values. We will need to put in manual work for feature engineering in order to improve our results.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="2.-Linear-Models-for-RL">2. Linear Models for RL<a class="anchor-link" href="#2.-Linear-Models-for-RL">&#182;</a></h1><p>What we are going to do now is apply supervised learning to reinforcement learning. Recall that supervised learning basically amounts to function approximation. We are trying to find a parameterized function that closely approximates the true function. In this case, that true function is the value function that we use to solve MDPs.</p>
<p>Earlier in the course, we talked about the fact that rewards have to be real numbers. Since returns are sums of rewards, they also have to be real numbers. And since values are expected values of returns, they are also real numbers. So, thinking about the supervised learning techniques we have at our disposal-classification and regression-it should be clear that what we want to do here is regression.</p>
<h2 id="2.2-Error">2.2 Error<a class="anchor-link" href="#2.2-Error">&#182;</a></h2><p>In particular, we want our estimate, $\hat{V}$, to be close to the true $V$. As we know, for all supervised learning methods we need a cost function, and the appropriate cost function for linear regression is <em>squared error</em>. We can represent it as follows:</p>
<p>$$Error = \big[V(s) - \hat{V}(s)\big]^2$$</p>
<p>Now that we have our basic error function, we can replace $V$ with is definition:</p>
<p>$$Error = \big[E[G(t) \mid S_t = s] - \hat{V}(s)\big]^2$$</p>
<p>However, since we do not know this expected value, we need to replace it with something else. In particular, we can take what we learned from Monte Carlo, and we can replace it with the <em>sample mean</em> of the actual returns:</p>
<p>$$Error = \big[\frac{1}{N}\sum_{i=1}^N G_{i,s}- \hat{V}(s)\big]^2$$</p>
<p>An alternative way of looking at this is that we treat each state and return pair as a training sample. In this way, we will try to minimize the individual squared differences between $G$ and $\hat{V}$ simultaneously. This will look just like linear regression, as expected:</p>
<p>$$Error = \sum_{i=1}^N \big[G_{i,s} - \hat{V}(s) \big]^2$$</p>
<p>$$Error = \sum_{i=1}^N \big(y_i - \hat{y}_i \big)^2$$</p>
<h2 id="2.3-Stochastic-Gradient-Descent">2.3 Stochastic Gradient Descent<a class="anchor-link" href="#2.3-Stochastic-Gradient-Descent">&#182;</a></h2><p>The advantage of representing the error in this way, is that it allows us to do <em>stochastic gradient descent</em>. This is where we take a small step in the direction of the gradient, with respect to the cost of only one sample at a time. This is perfect for our needs, because at every iteration of the game, we only have one sample (state and return pair) to look at.</p>
<h2 id="2.4-Gradient-Descent">2.4 Gradient Descent<a class="anchor-link" href="#2.4-Gradient-Descent">&#182;</a></h2><p>We can recall, that in linear regression our function approximator is parameterized by a set of weights (generally either refered to as $w$ of $\theta$). Here, we can let $\hat{V}$ be parameterized by $\theta$. In other words, what we are trying to find is the $\theta$ that allows $\hat{V}$ to be the best approximation. To achieve this, we want to do gradient descent with respect to $\theta$, and minimize the error we derived earlier.</p>
<p>$$\theta = \theta - \alpha \frac{\partial E}{\partial \theta}$$</p>
<p>For clarity, recall that $\frac{\partial E}{\partial \theta}$ is just representing how the squared error changes with respect to a change in $\theta$. And once again, $\alpha$ represents the learning rate. We can take this a step further, and replace the error with the squared difference we just derived earlier:</p>
<p>$$\frac{\partial E}{\partial \theta} = \frac{\big[G_{i,s}- \hat{V}(s, \theta)\big]^2}{\partial \theta}$$</p>
<p>$$\theta = \theta + \alpha \Big( G - \hat{V}(s, \theta)\Big) \frac{\partial \hat{V}(s,\theta)}{\partial \theta}$$</p>
<p>Note: above the 2 from the exponent is drop after the derivative is taken. Remember, this is stochastic gradient descent, so we are only looking at one sample of $G$ at a time.</p>
<h2 id="2.5-Gradient-Descent-for-Linear-Models">2.5 Gradient Descent for Linear Models<a class="anchor-link" href="#2.5-Gradient-Descent-for-Linear-Models">&#182;</a></h2><p>Recall that we are only looking at linear models in this class, so $\hat{V}$ is the dot product of the feature vector $x$ and $\theta$.</p>
<p>$$\hat{V}(s, \theta) = \theta^T \varphi(s) = \theta^T x$$</p>
<p>This dot product is just a linear combination, which can be expanded into the more familiar form:</p>
<p>$$\hat{V}(s, \theta) = \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n $$</p>
<p>In other words, the derivative of $V$ with respect to $\theta$ is $x$:</p>
<p>$$\frac{\partial \hat{V} (s, \theta)}{\partial \theta} = x$$</p>
<p>So we can formulate our new update rule as follows:</p>
<p>$$\theta = \theta + \alpha \Big( G - \hat{V}(s, \theta)\Big) x$$</p>
<h2 id="2.6-Relationship-to-Monte-Carlo">2.6 Relationship to Monte Carlo<a class="anchor-link" href="#2.6-Relationship-to-Monte-Carlo">&#182;</a></h2><p>Something interesting happens when we think back to our Monte Carlo methods; in other words, when we were not parameterizing $V$. Instead, $\hat{V}$ itself was the parameter we were trying to find, and we were trying to find it for all states $s$. If $\hat{V}$ itself is the parameter, then we get this update equation:</p>
<p>$$\hat{V}(s) = \hat{V}(s) + \alpha \big(G_s - \hat{V}(s)\big) \frac{\partial \hat{V}(s)}{\partial \hat{V}(s)}$$</p>
<p>$$\hat{V}(s) = \hat{V}(s) + \alpha \big(G_s - \hat{V}(s)\big)$$</p>
<p>But we can recall that this is the exact same equation that we had before for updating the mean! So, we can see what we were doing before to find $V$ was actually an instance of gradient descent.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="3.-Feature-Engineering">3. Feature Engineering<a class="anchor-link" href="#3.-Feature-Engineering">&#182;</a></h1><p>We are now going to look at how feature engineering can be applied in the case of RL. Recall, Neural Networks can in some sense automatically find good nonlinear transformations/features of the raw data. But, we are only looking at linear methods for now, which means we will need to come up with features on our own.</p>
<h2 id="3.1-Mapping-s-$\rightarrow$-x">3.1 Mapping s $\rightarrow$ x<a class="anchor-link" href="#3.1-Mapping-s-$\rightarrow$-x">&#182;</a></h2><p>One way to think of states is that they are categorical variables. For instace:</p>
<blockquote><ul>
<li><strong>(0,0)</strong> $\rightarrow$ category 1 </li>
<li><strong>(0,1)</strong> $\rightarrow$ category 2</li>
<li>and so on...</li>
</ul>
</blockquote>
<p>How do we treat categorical variables? We do so via the technique <em><strong>one-hot encoding</strong></em>. So, what is do one-hot encoding? Well, we have $S$ states, so the dimensionality of $x$, which we will call $D$, is:</p>
<p>$$D = \mid S \mid$$</p>
<p>This means that we have a long feature vector of size $\mid S \mid$. Then for each of the states, we set one of the values in $x$ to 1:</p>
<p>$$s = (0,0) \rightarrow x = [1,0,0,...]$$</p>
<p>$$s = (0,1) \rightarrow x = [0,1,0,...]$$</p>
<p>The problem with this is that it doesn't allow us to compress the amount of space it takes to store the value function! It requires the same number of parameters as measuring $V(s)$ directly. Remember, compressing the amount of space is the whole reason we are doing this in the first place. If we store $V$ as a dictionary, it will have $\mid S \mid $ keys, and $\mid S \mid$ values. Hence, we make no improvement if we do one hot encoding. In fact, it is equivalent to what we were doing before, since each $\theta$ would represent the value for the corresponding state:</p>
<p>$$V(s=0) = \theta^T [1,0,0,...] = \theta_0$$</p>
<h2 id="3.2-One-Hot-Encoding">3.2 One-Hot Encoding<a class="anchor-link" href="#3.2-One-Hot-Encoding">&#182;</a></h2><p>There is, however, one positive aspect to using one-hot encoding for your feature transformation. Let's suppose your algorithm isn't working, and $\hat{V}$ isn't representing the true $V$ very well. One reason your $\hat{V}$ may not be good, is because your features may be bad! So, your code may be fine and free of bugs, but still yield poor results because the features are bad. In that case, you could change your feature transformer to use one-hot encoding, where you could predict each $V(s)$ individually. If you do this and your code works, that tells your that your features are bad (since it's the same as a non-approximation method).</p>
<h2 id="3.3-Alternative-to-One-Hot-Encoding">3.3 Alternative to One-Hot Encoding<a class="anchor-link" href="#3.3-Alternative-to-One-Hot-Encoding">&#182;</a></h2><p>So, if we know that one-hot encoding is bad, then what is good? Well, in the case of grid world, consider that each (i,j) represents a position in 2-d space. Therefore, it is more like a real number than a category. So, we can represent the $x$ vector as simply (i,j) itself. You may want to scale it so that its mean is 0 and variance is 1. We would consider this feature vector to simply be the raw data, without any feature engineering:</p>
<p>$$(x_1, x_2) = (i,j)$$</p>
<p>So, what is the problem with just setting $x$ to be the location of the agent? Well, remember that our model is linear. Let's say $j$ is fixed-that means $V$ can only change linearly with respect to $i$. That means $V$ can only be a line with respect to this x coordinate. So, it is always increasing, or always decreasing-this is <em>not</em> very expressive.</p>
<h2 id="3.4-Polynomials">3.4 Polynomials<a class="anchor-link" href="#3.4-Polynomials">&#182;</a></h2><p>Recall, that one easy way to make new features is by creating polynomials. In calculus, it is shown that an infinite Taylor Expansion can approximate any function. So, if we start with $x_1$ and $x_2$ we can create the terms:</p>
<p>$$x_2^2$$</p>
<p>$$x_2^2$$</p>
<p>$$x_1 x_2$$</p>
<p>We can also create higher order polynomials, but we must be careful of overfitting. This is the method we will use from here on out.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="4.-Monte-Carlo-Prediction-with-Approximation">4. Monte Carlo Prediction with Approximation<a class="anchor-link" href="#4.-Monte-Carlo-Prediction-with-Approximation">&#182;</a></h1><p>We can now start applying approximation methods to the prediction problem and control problem. We are going to start with the prediction problem! We are going to use Monte Carlo estimation but replace the value function dictionary with our linear model of the value function.</p>
<h2 id="4.1-Two-Main-Steps">4.1 Two Main Steps<a class="anchor-link" href="#4.1-Two-Main-Steps">&#182;</a></h2><p>Recall that for MC estimation, we have two main steps which we want to do repeatedly.</p>
<blockquote><ol>
<li>Play the game and calculate a list of states and returns. </li>
<li>Update $\hat{V}(S)$ using the return as the target and $V(S)$ as the prediction:<br>
<br>
$$\hat{V}(s) = \hat{V}(s) + \alpha \big(G_s - \hat{V}(s) \big)$$
<br>
Remember, this is gradient descent, but it is also equivalent to calculating the mean of all the returns for this state.</li>
</ol>
</blockquote>
<h2 id="4.2-With-Approximation">4.2 With Approximation<a class="anchor-link" href="#4.2-With-Approximation">&#182;</a></h2><p>Since in this lecture we are approximation $V(s)$ with the parameter $\theta$, what we want to update instead is the parameter $\theta$:</p>
<blockquote><ol>
<li>Play the game and calculate a list of states and returns. </li>
<li>Update $\hat{V}(S)$ as average of returns:<br>
<br>
$$\theta = \theta + \alpha \big(G - \hat{V}(s, \theta) \big)x$$
<br>
Here, we continue to do stochastic gradient descent, but with respect to $\theta$ instead of $\hat{V}$. </li>
</ol>
</blockquote>
<h2 id="4.3-Pseudocode">4.3 Pseudocode<a class="anchor-link" href="#4.3-Pseudocode">&#182;</a></h2><p>Remember that this is for a fixed policy, since we are focusing on the prediction problem. Let's look at some pseudocode to solidify this idea:</p>
<hr>
<p>$
\text{def mc_approx_prediction}(\pi)\text{:} \\
\hspace{1cm} \theta =\text{random} \\
\hspace{1cm} \text{for i=1..N:} \\
\hspace{2cm} \text{states_and_returns = play_game} \\
\hspace{2cm} \text{for s, g in states_and_returns:} \\
\hspace{3cm} x = \Phi(s)\\
\hspace{3cm} \theta \leftarrow \theta + \alpha (g - \theta^T x)x \\
$</p>
<hr>
<blockquote><ul>
<li>We start by taking in an input policy, $\pi$, and randomly initializing the $\theta$ vector</li>
<li>Next, we enter a loop for some number of iterations</li>
<li>On each iteration, we play the game, which returns a sequence of states and returns</li>
<li>Loop through these states and returns, and apply our update equation to $\theta$, treating our return as the target </li>
</ul>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="5.-MC-Prediction-with-Approximation-in-Code">5. MC Prediction with Approximation in Code<a class="anchor-link" href="#5.-MC-Prediction-with-Approximation-in-Code">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">standard_grid</span><span class="p">,</span> <span class="n">negative_grid</span><span class="p">,</span> <span class="n">print_policy</span><span class="p">,</span> <span class="n">print_values</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">random_action</span><span class="p">,</span> <span class="n">play_game</span><span class="p">,</span> <span class="n">SMALL_ENOUGH</span><span class="p">,</span> <span class="n">GAMMA</span><span class="p">,</span> <span class="n">ALL_POSSIBLE_ACTIONS</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Note: This is policy evaluation, not optimization </span>

<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># use the standard grid again (0 for every step) so that we can compare</span>
  <span class="c1"># to iterative policy evaluation</span>
  <span class="n">grid</span> <span class="o">=</span> <span class="n">standard_grid</span><span class="p">()</span>

  <span class="c1"># print rewards</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rewards:&quot;</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>

  <span class="c1"># state -&gt; action</span>
  <span class="c1"># found by policy_iteration_random on standard_grid</span>
  <span class="c1"># MC method won&#39;t get exactly this, but should be close</span>
  <span class="c1"># values:</span>
  <span class="c1"># ---------------------------</span>
  <span class="c1">#  0.43|  0.56|  0.72|  0.00|</span>
  <span class="c1"># ---------------------------</span>
  <span class="c1">#  0.33|  0.00|  0.21|  0.00|</span>
  <span class="c1"># ---------------------------</span>
  <span class="c1">#  0.25|  0.18|  0.11| -0.17|</span>
  <span class="c1"># policy:</span>
  <span class="c1"># ---------------------------</span>
  <span class="c1">#   R  |   R  |   R  |      |</span>
  <span class="c1"># ---------------------------</span>
  <span class="c1">#   U  |      |   U  |      |</span>
  <span class="c1"># ---------------------------</span>
  <span class="c1">#   U  |   L  |   U  |   L  |</span>
  <span class="n">policy</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="s1">&#39;L&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span> <span class="s1">&#39;L&#39;</span><span class="p">,</span>
  <span class="p">}</span>
  
  <span class="c1"># Randomly initialize theta vector. Our model is V_hat = theta.dot(x), where</span>
  <span class="c1"># x = [row, column, row*column, 1] , where the 1 is for the bias term</span>
  <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> 
  
  <span class="c1"># Define a function that turns the state into a feature vector x. The only nonlinear </span>
  <span class="c1"># feature is the interaction effect between the i and j coordinate (s[0] and s[1])</span>
  <span class="k">def</span> <span class="nf">s2x</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  
  <span class="c1"># Repeat until converge - Main Loop</span>
  <span class="n">deltas</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">t</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">it</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">t</span> <span class="o">+=</span> <span class="mf">0.01</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="o">/</span><span class="n">t</span> <span class="c1"># Using decaying learning rate</span>
    
    <span class="c1"># Generate an episode using pi. Pattern is the same as before. We play the game, </span>
    <span class="c1"># and get a sequence of states and returns. We then loop through the states and </span>
    <span class="c1"># returns, but instead of updating V, we now update the parameter theta, using the </span>
    <span class="c1"># equation we derived earlier for stochastic gradient descent. </span>
    <span class="n">biggest_change</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">states_and_returns</span> <span class="o">=</span> <span class="n">play_game</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">policy</span><span class="p">)</span>
    <span class="n">seen_states</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">G</span> <span class="ow">in</span> <span class="n">states_and_returns</span><span class="p">:</span>
      <span class="c1"># Check if we have already seen s. This is &#39;first-visit&#39; MC policy evaluation</span>
      <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_states</span><span class="p">:</span>
        <span class="n">old_theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">s2x</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">V_hat</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">V_hat</span><span class="p">)</span><span class="o">*</span><span class="n">x</span>
        <span class="n">biggest_change</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">old_theta</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
        <span class="n">seen_states</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">)</span>
  
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">deltas</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
  
  <span class="c1"># Obtain predicted values</span>
  <span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">states</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">all_states</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">grid</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
      <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s2x</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># terminal state or state we can&#39;t otherwise get to</span>
      <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;values:&quot;</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;policy:&quot;</span><span class="p">)</span>
  <span class="n">print_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>rewards:
---------------------------
 0.00| 0.00| 0.00| 1.00|
---------------------------
 0.00| 0.00| 0.00|-1.00|
---------------------------
 0.00| 0.00| 0.00| 0.00|
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>values:
---------------------------
 0.43| 0.59| 0.75| 0.00|
---------------------------
 0.37| 0.00| 0.36| 0.00|
---------------------------
 0.32| 0.15|-0.02|-0.19|
policy:
---------------------------
  R  |  R  |  R  |     |
---------------------------
  U  |     |  U  |     |
---------------------------
  U  |  L  |  U  |  L  |
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="6.-TD(0)-Semi-Gradient-Prediction">6. <code>TD(0)</code> Semi-Gradient Prediction<a class="anchor-link" href="#6.-TD(0)-Semi-Gradient-Prediction">&#182;</a></h1><p>We now ask: Can we apply approximation's to TD learning, in addition to Monte Carlo? The answer is that yes we can, but there is one caveat. Let's start by looking at the algorithm.</p>
<p>Recall that the main difference between Monte Carlo and Temporal Difference Learning, is that with TD we don't use the actual return. We instead estimate the return based on the reward and the value function for the next state, i.e. instead of using $G$ we use:</p>
<p>$$r + \gamma V(s')$$</p>
<p>If we then look at how this transforms our algorithm:</p>
<p>$$\theta = \theta + \alpha \big(G - \hat{V}(s, \theta) \big)x$$</p>
<p>$$\theta = \theta + \alpha \big(r + \gamma \hat{V}(s', \theta) - \hat{V}(s, \theta) \big)\frac{\partial \hat{V}(s,\theta)}{\partial \theta}$$</p>
<p>$$\theta = \theta + \alpha \big(target - prediction \big)\frac{\partial prediction}{\partial \theta}$$</p>
<p>We can quickly see that we run into a problem! Our target is not a real target, because it requires a prediction that the model makes; in particular, $\hat{V}(s',\theta)$. So, in a sense we are using the model output as a target to fix the model parameters, which seems strange. However, at the same time it works, so we do it anyways.</p>
<p>We call this a <em><strong>semi-gradient</strong></em> method because since the target we are using is not a true target, the gradient we are using is not a true gradient.</p>
<h2 id="6.1-TD(0)-Semi-Gradient-Prediction-in-Code">6.1 <code>TD(0)</code> Semi-Gradient Prediction in Code<a class="anchor-link" href="#6.1-TD(0)-Semi-Gradient-Prediction-in-Code">&#182;</a></h2><p>Remeber, there is a main differences between this and Monte Carlo:</p>
<blockquote><ol>
<li>In MC, we do not use the returns in the update for $\theta$. Because of this, when we play the game we also don't need to bother calculating the returns, since we will be using the rewards directly. </li>
</ol>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">standard_grid</span><span class="p">,</span> <span class="n">negative_grid</span><span class="p">,</span> <span class="n">print_policy</span><span class="p">,</span> <span class="n">print_values</span><span class="p">,</span> <span class="n">ALPHA</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">random_action</span><span class="p">,</span> <span class="n">play_game_td</span><span class="p">,</span> <span class="n">SMALL_ENOUGH</span><span class="p">,</span> <span class="n">GAMMA</span><span class="p">,</span> <span class="n">ALL_POSSIBLE_ACTIONS</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span> 
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    
  <span class="k">def</span> <span class="nf">s2x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="s2">&quot;Transforms states into feature vector. Same feature transformation as MC.&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="s2">&quot;Takes in state s, transforms into x, and returns dot product between theta and x.&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s2x</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="s2">&quot;Gradient function, just returns x.&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">s2x</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
  
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="c1"># use the standard grid again (0 for every step) so that we can compare</span>
  <span class="c1"># to iterative policy evaluation</span>
  <span class="n">grid</span> <span class="o">=</span> <span class="n">standard_grid</span><span class="p">()</span>

  <span class="c1"># print rewards</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rewards:&quot;</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>

  <span class="c1"># state -&gt; action</span>
  <span class="n">policy</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span> <span class="s1">&#39;U&#39;</span><span class="p">,</span>
  <span class="p">}</span>
  
  <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
  <span class="n">deltas</span> <span class="o">=</span> <span class="p">[]</span>
  
  <span class="c1"># Main loop. Repeat until convergence.</span>
  <span class="n">k</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">it</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">k</span> <span class="o">+=</span> <span class="mf">0.01</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">ALPHA</span> <span class="o">/</span> <span class="n">k</span>
    <span class="n">biggest_change</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># Play game to get sequence of states and rewards</span>
    <span class="n">states_and_rewards</span> <span class="o">=</span> <span class="n">play_game_td</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">policy</span><span class="p">)</span>
    
    <span class="c1"># The first (s, r) tuple is the state we start in and 0 (since we don&#39;t </span>
    <span class="c1"># get a reward) for simply starting the game. The last (s,r) tuple is the </span>
    <span class="c1"># terminal state and the final reward. The value for the terminal state is</span>
    <span class="c1"># by definition 0, so we don&#39;t care about updating it. </span>
    <span class="c1"># Loop through states and rewards.</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">states_and_rewards</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">states_and_rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
      <span class="n">s2</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">states_and_rewards</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
      
      <span class="c1"># We will update V(s) AS we experience the episode</span>
      <span class="n">old_theta</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">grid</span><span class="o">.</span><span class="n">is_terminal</span><span class="p">(</span><span class="n">s2</span><span class="p">):</span>
        <span class="c1"># Since we know value at terminal state is 0, we don&#39;t make prediction if s2 </span>
        <span class="c1"># is terminal</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">r</span>
        
      <span class="k">else</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span>
      <span class="c1"># Implement update equation for theta</span>
      <span class="n">model</span><span class="o">.</span><span class="n">theta</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
      <span class="n">biggest_change</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">old_theta</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">)</span>
  
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">deltas</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="c1"># obtain predicted values</span>
  <span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">states</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">all_states</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">grid</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
      <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># terminal state or state we can&#39;t otherwise get to</span>
      <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;values:&quot;</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;policy:&quot;</span><span class="p">)</span>
  <span class="n">print_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>rewards:
---------------------------
 0.00| 0.00| 0.00| 1.00|
---------------------------
 0.00| 0.00| 0.00|-1.00|
---------------------------
 0.00| 0.00| 0.00| 0.00|
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>values:
---------------------------
 0.29| 0.40| 0.51| 0.00|
---------------------------
 0.14| 0.00|-0.08| 0.00|
---------------------------
-0.00|-0.34|-0.67|-1.00|
policy:
---------------------------
  R  |  R  |  R  |     |
---------------------------
  U  |     |  R  |     |
---------------------------
  U  |  R  |  R  |  U  |
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="7.-Semi-Gradient-SARSA">7. Semi-Gradient SARSA<a class="anchor-link" href="#7.-Semi-Gradient-SARSA">&#182;</a></h1><p>We are now going to move onto the control problem (choosing the optimal policy). However, instead of using $Q$, we are going to create a parameterized model to approximate $Q$. Note, this is going to be more difficult than approximating $V$, since instead of approximating $\mid S \mid$ values, we now have to approximate $\mid S \mid x \mid A \mid$ values. However, the basic idea is the same. Recall that our target is:</p>
<p>$$target = r  +\gamma \hat{Q}(s',a')$$</p>
<p>And our prediction is:</p>
<p>$$prediction = \hat{Q}(s,a)$$</p>
<p>Since $\hat{Q}$ is an approximation and it appears in both the target and the prediction, this is also not a true gradient, so this is also a semi-gradient method:</p>
<p>$$\theta = \theta + \alpha \big [r  +\gamma \hat{Q}(s',a') - \hat{Q}(s,a)\big] \frac{\partial \hat{Q}(s,a)}{\partial \theta}$$</p>
<h2 id="7.1-Features">7.1 Features<a class="anchor-link" href="#7.1-Features">&#182;</a></h2><p>Since $Q$ is indexed by both $s$ and $a$, we need to find a way to combine them into a feature vector $x$:</p>
<p>$$(s,a) \rightarrow x$$</p>
<p>One simple method, would be to take what we had before for $V$ ($row, column, row*column$), and just one hot encode the actions. So, in effect we would have:</p>
<p>$$x = (row, column, row*column, U, D, L, R, 1)$$</p>
<p>However, if you do this, you will find that it is not quite expressive enough. In other words, a linear model given these features can't accurately approximate $Q$.</p>
<p>What does seem to work better is to add squared terms, and have a separate set of features for each action. So, for each action, we now have features that look like:</p>

<pre><code>x = [
  r &amp;&amp; U, 
  c &amp;&amp; U,
  r*r &amp;&amp; U,
  c*c &amp;&amp; U,
  r*c &amp;&amp; U,
  1 &amp;&amp; U,
  r &amp;&amp; D,
  ...
]</code></pre>
<p>The 1 represents the one hot encoded action (not really a bias). You may wonder, given so many features does this still compress $Q$? Well, we have:</p>
<p>$$(6 \; per \; action) * (4 \; actions) + 1 \; bias = 25 \; features$$</p>
<p>Whereas our tabular method (storing $Q$ as a table) had:</p>
<p>$$9 \; states * 4 \; actions = 36$$</p>
<h2 id="7.2-Semi-Gradient-SARSA-in-Code">7.2 Semi-Gradient SARSA in Code<a class="anchor-link" href="#7.2-Semi-Gradient-SARSA-in-Code">&#182;</a></h2><p>We are now going to implement semi-gradient SARSA in code to find the optimal policy (and solve the control problem)!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">standard_grid</span><span class="p">,</span> <span class="n">negative_grid</span><span class="p">,</span> <span class="n">print_policy</span><span class="p">,</span> <span class="n">print_values</span><span class="p">,</span> <span class="n">ALPHA</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">random_action_td</span><span class="p">,</span> <span class="n">play_game_td</span><span class="p">,</span> <span class="n">SMALL_ENOUGH</span><span class="p">,</span> <span class="n">GAMMA</span><span class="p">,</span> <span class="n">ALL_POSSIBLE_ACTIONS</span>
<span class="kn">from</span> <span class="nn">common</span> <span class="k">import</span> <span class="n">max_dict</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Dictionary to map state-action pairs to indexes, and global index that keeps track </span>
<span class="c1"># of number of state action pairs that we have already seen so that it can assign </span>
<span class="c1"># the next index.</span>
<span class="n">SA2IDX</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">IDX</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="s2">&quot;Initializes theta. Dimensionality is now 25.&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">sa2x</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="c1"># NOTE: using just (r, c, r*c, u, d, l, r, 1) is not expressive enough</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>              <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;U&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.5</span>            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;U&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;U&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;U&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">4.5</span><span class="p">)</span><span class="o">/</span><span class="mf">4.5</span> <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;U&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="mi">1</span>                     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;U&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>              <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;D&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.5</span>            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;D&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;D&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;D&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">4.5</span><span class="p">)</span><span class="o">/</span><span class="mf">4.5</span> <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;D&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="mi">1</span>                     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;D&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>              <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;L&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.5</span>            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;L&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;L&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;L&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">4.5</span><span class="p">)</span><span class="o">/</span><span class="mf">4.5</span> <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;L&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="mi">1</span>                     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;L&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>              <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;R&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">1.5</span>            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;R&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;R&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;R&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mf">4.5</span><span class="p">)</span><span class="o">/</span><span class="mf">4.5</span> <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;R&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="mi">1</span>                     <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="s1">&#39;R&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
      <span class="mi">1</span>
    <span class="p">])</span>
  
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="s2">&quot;Takes in state s, transforms into x, and returns dot product between theta and x.&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa2x</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="s2">&quot;Gradient function, just returns x.&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa2x</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">getQs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;At some point during SARSA, we need to take the argmax over Q, but we can&#39;t do that</span>
<span class="sd">  if Q isn&#39;t represented as a dictionary. So, the job of this function is to turn the </span>
<span class="sd">  predictions into a dictionary given some state s.&quot;&quot;&quot;</span>
  <span class="n">Qs</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ALL_POSSIBLE_ACTIONS</span><span class="p">:</span>
    <span class="n">q_sa</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
    <span class="n">Qs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">q_sa</span>
  <span class="k">return</span> <span class="n">Qs</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">grid</span> <span class="o">=</span> <span class="n">negative_grid</span><span class="p">(</span><span class="n">step_cost</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">)</span>

  <span class="c1"># print rewards</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rewards:&quot;</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>

  <span class="c1"># no policy initialization, we will derive our policy from most recent Q</span>
  <span class="c1"># enumerate all (s,a) pairs, each will have its own weight in our &quot;dumb&quot; model</span>
  <span class="c1"># essentially each weight will be a measure of Q(s,a) itself</span>
  <span class="n">states</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">all_states</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
    <span class="n">SA2IDX</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">ALL_POSSIBLE_ACTIONS</span><span class="p">:</span>
      <span class="n">SA2IDX</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">IDX</span>
      <span class="n">IDX</span> <span class="o">+=</span> <span class="mi">1</span>
  
  <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
  
  <span class="c1"># Main Loop - repeat until convergence</span>
  <span class="n">t</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="n">t2</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="n">deltas</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">it</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">t</span> <span class="o">+=</span> <span class="mf">0.01</span>
      <span class="n">t2</span> <span class="o">+=</span> <span class="mf">0.01</span>
    <span class="k">if</span> <span class="n">it</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;it:&quot;</span><span class="p">,</span> <span class="n">it</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">ALPHA</span> <span class="o">/</span> <span class="n">t2</span>
    
    <span class="c1"># instead of &#39;generating&#39; an epsiode, we will PLAY</span>
    <span class="c1"># an episode within this loop</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># start state</span>
    <span class="n">grid</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># get Q(s) so we can choose the first action</span>
    <span class="n">Qs</span> <span class="o">=</span> <span class="n">getQs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    
    <span class="c1"># the first (s, r) tuple is the state we start in and 0</span>
    <span class="c1"># (since we don&#39;t get a reward) for simply starting the game</span>
    <span class="c1"># the last (s, r) tuple is the terminal state and the final reward</span>
    <span class="c1"># the value for the terminal state is by definition 0, so we don&#39;t</span>
    <span class="c1"># care about updating it.</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">max_dict</span><span class="p">(</span><span class="n">Qs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">random_action_td</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="o">/</span><span class="n">t</span><span class="p">)</span> <span class="c1"># epsilon-greedy</span>
    <span class="n">biggest_change</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">grid</span><span class="o">.</span><span class="n">game_over</span><span class="p">():</span>
      <span class="n">r</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">move</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="n">s2</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">current_state</span><span class="p">()</span>

      <span class="c1"># we need the next action as well since Q(s,a) depends on Q(s&#39;,a&#39;)</span>
      <span class="c1"># if s2 not in policy then it&#39;s a terminal state, all Q are 0</span>
      <span class="n">old_theta</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">grid</span><span class="o">.</span><span class="n">is_terminal</span><span class="p">(</span><span class="n">s2</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">theta</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="c1"># not terminal</span>
        <span class="n">Qs2</span> <span class="o">=</span> <span class="n">getQs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">max_dict</span><span class="p">(</span><span class="n">Qs2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">random_action_td</span><span class="p">(</span><span class="n">a2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="o">/</span><span class="n">t</span><span class="p">)</span> <span class="c1"># epsilon-greedy</span>

        <span class="c1"># we will update Q(s,a) AS we experience the episode</span>
        <span class="n">model</span><span class="o">.</span><span class="n">theta</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">GAMMA</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="n">a2</span><span class="p">)</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        
        <span class="c1"># next state becomes current state</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s2</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a2</span>

      <span class="n">biggest_change</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">theta</span> <span class="o">-</span> <span class="n">old_theta</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">biggest_change</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">deltas</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="c1"># determine the policy from Q*</span>
  <span class="c1"># find V* from Q*</span>
  <span class="n">policy</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">Q</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">grid</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">Qs</span> <span class="o">=</span> <span class="n">getQs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qs</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">max_q</span> <span class="o">=</span> <span class="n">max_dict</span><span class="p">(</span><span class="n">Qs</span><span class="p">)</span>
    <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>
    <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_q</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;values:&quot;</span><span class="p">)</span>
  <span class="n">print_values</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;policy:&quot;</span><span class="p">)</span>
  <span class="n">print_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>rewards:
---------------------------
-0.10|-0.10|-0.10| 1.00|
---------------------------
-0.10| 0.00|-0.10|-1.00|
---------------------------
-0.10|-0.10|-0.10|-0.10|
it: 0
it: 1000
it: 2000
it: 3000
it: 4000
it: 5000
it: 6000
it: 7000
it: 8000
it: 9000
it: 10000
it: 11000
it: 12000
it: 13000
it: 14000
it: 15000
it: 16000
it: 17000
it: 18000
it: 19000
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>values:
---------------------------
 0.63| 0.81| 1.03| 0.00|
---------------------------
 0.45| 0.00| 0.63| 0.00|
---------------------------
 0.27| 0.07| 0.17| 0.59|
policy:
---------------------------
  R  |  R  |  R  |     |
---------------------------
  U  |     |  U  |     |
---------------------------
  U  |  U  |  U  |  U  |
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="8.-Summary">8. Summary<a class="anchor-link" href="#8.-Summary">&#182;</a></h1><p>This section was all about approximation methods. We learned eariler in our discussions of RL that the state space of an MDP can quickly grow too large to enumerate. Hence, trying to store a value function for every state that needs to be defined in the state space can be infeasible. Approximation methods allow us to parameterize the value function, so that effectively we need a smaller number of parameters than the size of the state space.</p>
<p>Specifically, we saw that differentiable methods were a good fit, since they allow us to use <em>stochastic gradient descent</em> and continue learning in an online fashion, just like we have done throughout the course. Remember, online learning is desirable because episodes can be very long, so it is good if our agent can improve during the episode.</p>
<p>The type of differentiable model that we looked at was linear regression. But, we can just as easily apply this to neural networks and deep learning. Using linear regression, we were forced to engineer our own features, since the raw features we had- the $i$ coordinate and the $j$ coordinate- did not yield an expressive enough model.</p>
<p>We applied approximation methods to MC prediction, <code>TD(0)</code> prediction (semi-gradient), the control problem via the SARSA method (again, semi-gradient). Note that we did <em>not</em> apply approximation methods to <em><strong>Q-Learning</strong></em>. Recall that Q-learning is an off-policy method. Certain sources have stated that approximation techniques applied to off-policy control problems do not have good convergence characteristics. However, recall that the main difference between SARSA and Q-Learning is that the $Q$ that we use for the update does not necessarily correspond to the next action we take. See "Deep Q-Learning" for how this may be done.</p>
<h2 id="8.1-Next-steps">8.1 Next steps<a class="anchor-link" href="#8.1-Next-steps">&#182;</a></h2><p>It is important to realize that the only requirement for our function approximators is that they be differentiable. Recall that deep learning methods are also differentiable. Libraries like theano and tensorflow automatically calculate tensorflow for us, which would make the update equations easier in the case where we have lots of parameters and complex functions.</p>
<p>A few things that we didn't see, but would be worth going over are:</p>
<blockquote><ul>
<li><strong>Continuous state-spaces:</strong> We know that state spaces are not necessarily discrete. For example, if our state space is an image of our environment, images represent the real world-light intensity in a continuous 3-dimensional space. Light intensity is also continuous.</li>
<li><strong>Continuous action-spaces:</strong> Actions spaces are not necessarily discrete either. For example, your agent may need to decide how much force to apply to a motor. Force is a continuous variable. </li>
<li><strong>Policy Gradient Method:</strong> We didn't parameterize $\pi$ at all in this course, but that is a next step, commonly referred to as the policy gradient method. </li>
<li><strong>Store input target pairs:</strong> Use previous experience to learn. In other words, replay previous episode and continue to update parameters of model with gradient descent based on those old episodes. </li>
</ul>
</blockquote>

</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
