
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Artificial_Intelligence.html">Artificial Intelligence</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="3.-Components-of-an-RL-System">3. Components of an RL System<a class="anchor-link" href="#3.-Components-of-an-RL-System">&#182;</a></h1><p>Let's quickly recall what we had discussed earlier concerning the components of an RL system. We talked about the following:</p>
<p><img src="images/sar-flow.png" width="500"></p>
<blockquote><ul>
<li><strong>Agent</strong>: The thing that is playing the game, that we want to program the RL algorithm into.
<br>
<br></li>
<li><strong>Environment</strong>: The thing that the agent interacts with; the agents world.
<br>
<br> </li>
<li><strong>State</strong>: Specific configuration of the environment that the agent is sensing. Note, the state only represents that which the agent can sense. 
<br>
<br> </li>
<li><strong>Actions</strong>: Things that an agent can do that will affect its state. In Tic-Tac-Toe, that's placing a piece on the board. Performing an actions always brings us to the next state, which also comes with a possible reward. 
<br>
<br> </li>
<li><strong>Rewards</strong>: Tells you how good your action was, not whether it was a correct or incorrect action. It does not tell you whether it was the best or worst action, it is just a number. The rewards you have received over the course of your existence doesn't necessarily represent possible rewards you could get in the future. For example, you could search a bad part of state space, hit a local max of 10pts, while the global max was actually 1000 pts. The agent does not know that (but in our case we will, since we designed the game). </li>
</ul>
</blockquote>
<h2 id="1.1-Notation">1.1 Notation<a class="anchor-link" href="#1.1-Notation">&#182;</a></h2><p>We know that being in a state, $S(t)$, and taking an action $A(t)$, will lead us to the reward $R(t+1)$ and the state $S(t+1)$:</p>
<p>$$S(t), A(t) \rightarrow R(t+1), S(t+1)$$</p>
<p>However, when we drop the time index's, we represent this as the 4-tuple:</p>
<p>$$(s,a,r,s')$$</p>
<p>The $r$ is not given a prime as you would expect, but it is standard notation. So, the prime symbol doesn't strictly mean "at time t + 1"</p>
<h2 id="1.2-New-Terms">1.2 New Terms<a class="anchor-link" href="#1.2-New-Terms">&#182;</a></h2><p>The first new term we want to discuss is <strong>Episode</strong>. This represents one run of the game. For example, we will start a game of tic tac toe with an empty board, and as soon as one player gets 3 pieces in a row, that's the end of the episode. As you may imagine, our RL agent will learn across many episodes. For example, after playin 1000, 10000, or 100000 episodes, we can possibly have trained an intelligent agent. The number of episodes we will use is a hyper parameter and will depend on the game being played, the number of states, how random the game is, etc.</p>
<p>Playing the game tic-tac-toe is an <strong>episodic task</strong> because you can play it again and again. This is different from a <strong>continuous task</strong> which never ends. We will not be looking at continuous tasks in this course.</p>
<p>Now, the next question we ask is: when is the end of an episode? Well, there are certain states in the state space that tell us when the episode is over. These are states from which no more action can be taken. They are referred to as <strong>terminal states</strong>. For tic-tac-toe these are when one player gets 3 in a row, or when the board is full (a draw).</p>
<h2 id="1.3-Cart-Pole-/-Inverted-Pendulum">1.3 Cart-Pole / Inverted Pendulum<a class="anchor-link" href="#1.3-Cart-Pole-/-Inverted-Pendulum">&#182;</a></h2><p>This problem comes up all the time in RL and control systems. If you search google for inverted pendulum, you will see research papers concerning control systems, and if you search cart-pole you will see all kinds of RL research papers. At the beginning of an episode, the cart is stationary and the pole is perpendicular to the ground. Because the system is unstable, the pole will then begin to fall, and the job of the cart is to move so that the pole does not fall down.</p>
<p>When the pole falls so far that it is impossible to get back up, any angle past a threshold where it is impossible to get the pole back up is a terminal state. Note, because the angle in this example is a real number, is a continuous/infinite space. We will not deal with these.</p>
<h2 id="1.4-Assigning-Rewards">1.4 Assigning Rewards<a class="anchor-link" href="#1.4-Assigning-Rewards">&#182;</a></h2><p>One difficult problem in reinforcement learning that we will come across is: defining rewards. We, the programmers, can be thought of as coaches to the AI. The reward is something that we give to the agent. So, we can define how we are going to reward the agent, which will drive how it learns.</p>
<p>For example, if we just give it the same reward no matter what it does, then the agent will probably just end up acting randomly, since any action will lead to the same value. You don't want to do that, because it will encourage bad behavior.</p>
<p>A real situation could be seen with a robot trying to solve a maze. If it manages to exit the maze it would receive a reward of 1, else it would receive 0. Most people would think that this seems reasonable, and it is <em>semi-reasonable</em>. However, with this reward structure, the robot may never actually solve the maze. So, if it has only ever received a reward of 0, it may think that it is the best it can do. A better solution would be to give the robot a -1 for every step it takes, and then it would be encouraged to solve the maze as quickly as possible.</p>
<h3 id="1.4.1-Caution">1.4.1 Caution<a class="anchor-link" href="#1.4.1-Caution">&#182;</a></h3><p>One point of caution is to not build your own prior knowledge into the AI. For example, in a game such as chess, an agent should be rewarded only for winning, not taking opponent's pieces, or implementing some strategy that you read about in a chess book. You want to leave the agent free to come up with its own solution. The danger of rewarding the agent for achieving sub goals, is that they may find a novel way to maximize the reward for the subgoals, without actually winning the game. For example, taking all of the opponents chess pieces and then still losing the game.</p>
<p>So to summarize, we can say that:</p>
<blockquote><p>"The reward signal is your way of telling the agent what you want it to achieve, now how you want it to be achieved."</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="3.-The-Value-Function">3. The Value Function<a class="anchor-link" href="#3.-The-Value-Function">&#182;</a></h1><p>Take a moment to consider the following scenario. You have an exam tomorow. You would like to hang out with your friends. You know if you hangout with you friends you will most likely have a dopamine hit and feel happy. On the other hand if you study for your exam you will feel tired and potentially bored. So why study? Well, this is the idea of <strong>planning</strong>.</p>
<p>In particular, we can describe a <em>value function</em>:</p>
<blockquote><p><strong>Value Function</strong>: We don't just think about immediate rewards, but future rewards too. We want to assign some value to the current state that reflects the future too.</p>
</blockquote>
<p>Now, we can think of this in the reverse direction as well. Let's say you receive a reward; getting hired for your dream job. Now, if you look back to you career and things you did in school, what would you attribute your success to? What state of being in your past lead you to get your dream job today? This is refered to as the <em>credit assignment problem</em>:</p>
<blockquote><p><strong>Credit Assignment Problem</strong>: What did you do in the past that led to the reward you are receiving now? In other words, what action gets the credit.</p>
</blockquote>
<p>The credit assignment problem shows up in online advertising as well, but the concept is referred to as <strong>attribution</strong>. The idea is that if a user is shown the same ad 10 different times before they buy, which ad gets the credit?</p>
<p>Now, closely related to the credit assignment problem is the idea of <strong>delayed rewards</strong>. Note that these are all kind of just different ways of saying the same thing, and the solution is also the same. Delayed rewards is just looking at the problem from the other direction. With credit assignment, we are looking into the past and asking "what action lead to the reward we are getting now?". With delayed rewards, we are asking "How is the action I am taking now related to rewards I may potentially receive in the future?"</p>
<p>The idea of delayed rewards tells us that an AI must have the ability of foresight, or planning.</p>
<h2 id="3.1-Example-Scenario">3.1 Example Scenario<a class="anchor-link" href="#3.1-Example-Scenario">&#182;</a></h2><p><img src="images/scenario-1.png"></p>
<p>Imagine the following: you are in state A, which is the second last state in a game. There are only two possible next states, both of which are terminal states. B gives you a reward of 1, and C gives you a reward of 0. You have a 50% probability of going to either state; perhaps your agent doesn't know which one is best. We can think of the value of state B as 1, and the value of state C as 0. So, what is the value of state A? We can think of it as 0.5, since it is the expected value of your final reward, given that you have a 50% chance of ending up in either state:</p>
<p>$$Value(A) = 0.5*1 + 0.5*0 = 0.5$$</p>
<p>Now, lets say that you are in state A, and state A can only lead to state B, and there is no other possible next state:</p>
<p><img src="images/scenario-2.png"></p>
<p>If B gives you a reward of 1, then perhaps A's value should also be 1, since the only possible final scenario is to have a final reward of 1, once you reach A:</p>
<p>$$Value(A) = 1 * 1 = 1$$</p>
<p>Thus, the value tells us about the "future goodness" of a state. We can make this a little more formal; we actually call this value, the value function.</p>
<h2 id="3.2-Value-Function">3.2 Value Function<a class="anchor-link" href="#3.2-Value-Function">&#182;</a></h2><p>The value function is a measure of the future rewards that we may get:</p>
<blockquote><p><strong>V(s)</strong> = the value (taking into account the probability of all possible future rewards) of a state</p>
</blockquote>
<p>The name value, is not quite ideal, since it is very ambiguous. However, it is part of the RL nomenclature, so we will deal with it.</p>
<h3 id="3.2.1-Rewards-vs.-Values">3.2.1 Rewards vs. Values<a class="anchor-link" href="#3.2.1-Rewards-vs.-Values">&#182;</a></h3><p>It is easy to get rewards and values mixed up. The difference is that the value of state, is a measure of the possible future rewards we may get from being in that state. Rewards on the other hand are immediate.</p>
<p>We, therefore, chose actions to take based on <strong>values of states</strong>, and not on the reward we would get by going to a state! The reward is the main goal, but we can't use the reward to tell us how good a state is, since it doesn't tell us anything about future rewards.</p>
<h3 id="3.2.2-Efficiency">3.2.2 Efficiency<a class="anchor-link" href="#3.2.2-Efficiency">&#182;</a></h3><p>One way to think about the value function, is that it is a fast and efficient way to determine how good it is to be in a state, without needing to search the rest of the game tree. You could try to enumerate every possible state transition, and their probabilities of occuring; however, you can guess that this would be a computationally inefficient task. In fact, tic tac toe is easy since it is only a 3x3 board, so the number of states is approximately 3^(3x3) = 19683. However, that will grow exponentially with the size of the board! For example, if you have a connect 4 board, then you get 3^(4x4) = 43 million! As we know, exponential growth is never good, and hence searching the state space is only possible in the simplest of cases. Hence, a value function that can tell you how good you will do in the future, given only your current state, is <em>extremely</em> helpful. This means that $V(s)$ gives an answer instantly, in only $O(1)$ time! The only question is, is it even possible to find a value function...</p>
<h3 id="3.2.3-Value-Functions-in-RL">3.2.3 Value Functions in RL<a class="anchor-link" href="#3.2.3-Value-Functions-in-RL">&#182;</a></h3><p>Estimating the value function is a central task in RL, but it should be noted that not all RL algorithms require it. For instance, a genetic algorithm simply spawns offspring that have random genetic mutations, and the ones who survive the longest will go on to spawn offspring in the next generation. So, by pure evolution and natural selection, we can breed better and better agents that get iteratively better at the game! However, this is not the type of algorithm that we are interested in for RL, most of the time.</p>
<h3 id="3.2.4-Value-Function:-Math">3.2.4 Value Function: Math<a class="anchor-link" href="#3.2.4-Value-Function:-Math">&#182;</a></h3><p>So, we can dig into the math and notation surrounding the value function now. The value function takes in one parameter, the state, so we can denote it $V(s)$. It is the expected value of all future rewards, given that the current state is $s$:</p>
<p>$$V(s) = E[all \; future \; rewards \; | \; S(t) = s]$$</p>
<p>In other words, this is the average value of all possible future rewards, given that the current state is $s$.</p>
<h3 id="3.2.5-Finding-$V(s)$">3.2.5 Finding $V(s)$<a class="anchor-link" href="#3.2.5-Finding-$V(s)$">&#182;</a></h3><p>Let's now look at a generic algorithm that we can use to find the value function. For now, we are going to introduce some unrealistic constraints, to show that the problem can actually be solved. However, later on it will be clear that we don't need to do this. The algorithm we will use is an iterative one, and each time we run the loop we will get closer and closer to the true value function.</p>
<p>So, the first thing that we do is <strong>initialize the value function</strong>:</p>
<blockquote><ul>
<li>For all states where the agent wins, we say the value is 1. 
  $$V(s) = 1 \text{if s = winning state}$$ </li>
<li>For all states where the agent loses or it is a draw, we say the value is 0. 
  $$V(s) = 0 \text{if s = losing state}$$ </li>
<li>For all other states, we say the value is 0.5. 
  $$V(s) = 0.5 \text{if s = not winning or losing}$$ </li>
</ul>
</blockquote>
<p>When we study the "real" algorithms soon, we will not need to focus on such careful initialization. For this game, $V(s)$ can be interpreted as probability of winning after arriving in s (for this game only).</p>
<p><strong>Update Function</strong><br>
The update function looks simple, but there are some hidden details. It kind of looks like gradient descent:</p>
<p>$$V(s) \leftarrow V(s) + \alpha * (V(s') - V(s))$$</p>
<p>We take the value function at a state, $V(s)$, and we update it by adding the learning rate times $V(s')$ minus $V(s)$; here, $s$ is the current state, and $s'$ is the next state.</p>
<h3 id="3.2.6-Detail-1">3.2.6 Detail 1<a class="anchor-link" href="#3.2.6-Detail-1">&#182;</a></h3><p>The first detail here is that $s$ represents every state that we encounter in an episode. That means that for each iteration of the loop, we actually need to play the game and remember all of the states that we were in. We then loop through each of the states in the state history, and then update the value using the above equation. Notice that a terminal state value will never be updated, because to update a state you need the next state value, which does not exist if we are in a terminal state.</p>
<p>In Pseudocode it may look like:</p>

<pre><code>for t in range(max_iterations):
  state_history = play_game
  for (s, s') in state_history from end to start:
    V(s) = V(s) + learning_rate * (V(s') - V(s))</code></pre>
<h3 id="3.2.7-Detail-2">3.2.7 Detail 2<a class="anchor-link" href="#3.2.7-Detail-2">&#182;</a></h3><p>The next detail we need to think about is how do we actually play the game. Are we just going to take random actions? No! First, think about what taken random actions would lead to; it would result in a game tree having different probabilities than a game tree that was created by taking best actions. Second, remember that we have the value function! The value function tells us how good a state is, based on how good the future states will be. So, all we need to do is perform the action that leads to the next best state.</p>
<p>In pseudocode that may look like:</p>

<pre><code>maxV = 0
maxA = None
for a, s' in possible_next_states:
  if V(s') &gt; maxV:
    maxV = V(s')  // max value 
    maxA = a      // max action that lead to max value
perform action maxA</code></pre>
<h3 id="3.2.8-Problem-with-this">3.2.8 Problem with this<a class="anchor-link" href="#3.2.8-Problem-with-this">&#182;</a></h3><p>What is the main problem with this strategy? The problem is that the value function isn't accurate. If had the true value function, we wouldn't need to do any of this work in the first place. This problem is one that we have touched on before; the explore-exploit dilemma. In particular, by taking random actions we can learn about new states that we otherwise may never have gone to. By doing so, we can improve our estimate of the value function for those states.</p>
<p>However, in order to win, we need to take the action that yields the maximum value. So, we will be using the epsilon-greedy strategy to be making a tradeoff between exploration and exploitation.</p>
<h3 id="3.2.9-Intuition">3.2.9 Intuition<a class="anchor-link" href="#3.2.9-Intuition">&#182;</a></h3><p>Let's take a moment to gain some intuition about why this iterative update works. First, you should recognize this iterative update equation from before. It is reminiscent of the low-pass filter and average-value-finding equation we saw earlier, as well as gradient descent. V(s') is like the target value, and we want to move $V(s)$ towards that value. However, with RL there are multiple possible next states, so there are multiple s'. So, by doing this update equation multiple times, we are pulling V(s) in multiple different directions all at once. The idea is that, by playing infinitely many episodes, the proportion of time we spend in s' will approach the true next state probabilities.</p>
<h3 id="3.2.10-Extremely-Important-Detail">3.2.10 Extremely Important Detail<a class="anchor-link" href="#3.2.10-Extremely-Important-Detail">&#182;</a></h3><p>One incredibly important detail that is somewhat difficult to discern given only the update formula, is the order in which you have to update the values for any given state. We know that for any particular episode, we are only going to be updating the values for the states that were in that episode. But, the key of this equation is that we are moving $V(s)$ closer to $V(s')$. That mean that we are doing this update under the assumption that $V(s')$ is more accurate than $V(s)$. For the terminal state this is true, since that is always going to be 0 or 1, and will never change. But, if $V(s)$ and $V(s')$ are of the same accuracy, aka they are both not accurate at all, then making one closer to the other won't make anything better. So, the idea is we want to <strong>move backwards</strong> through the state history, because the current state value is always updated using the next state value. And we want the next state value to be more accurate, so we should update the state values in reverse order. This is also consistent with the terminal state being precisely accurate with a value of 0 or 1 that never changes.</p>
<p>$$V(terminal) = 0 \; or \; 1$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="4.-Tic-Tac-Toe-Implementation">4. Tic-Tac-Toe Implementation<a class="anchor-link" href="#4.-Tic-Tac-Toe-Implementation">&#182;</a></h1><p>Typically in ML we're implementing <em>procedural</em> algorithms. However, in RL we have multiple objects interacting, and we take an OOP approach.</p>
<p>At a high level, we will have two main objects: the <strong>agent</strong> and the <strong>environment</strong>. During any game, there will be two instances of the <strong>agent</strong>, and they will both interact with the same instance of environment.</p>
<p><img src="images/tic-tac-toe.png" width="500"></p>
<p>In order to make these things interact, we can create a function called <code>play_game()</code>, which accepts the agent for player 1, the agent for player 2, and the environment object. Inside of this function, it will essentially be a single loop. We will alternate between the two players, and at each iteration of the loop we need to do a few things.</p>
<ol>
<li>We need to make the current player perform an action which updates the environment. </li>
<li>We need to switch who the current player is, so that it alternates. </li>
<li>We need to check if the game is over, since that is when the loop will terminate. </li>
</ol>
<p>We can write this function now:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">play_game</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">draw</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="c1"># Loop until the game is over</span>
  <span class="n">current_player</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">env</span><span class="o">.</span><span class="n">game_over</span><span class="p">():</span>
    <span class="c1"># Alternate between players</span>
    <span class="n">current_player</span> <span class="o">=</span> <span class="n">p2</span> <span class="k">if</span> <span class="n">current_player</span> <span class="o">==</span> <span class="n">p1</span> <span class="k">else</span> <span class="n">p1</span>
    
    <span class="c1"># Draw the board before the user who wants to see it makes a move</span>
    <span class="n">condition1</span> <span class="o">=</span> <span class="n">draw</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">current_player</span> <span class="o">==</span> <span class="n">p1</span>
    <span class="n">condition2</span> <span class="o">=</span> <span class="n">draw</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">current_player</span> <span class="o">==</span> <span class="n">p2</span>
    <span class="k">if</span> <span class="n">draw</span> <span class="ow">and</span> <span class="p">(</span><span class="n">condition1</span> <span class="ow">or</span> <span class="n">condition2</span><span class="p">):</span>
      <span class="n">env</span><span class="o">.</span><span class="n">draw_board</span><span class="p">()</span>
      
    <span class="c1"># Current player makes move</span>
    <span class="n">current_player</span><span class="o">.</span><span class="n">take_action</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
    
    <span class="c1"># Update State histories</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
    <span class="n">p1</span><span class="o">.</span><span class="n">update_state_history</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">p2</span><span class="o">.</span><span class="n">update_state_history</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    
  <span class="k">if</span> <span class="n">draw</span><span class="p">:</span>
    <span class="n">env</span><span class="o">.</span><span class="n">draw_board</span><span class="p">()</span>

  <span class="c1"># Do the value function update</span>
  <span class="n">p1</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
  <span class="n">p2</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>  
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that above we have created a partial API for some instance methods that our objects will need.</p>
<ul>
<li>The environment will need a <code>game_over()</code> function, that returns a boolean. True if the game is over, false otherwise. You can guess that this function will need to scan the board to check if there is a winner, or if the board is full and it is a draw. </li>
<li>The agent will need a take action function, which accepts as a parameter the environment. This will update the environment, and hence the state. </li>
<li>Another function we need on the agent is an update function, that updates the agents internal estimate of the value function. This is where the update equation that we have talked about is going to go. The update function will have to accept the environment as well, since it will have to query the most current reward from the environment. </li>
<li>Another function that will be very useful for us, but not entirely necessary, is a draw board function. This will display what positions are currently occupied, and by what pieces. </li>
</ul>
<h3 id="Representing-States">Representing States<a class="anchor-link" href="#Representing-States">&#182;</a></h3><p>The next thing that we need to consider is how we will be representing states. We talked about this earlier and know that this is going to be a $O(1)$ lookup. One way to accomplish this will be to use a dictionary, since the keys to a dictionary can be any immutable object. So, we could convert the game board to a 2-d tuple (size 3x3) and store the values inside as tuple elements.</p>
<p>However, we can also do this so that each state maps to a number, and in doing so we can store this as a numpy array.</p>
<p><strong>Mapping state to a number</strong>:
We talked earlier about how to calculate the upper limit on the number of states earlier. We calculated it to be 3^9 = 19683, which is not too big of an array to store in memory. Notice, this also encodes impossible states, such as an x in all 9 board locations, when in fact you only need 3 in a row to win.</p>
<p>The idea of having 9 locations, and 3 possible values per location should feel similar to the idea of binary numbers. $N$ bits can store $2^N$ different integers, and we can enumerate all of those possibilities by finding all of the permutations of a string of length $N$, where each location can only contain 0 or 1. The equation to convert a binary number to a decimal number is:</p>
<p>$$D = 2^{N-1}b_{N-1} + ... + 2^1 b_1 + 2^0 b_0$$</p>
<p>And in our case it is:</p>
<p>$$D = 3^{N-1}b_{N-1} + ... + 3^1 b_1 + 3^0 b_0$$</p>
<p>In pseudocode we may end up with something like this:</p>

<pre><code>k = 0 
h = 0
for i in range(LENGTH):
  for j in range(LENGTH):
    if self.board[i,j] == 0:
      v = 0 
    elif self.board[i,j] == self.x:
      v = 1
    elif self.board[i,j] == self.o:
      v = 2
    h += (3**k) * v
    k += 1 
return h</code></pre>
<p>Keep in mind the following: The entire reason for creating a the above function/pseudocode is to be able to take what was a 3x3 board (array) and map it to a single number. Note, this number represents the board perfectly and contains all of the information necessary to rebuilt the board.</p>
<h3 id="Initialize-the-value-function">Initialize the value function<a class="anchor-link" href="#Initialize-the-value-function">&#182;</a></h3><p>We can now talk about how we will be initializing the value function properly. So, the initialization that we need is:</p>
<blockquote><p>$$\text{1 if s == winning terminal state}$$
  $$\text{0 if s == losing or draw terminal state}$$ 
  $$\text{0.5 otherwise}$$</p>
</blockquote>
<p>In order to do this, we need to enumerate every possible state, and assign them values. Here is a question to think about; is it better to create a game tree, where you enumerate every possible sequence of moves, or is it better to permute all possible settings of all possible positions on the board, even though some of those represent impossible game states?</p>
<p>The problem with the game tree is that we end up with many redundant states. For instance, consider the case where x goes in the top middle, and then o goes in the center. We can reach this same state if o goes in the center, and then x goes in the top middle.</p>
<p><img src="images/tic-tac-toe-board.png" width="500"></p>
<p>If we take a second to think about how many choices this will lead to we can see that at the root we have 9 different choices, and then at the layer below we have 8 different choices, and then 7 and so on. Hence the total number of states that we will see is $9!$ which is equivalent to 326,880, and is much greater than $3^9$. So, the algorthms that we want to use to generate the states is definitely generating all permutations.</p>
<p>How do we go about generating these permutations? Well, this is naturally a recursive problem, since a binary number of length $N$ can start with either 0 or 1, and then we can use all of the permutations for a binary number of length $N-1$, and append it to that. In pseudocode that may look like:</p>

<pre><code>def generate_all_binary_numbers(N):
  results = []
  child_results = generate_all_binary_numbers(N-1)
  for prefix in ('0','1'):
    for suffice in child_results:
      new_result = prefix + suffix 
      results.append(new_result)
  return results</code></pre>
<p>Note that above the base case is not shown for simplicity. Okay, now that was just an example for doing permutations in general. We are going to have a recursive function called <code>get_state_hash_and_winner</code> and this is going to return a list of triples. Each triple is going to contain the state as a hash integer, who the winner is if there is one, and whether or not this state represents a finished game. The 3 arguments for this function are the environment, which we really just need for the board matrix, and the i and j coordinates to place the next value, which can be 0, x, or o.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_state_hash_and_winner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Recursive function that will return all possible states (as ints) and who the</span>
<span class="sd">  corresponding winner is for those sates (if any). (i,j) refers to the next cell </span>
<span class="sd">  on the board to permute (we need to try -1, 0, 1). Impossible games are ignored;</span>
<span class="sd">  i.e. 3x&#39;s and 3o&#39;s in a row simultaneously.&quot;&quot;&quot;</span>
  <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">o</span><span class="p">):</span>
    <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span> <span class="c1"># if empty board, it should already be 0 </span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span> 
      <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Base Case: i = 2, j = 2, filling last board location, collect results and return</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
        <span class="n">ended</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">game_over</span><span class="p">(</span><span class="n">force_recalculate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">winner</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">winner</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">winner</span><span class="p">,</span> <span class="n">ended</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span> 
        <span class="c1"># Recursive Call 1, at end of row: Want to go to next row, first column (j=0, i++)</span>
        <span class="n">results</span> <span class="o">+=</span> <span class="n">get_state_hash_and_winner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Recursive Call 2, not at end of row: Increment j, leave i the same </span>
      <span class="n">results</span> <span class="o">+=</span> <span class="n">get_state_hash_and_winner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">results</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We should also recognize that the value function for the two players won't be equivalent. One player has to be x and the other player has to be o. So, if the value function is 1 for x, it is going to be 0 for o, and vice versa. However, in the case of the draw both players do get 0. So, we will have two functions which are almost the reverse of eachother;= <code>initialV_x</code> and <code>initialV_o</code>. Note, these functions could most definitely be composed in a more efficient way, but we will leave them separate for clarity purposes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">initialV_x</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">state_winner_triples</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Initialize state values as follows: </span>
<span class="sd">  * if x wins, V(s) = 1</span>
<span class="sd">  * if x loses or draw, V(s) = 0</span>
<span class="sd">  * otherwise, V(s) = 0.5&quot;&quot;&quot;</span>
  <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">num_states</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">winner</span><span class="p">,</span> <span class="n">ended</span> <span class="ow">in</span> <span class="n">state_winner_triples</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ended</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">winner</span> <span class="o">==</span> <span class="n">env</span><span class="o">.</span><span class="n">x</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span> 
      <span class="n">v</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
  <span class="k">return</span> <span class="n">V</span>

<span class="k">def</span> <span class="nf">initialV_o</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">state_winner_triples</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Almost exact opposite of initialV_x: </span>
<span class="sd">  * if o wins, V(s) = 1</span>
<span class="sd">  * if o loses or draw, V(s) = 0</span>
<span class="sd">  * otherwise, V(s) = 0.5&quot;&quot;&quot;</span>
  <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">num_states</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">winner</span><span class="p">,</span> <span class="n">ended</span> <span class="ow">in</span> <span class="n">state_winner_triples</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ended</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">winner</span> <span class="o">==</span> <span class="n">env</span><span class="o">.</span><span class="n">o</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span> 
      <span class="n">v</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
  <span class="k">return</span> <span class="n">V</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Recap">Recap<a class="anchor-link" href="#Recap">&#182;</a></h3><p>So, above, we just went over the following:</p>
<ul>
<li>In order to initialize V(s) to our desired values, we must enumerate all the states<ul>
<li>There were two ways of doing this: either searching the game tree (9!) or finding board permutations ($3^9$)</li>
</ul>
</li>
<li>We then coded out the recursive function that we are going to use in order to enumerate all of the states. </li>
</ul>
<p>We are slowly putting the pieces together for our tic tac toe game!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Environment-Class">The Environment Class<a class="anchor-link" href="#The-Environment-Class">&#182;</a></h1><p>Recall that the environment is the thing that our agents are going to interact with. Lets take a look at the methods that we were going to look at.</p>
<blockquote><ul>
<li><code>__init__()</code>: The constructor, where we initialize important instance variables</li>
<li><code>is_empty(i, j)</code>: returns true if location (i, j) is empty</li>
<li><code>reward(symbol)</code>: returns reward when we pass a symbol representing specific player</li>
<li><code>get_state()</code>: looks at state of the board and returns a number</li>
<li><code>game_over()</code>: looks at state of game and returns true or false. Will take a <code>force_recalculate</code> argument. This is because <code>game_over</code> is called multiple times in our script, and if a game has ended already, we don't want to have to do this complicated operation again and again-so we use the fact that it has alreaday ended as a shortcut. But, in our earlier function <code>get_state_hash_and_winner</code>, we recursively filled out each location on the board. This involves placing pieces on the board, taking them off, placing other pieces on, and so on. This means we will be going back and forth between a game over state and a non game over state, so we are not able to use that short cut. <code>force_recalculate</code> allows us to make sure the entire operation happens every time. </li>
<li><code>draw_board()</code>: Draws tic-tac-toe board</li>
</ul>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Represents a tic-tac-toe game.&quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">board</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">LENGTH</span><span class="p">,</span> <span class="n">LENGTH</span><span class="p">))</span> <span class="c1"># intialize as 0, our empty symbol</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># represents an x on the board, player 1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># represents an o on the board, player 2 </span>
    <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ended</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_states</span> <span class="o">=</span> <span class="mi">3</span><span class="o">**</span><span class="p">(</span><span class="n">LENGTH</span><span class="o">*</span><span class="n">LENGTH</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">is_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span>
  
  <span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sym</span><span class="p">):</span>
    <span class="c1"># no reward until game is over</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">game_over</span><span class="p">():</span>
      <span class="k">return</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">==</span> <span class="n">sym</span> <span class="k">else</span> <span class="mi">0</span>
  
  <span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the current state, represented as an int from 0...|S|-1,</span>
<span class="sd">    where S = set of all possible states. |S| = 3^(BOARD SIZE), since </span>
<span class="sd">    each cell can have 3 possible values - empty, x, o - some states are </span>
<span class="sd">    not possible, e.g. all cells are x, but we can ignore that detail.</span>
<span class="sd">    This is like finind the integer represented by a base 3 number.&quot;&quot;&quot;</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">:</span>
          <span class="n">v</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">:</span>
          <span class="n">v</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="n">h</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">3</span><span class="o">**</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">h</span>
  
  <span class="k">def</span> <span class="nf">game_over</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">force_recalculate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns true if game over (a player has won or it&#39;s a draw), </span>
<span class="sd">    otherwise returns false. Also, sets &#39;winner&#39; instance variable </span>
<span class="sd">    and &#39;ended&#39; instance variable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">force_recalculate</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ended</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ended</span>
    
    <span class="c1"># -&gt; Check if game over </span>
    <span class="c1"># check rows</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">player</span> <span class="o">*</span> <span class="n">LENGTH</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="n">player</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="k">return</span> <span class="kc">True</span>
    
    <span class="c1"># check columns</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="n">player</span> <span class="o">*</span> <span class="n">LENGTH</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="n">player</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
          <span class="k">return</span> <span class="kc">True</span>
        
    <span class="c1"># check diagonals - use trace (sum of all elements in a matrix along main diagonal)</span>
    <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">):</span>
      <span class="c1"># top-left -&gt; bottom-right diagonal</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="o">.</span><span class="n">trace</span><span class="p">()</span> <span class="o">==</span> <span class="n">player</span> <span class="o">*</span> <span class="n">LENGTH</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="n">player</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">True</span>
      <span class="c1"># top-right -&gt; bottom-left diagonal (flip matrix, then trace)</span>
      <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">)</span><span class="o">.</span><span class="n">trace</span><span class="p">()</span> <span class="o">==</span> <span class="n">player</span><span class="o">*</span><span class="n">LENGTH</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="n">player</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">True</span>
    
    <span class="c1"># check if draw -&gt; are all elements on board simultaneously non-zero</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">board</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">ended</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">return</span> <span class="kc">True</span>
    
    <span class="c1"># game is not over</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">winner</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="kc">False</span>
  
  <span class="k">def</span> <span class="nf">draw_board</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">o</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;o &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------&quot;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Agent-Class">The Agent Class<a class="anchor-link" href="#The-Agent-Class">&#182;</a></h1><p>The next major class in our tic tac toe game will be the agent. So far, we know that this is the AI, or the thing that contains the AI. This is different from regular machine learning, because in this situation we are not just feeding it data. Instead, our agent needs to interact with the environment. A one-line update for $V(s)$ is a very small part of our agent, and yet it is responsible for 100% of its intelligence.</p>
<p>We can begin by looking at an overview of our instance methods:</p>
<blockquote><ul>
<li><code>__init__()</code>: </li>
<li><code>setV(V)</code>: Allows us to initialize the value function. Recall our value function is going to be initialized in a very specific way. </li>
<li><code>set_symbol(symbol)</code>: Allows us to give the agent a symbol that it will use when making move. </li>
<li><code>set_verbose(b)</code>: Prints more information if verbose is set to true. </li>
<li><code>reset_history()</code>: Need this because we want to keep track of state history for every episode. However, once the episode is finished and we are done learning, we don't need that state history anymore and we can start a new episode. </li>
<li><code>take_action(env)</code>: Takes in the environment as an input. Checks the board for valid moves, and will make moves based on a strategy. Recall, we are using the epsilon greedy strategy for this game. </li>
<li><code>update_state_history(s)</code>: Adds a state to the state history. Updated whenever any player takes a turn. </li>
<li><code>update(env)</code>: Update function. Queries environment for latest reward, which corresponds to the end of an episode, because we are only going to call update at the end of an episode. This is where all of the learning will happen. </li>
</ul>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span> <span class="c1"># probability of choosing random action instead of greedy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="c1"># learning rate</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state_history</span> <span class="o">=</span> <span class="p">[]</span>
    
  <span class="k">def</span> <span class="nf">setV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">V</span>
    
  <span class="k">def</span> <span class="nf">set_symbol</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sym</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sym</span> <span class="o">=</span> <span class="n">sym</span>
    
  <span class="k">def</span> <span class="nf">set_verbose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># if true, will print values for each position on the board</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">v</span>
    
  <span class="k">def</span> <span class="nf">reset_history</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state_history</span> <span class="o">=</span> <span class="p">[]</span>
    
  <span class="k">def</span> <span class="nf">take_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="c1"># choose an action based on epsilon-greedy strategy </span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
    <span class="n">best_state</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">:</span>
      <span class="c1"># take random action </span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taking a random action.&quot;</span><span class="p">)</span>
      <span class="n">possible_moves</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">is_empty</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span> <span class="c1"># find all empty positions on board</span>
            <span class="n">possible_moves</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">))</span>
      <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">possible_moves</span><span class="p">))</span> <span class="c1"># select random move from empty positions</span>
      <span class="n">next_move</span> <span class="o">=</span> <span class="n">possible_moves</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> 
    
    <span class="k">else</span><span class="p">:</span> 
      <span class="c1"># Greedy portion. Choose the best action based on current values of states. </span>
      <span class="c1"># Loop through all possible moves, get their values. Keep track of best value. </span>
      <span class="n">pos2value</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># for debugging</span>
      <span class="n">next_move</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">best_value</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> 
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">is_empty</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
            <span class="c1"># what is the state if we made this move?</span>
            <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sym</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
            <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># don&#39;t forget to change it back!</span>
            <span class="n">pos2value</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">best_value</span><span class="p">:</span>
              <span class="n">best_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
              <span class="n">best_state</span> <span class="o">=</span> <span class="n">state</span>
              <span class="n">next_move</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
      
      <span class="c1"># if verbose, draw the board w/ the values</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taking a greedy action&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------&quot;</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LENGTH</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">is_empty</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
              <span class="c1"># print the value</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; </span><span class="si">%.2f</span><span class="s2">|&quot;</span> <span class="o">%</span> <span class="n">pos2value</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
              <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">env</span><span class="o">.</span><span class="n">x</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x  |&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
              <span class="k">elif</span> <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">env</span><span class="o">.</span><span class="n">o</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;o  |&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
              <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   |&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------&quot;</span><span class="p">)</span>  
    
    <span class="c1"># make the move</span>
    <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">next_move</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_move</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sym</span>
    
  <span class="k">def</span> <span class="nf">update_state_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Cannot put this in take_action, because take_action only happens once every </span>
<span class="sd">    other iteration for each player. State history needs to be updated every iteration.</span>
<span class="sd">    s = env.get_state(), don&#39;t want to do this twice, so pass it in.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Contains the &#39;AI&#39;. Only want to do this at the end of an episode. We want to </span>
<span class="sd">    BACKTRACK over the states, so that: </span>
<span class="sd">    -&gt; V(prev_state) = V(prev_state) + alpha * (V(next_state) - V(prev_state))</span>
<span class="sd">    -&gt; where V(next_state) = reward if it&#39;s the most current state </span>
<span class="sd">    NOTE: we ONLY do this at the end of an episode. Also, we can see that the first </span>
<span class="sd">    target is exactly equal to the final reward. But after that the targets are all </span>
<span class="sd">    just Value estimates. The hope is that our value estimates will converge over time.&quot;&quot;&quot;</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sym</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span> 
    <span class="k">for</span> <span class="n">prev</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_history</span><span class="p">):</span>
      <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">prev</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">prev</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">prev</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reset_history</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Remaining-functionality">Remaining functionality<a class="anchor-link" href="#Remaining-functionality">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Human</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Human class, created so that we can play against the AI. We will set verbose to true</span>
<span class="sd">  so that we can see the value of each position every time the AI takes turn.&quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>
  
  <span class="k">def</span> <span class="nf">set_symbol</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sym</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sym</span> <span class="o">=</span> <span class="n">sym</span>
    
  <span class="k">def</span> <span class="nf">take_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
      <span class="c1"># Break if we make a legal move</span>
      <span class="n">move</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Enter coordinates i,j for your next move (i,j=0..2): &quot;</span><span class="p">)</span>
      <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">move</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
      <span class="n">i</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
      <span class="n">j</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">is_empty</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">env</span><span class="o">.</span><span class="n">board</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sym</span>
        <span class="k">break</span>
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="s2">&quot;Place holder.&quot;</span>
    <span class="k">pass</span>
  
  <span class="k">def</span> <span class="nf">update_state_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="s2">&quot;Place holder.&quot;</span>
    <span class="k">pass</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">LENGTH</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="c1"># train the agent</span>
  <span class="n">p1</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">()</span>
  <span class="n">p2</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">()</span>

  <span class="c1"># set initial V for p1 and p2</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">()</span>
  <span class="n">state_winner_triples</span> <span class="o">=</span> <span class="n">get_state_hash_and_winner</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>


  <span class="n">Vx</span> <span class="o">=</span> <span class="n">initialV_x</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">state_winner_triples</span><span class="p">)</span>
  <span class="n">p1</span><span class="o">.</span><span class="n">setV</span><span class="p">(</span><span class="n">Vx</span><span class="p">)</span>
  <span class="n">Vo</span> <span class="o">=</span> <span class="n">initialV_o</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">state_winner_triples</span><span class="p">)</span>
  <span class="n">p2</span><span class="o">.</span><span class="n">setV</span><span class="p">(</span><span class="n">Vo</span><span class="p">)</span>

  <span class="c1"># give each player their symbol</span>
  <span class="n">p1</span><span class="o">.</span><span class="n">set_symbol</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
  <span class="n">p2</span><span class="o">.</span><span class="n">set_symbol</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">o</span><span class="p">)</span>

  <span class="n">T</span> <span class="o">=</span> <span class="mi">10000</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">play_game</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">Environment</span><span class="p">())</span>

  <span class="c1"># play human vs. agent</span>
  <span class="c1"># do you think the agent learned to play the game well?</span>
  <span class="n">human</span> <span class="o">=</span> <span class="n">Human</span><span class="p">()</span>
  <span class="n">human</span><span class="o">.</span><span class="n">set_symbol</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">o</span><span class="p">)</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">p1</span><span class="o">.</span><span class="n">set_verbose</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">play_game</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">human</span><span class="p">,</span> <span class="n">Environment</span><span class="p">(),</span> <span class="n">draw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># I made the agent player 1 because I wanted to see if it would</span>
    <span class="c1"># select the center as its starting move. If you want the agent</span>
    <span class="c1"># to go second you can switch the human and AI.</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Play again? [Y/n]: &quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">answer</span> <span class="ow">and</span> <span class="n">answer</span><span class="o">.</span><span class="n">lower</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span>
      <span class="k">break</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Notes-on-Learning/Value-Function">Notes on Learning/Value Function<a class="anchor-link" href="#Notes-on-Learning/Value-Function">&#182;</a></h1><p>At this point you should have a good high level overview of the reinforcement learning process used in order to train our agent to perform optimally at tic tac toe. However, there may still be some haze surrounding the exact process-this is okay, considering there was a lot going on above that didn't even have to do with our agent. The actual AI portion was kept to only a few lines of code, with the rest being the equivalent of a first year CS coding assignment (using object oriented programming in order to build an environment, human, agent, etc).</p>
<p>Let's walk through the diagram below to ensure that we have a complete understanding of how our agent is learning over time.</p>
<p><img src="images/ai-update.png"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above we can see that the process essentially works as follows:</p>
<blockquote><ol>
<li>We initialize our value function. We must be aware in this case that our value function is not defined in the "traditional" sense; in other words, we don't technically have a view of its internals. We simply have the final mapping of input (<em>state</em>) to output (<em>value</em>). Mathematically that just looks like:
$$V(s) = value$$
We should be aware that this means that our value function will be an array. The index of the array will be the <em>state</em> (recall that our state is represented as a number, determined via a mapping from the board position). We can then access the value of a certain state like so: <code>V[14021] = some value</code></li>
<li>We then enter the iterative process of <em>training</em>. Above that is set to be 10,000 iterations. In this process, we begin by playing a game, which we refer to as an <em>episode</em>. Here, when it is our agents turn, the value function is used to decide where to place the game piece! In other words, if the board is fully empty, then we have 9 possible options as to where we should place our piece. The agent will check the value for each of those 9 states (using our value function), and then choose the arg max (they all will be initialized to 0.5 since they are not winning or losing states). <br>
<br>
However, if we always simply choose the best action we may converge too quickly, and miss exploring certain parts of the value function state space. To prevent this, an epsilon greedy approach is used! In other words, a small percentage of the time, instead of choosing the best action based on the value function, we will choose a random action to take. </li>
<li>At the end of each episode, we will take the state history for that particular episode and use it to update our value function. An example is shown in the diagram above. We see that in this episode, our agent won by placing 3 x's in a row diagonally. Based on how we initialized our value function, the last state (a winning state) has a value of 1:
$$V(s) = 1$$
We can then update the value of our 2nd last state, as follows: we know that it originally was initialized to be 0.5 (since it was not a winning or losing state), so $V(s) = 0.5$. We have an $\alpha$ of 0.5, and our previous state (remember, they are reversed in this process) was the winning state-hence $V(s') = 1$. So our update equation looks like:
$$V(s) \leftarrow V(s) + \alpha * (V(s') - V(s))$$
<br>
$$V(s) \leftarrow 0.5 + 0.5 * (1 - 0.5) = 0.75$$
<br>
We will continue doing this for the entire state history associated with this episode. So, for the 3rd last state we have:
$$V(s) \leftarrow 0.5 + 0.5 * (0.75 - 0.5) = 0.625$$
<br></li>
<li>We can see that for each episode, we will iteratively update our value function and over time it should approach the <em>true</em> value function. It is also clear why using an explore-exploit approach is beneficial; if we failed to do so, the state spaces that we update first as the only ones that would ever be chosen (since all others would remain at 0.5!)</li>
</ol>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="RL-vs.-Supervised-Learning">RL vs. Supervised Learning<a class="anchor-link" href="#RL-vs.-Supervised-Learning">&#182;</a></h1><p>A common question that comes up is why not use a supervised learning model that maps states to actions directly? Well first and foremost, how would we come up with labels? The state space would grow quickly and become too large to enumerate; tic-tac-toe/connect-4 is probably the limit. Now imagine we are playing a video game with 60 FPS, where there are millions of pixels, and hundreds of values per pixel; the number of states in a game like this is so large it is hard to conceptualize. There is also the fact that labels only tell us if we are right or wrong; a reward is more flexible and gives us information because it tells us how right or how wrong. Rewards tell us how good we are doing. Rewards are delayed, but the value function has this information built in.</p>

</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
