
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Projects.html">Projects</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Bayes-Rule">2. Bayes Rule<a class="anchor-link" href="#2.-Bayes-Rule">&#182;</a></h1><p>The main goal of this post is to dig a bit further into Bayes rule, from a purely probabilistic perspective! Before we begin I do want to make one note; a great deal of the power of Bayes Rule comes in the form of bayesian inference and bayesian statistics, which can be found in the statistics section. I would recommend reading both of those posts as well if you are interested, since they demonstrate the application of Bayes rule to real world problems. If you have caught the bayesian bug at that point then I recommend reading my posts on Bayesian AB testing, found in the Machine Learning section.</p>
<p>One more thing to note: I am going to hold of on explaining the importance of Bayes Rule until the end, and its many use cases will in reality be spread throughout the aformentioned posts. Just another reason to go through them all. With that out of the way, let's begin!</p>
<h2 id="1.1-Mathematical-Definition">1.1 Mathematical Definition<a class="anchor-link" href="#1.1-Mathematical-Definition">&#182;</a></h2><p>We worked with Bayes Rule briefly in the probability introduction, but just to recap, it can be derived as follows:</p>
<p>We know that the below statement represents the conditional probability of $A$ given $B$:</p>
<p>$$p(A \mid B)=\frac{p(A,B)}{p(B)}$$</p>
<p>And we also know that the opposite is also true:</p>
<p>$$p(B \mid A)=\frac{p(B,A)}{p(A)}$$</p>
<p>And since:</p>
<p>$$p(A,B)=p(B,A)$$</p>
<p>We can write:</p>
<p>$$p(A \mid B)=\frac{p(B \mid A)*p(A)}{p(B)}$$</p>
<p>Now, often times we may not have $p(B)$ directly, but this is just the marginal distribution of the joint probability $p(A,B)$, summed over all $p(A)$. It looks like:</p>
<p>$$p(B)=\sum_ip(A_i,B) = \sum_ip(B \mid A_i)*p(A_i)$$</p>
<p>If we are working with continuous distributions, sum turns into an integral.</p>
<p>Another way to think of this, is that the term on the bottom is just a normalization constant (Z) to ensure that the distribution sums to one.</p>
<p>$$p(A \mid B)=\frac{p(B \mid A)*p(A)}{Z}$$</p>
<p>Another way of saying this, is that they are proportional:</p>
<p>$$p(A \mid B)\propto p(B \mid A)*p(A)$$</p>
<p>Now this is a very powerful fact! Because the denominator ($p(B)$) does not depend on $A$, if we are simply trying to find the value of $A$ that maximizes the conditional probability of $p(A \mid B)$, we can ignore the denominator! In other words, this is used when we are trying to find the argmax of a distribution:</p>
<p>$$argmax_Ap(A \mid B)$$</p>
<p>So, we don't need to know the actual value of the probability, just the particular A that gives us the maximum probability. Because Z is independent of A:</p>
<p>$$argmax_Ap(A \mid B) = argmax_Ap(B \mid A)p(A)$$</p>
<p>This leads us into one of the main uses for Bayes Rule.</p>
<h2 id="1.2-Bayes-Rule-for-Classification">1.2 Bayes Rule for Classification<a class="anchor-link" href="#1.2-Bayes-Rule-for-Classification">&#182;</a></h2><p>In the context of the Bayes Classifier, $y$ represents the class, and $x$ represents the data.</p>
<p>$$p(y \mid x)=\frac{p(x \mid y)*p(y)}{p(x)}$$</p>
<p>We refer to $p(x \mid y)$ as the <strong>generative distribution</strong>, because it tells us what the features look like for a specific class y, which we are already given.</p>
<p>Note, that while the bayes classifier does make use of bayes rule, it does NOT necessarily make use of bayesian statistics. For more information on exactly what that means please see the posts on Bayesian Statistics. Again, the purpose of this post is really to just demonstrate it's role when purely confined to basic probability problems.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Examples">2. Examples<a class="anchor-link" href="#2.-Examples">&#182;</a></h2><h3 id="2.1-The-Monty-Hall-Problem">2.1 The Monty Hall Problem<a class="anchor-link" href="#2.1-The-Monty-Hall-Problem">&#182;</a></h3><p>We are now going to go over a few brief examples where Bayes Rule can be applied in a simple proabilistic setting. First we can start with a very famous problem in probability know as <strong>The Monty Hall Problem</strong>. Imagine you are on a game show and you have to pick a door. There are 3 doors, and behind 1 of the doors there is a car, and behind the other two doors there are goats. Here is how the game works:</p>
<ol>
<li>You pick a door (you do not get to see what is behind it) (door 1)</li>
<li>Monty Hall opens a door you didn't pick, always reveals a goat (door 2)</li>
<li>You are given a choice: stay with door 1, or switch to (door 3)</li>
</ol>
<p>The big question is, which door should you choose?</p>
<h4 id="2.1.1-Which-door-should-you-chose">2.1.1 Which door should you chose<a class="anchor-link" href="#2.1.1-Which-door-should-you-chose">&#182;</a></h4><p>So, remember, you choose door 1, and each probability is conditioned on this. We then define the following:</p>
<p>$$ C = \text{where the car really is}$$</p>
<p>$$ p(C=1) = p(C=2) = p(C=3) = 1/3$$</p>
<p>For example, $p(C=1)$ represents the probability that a car is behind door 1. We can then define the random variable $H$:</p>
<p>$$ H = \text{random variable to represent the door that Monty Hall opens}$$</p>
<p>We can assume he opens door 2 without loss of generality, since the problem is symmetric.</p>
<p>$$p(H=2 \mid C=1) = 0.5$$</p>
<p>Remember that you chose door 1. So if the car is behind door 1, he can choose either door 2 or 3 since they will each be a goat. If the car is behind door 2, he cannot open door 2, so the probability is 0:</p>
<p>$$ p(H=2 \mid C=2) = 0$$</p>
<p>Similarly, if the car is behind door 3, then monty hall has to open door 2, since that is the only door left with a goat:</p>
<p>$$p(H=2 \mid C=3) = 1$$</p>
<p>Now, What probability do we actually want? We want to know if we should stick with door 1 or switch to door 3. In other words we want to compare:</p>
<p>$$p(C=1 \mid H=2) \text{ vs. } p(C=3 \mid H=2)$$</p>
<p>Now, we can do that using bayes rule!</p>
<p>$$p(A \mid B)=\frac{p(B \mid A)*p(A)}{p(B)}$$</p>
<p>$$p(A \mid B)=\frac{p(B \mid A)*p(A)}{\sum_ip(B \mid A_i)*p(A_i)}$$</p>
<p>Where in our case:</p>
<p>$$A: C=3 \;, B: H=2$$</p>
<p>$$p(C=3 \mid H=2) = \frac{p(H=2 \mid C=3)p(C=3)}{p(H=2)}$$</p>
<p>$$p(C=3 \mid H=2) = \frac{p(H=2 \mid C=3)p(C=3)}{p(H=2 \mid C=1)p(C=1)+p(H=2 \mid C=2)p(C=2)+p(H=2 \mid C=3)p(C=3)}$$</p>
<p>$$p(C=3 \mid H=2) = \frac{\frac{1}{3}}{\frac{1}{2}*\frac{1}{3}+0*\frac{1}{3}+1*\frac{1}{3}} = \frac{2}{3}$$</p>
<p>And we can similarly show:</p>
<p>$$p(C=1 \mid H=2) = \frac{1}{3}$$</p>
<p>Hence, by the above application of Bayes Rule it is clear that we should always switch doors!</p>
<h4 id="2.1.2-Mathematical-Intuition">2.1.2 Mathematical Intuition<a class="anchor-link" href="#2.1.2-Mathematical-Intuition">&#182;</a></h4><p>We can also think about the problem like so:</p>
<p>$$ p(C=1) = 1/3 $$</p>
<p>$$ p(C=2) = 1/3$$</p>
<p>$$ p(C=3) = 1/3$$</p>
<p>$$ p(C=2 \text{ or } C=3) = 2/3$$</p>
<p>Now lets say that we pick door 1, and monty hall opens door 2, showing us there is a goat behind it. We now know that $p(C=2) = 0$. In other words, monty has <strong>revealed certain information to us</strong> that we did not have originally.  Hence, our equation $p(C=2 \text{ or } C=3) = 2/3$ still remains true, which means that $p(C=3) = 2/3$ and $p(C=1) = 1/3$. So we want to pick door 3! Note the reason this happens is because once door 2 is opened, it is known and is no longer a random variable.</p>
<h4 id="2.1.3-Advanced-Intuition">2.1.3 Advanced Intuition<a class="anchor-link" href="#2.1.3-Advanced-Intuition">&#182;</a></h4><p>Now, this problem is often referred to as a <strong>paradox</strong>. The reason it is viewed as a paradox is because it violates general human intuition and common sense. Now, this section will touch on some more advanced topics such as <strong>causal analysis</strong> (which will be covered in later posts), but I would feel remiss if I did not add a few sentences on the topic.</p>
<p>In general, human intuition operates under the logic of <strong>causation</strong>, while data conform to the logic of probabilties and proportions. Paradoxes often arise when we misapply the rules we have learned in one realm to another. In the case the Monty Hall problem, the main thing needed to resolve this apparent paradox is that we must take into account <em>not only the data</em>, but also the <em>data generating process</em> (the rules of the game). The main idea is as follows:</p>
<blockquote><p>The way that we obtain information is no less important than the information itself.</p>
</blockquote>
<p>Based on the rules of the game, we can deduce the following: If we open door 1, Monty cannot open door 1. However, he could have opened door 2. If, instead he choses to open door 3, it is more likely that he opened door 3 because he was forced to. This leads us to see that their is more evidence than before that the car is behind door 2.</p>
<p>If we start wading into the waters of causation, we learn that our minds rebel at the possibility of a correlation without a causation, since we learned to associate the two since birth. Causeless correlation violates our common sense.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.2-Imbalanced-Classes">2.2 Imbalanced Classes<a class="anchor-link" href="#2.2-Imbalanced-Classes">&#182;</a></h3><p>Lets look at another example of where Bayes rule comes into play. Suppose we are doing disease testing. We would take a blood sample, extract some features from it, and output whether or not that person has the disease. So, we would have:</p>
<ul>
<li>Input: blood sample</li>
<li>Output: Has disease, yes/no</li>
</ul>
<p>Lets look further at a realistic scenario where this is involved. Most people are healthy and non diseased, most of the time. So, suppose that only 1% of the population has the disease. We can build a classifier that just predicts "no" each time. In other words, it doesn't learn anything. It is already correct for 99% of cases though! Hence, accuracy is not always the best metric to utilize. Perhaps we do not care about overall accuracy?</p>
<h4 id="2.2.1-So-what-should-we-measure?">2.2.1 So what should we measure?<a class="anchor-link" href="#2.2.1-So-what-should-we-measure?">&#182;</a></h4><p>What we actually want to measure is $p(predict=1 | disease=1)$. This is called the <strong>true positive rate</strong>. In medical terminology this is referred to as <strong>sensitivity</strong>. In information retrieval is is known as <strong>hit rate</strong> or <strong>recall</strong>.</p>
<p>We can solve for the above using bayes rule:</p>
<p>$$p(prediction=1 | disease=1) = \frac{p(prediction=1, disease=1)}{p(disease=1)}$$</p>
<p>Typically, we count 4 things:</p>
<ol>
<li><strong>true positives</strong> (you have the disease, and we predict you have the disease)</li>
<li><strong>true negatives</strong> (you don't have the disease, and we predict you dont' have the disease)</li>
<li><strong>false positives</strong> (you don't have the disease, and we predict you have the disease)</li>
<li><strong>false negatives</strong> (you have the disease, and we predict you don't have the disease)</li>
</ol>
<p><br></p>
<table>
<thead><tr>
<th></th>
<th>Prediction = 1</th>
<th>Prediction = 0</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Disease = 1</strong></td>
<td>True Positive</td>
<td>False Negative</td>
</tr>
<tr>
<td><strong>Disease = 0</strong></td>
<td>True Positive</td>
<td>False Negative</td>
</tr>
</tbody>
</table>
<h4 id="2.2.2-Sensitivity">2.2.2 Sensitivity<a class="anchor-link" href="#2.2.2-Sensitivity">&#182;</a></h4><p>With that said, we can calculate sensitivity as follows:</p>
<p>$$p(prediction=1 | disease=1) = \frac{p(prediction=1, disease=1)}{p(disease=1)}$$</p>
<p>$$sensitivity = recall = \frac{TP}{TP+FN}$$</p>
<h4 id="2.2.3-Specificity">2.2.3 Specificity<a class="anchor-link" href="#2.2.3-Specificity">&#182;</a></h4><p>And we can then calculate the <strong>specificity</strong> (the true negative rate):</p>
<p>$$p(prediction=0 | disease=0) = \frac{p(prediction=0, disease=0)}{p(disease=0)}$$</p>
<p>$$specificity = \frac{TN}{TN+FP}$$</p>
<h4 id="2.2.4-Precision">2.2.4 Precision<a class="anchor-link" href="#2.2.4-Precision">&#182;</a></h4><p>Now, in information retrieval, rather than specificity, we are interested in <strong>precision</strong>.</p>
<p>$$precision = \frac{TP}{TP+FP}$$</p>
<p>What is this the probability of? Well, $TP$ can be defined as:</p>
<p>$$TP = p(prediction=1, disease=1)$$</p>
<p>And $TP + FP$:</p>
<p>$$TP+FP = p(prediction=1)$$</p>
<p>Which then looks like:</p>
<p>$$precision = \frac{TP}{TP+FP} = \frac{p(prediction=1, disease=1)}{p(prediction=1)}$$</p>
<p>Which equals:</p>
<p>$$p(disease=1|prediction=1) = \frac{p(prediction=1, disease=1)}{p(prediction=1)}$$</p>
<p>This is a useful measure! Just because your results come back positive, does not mean that you have the disease! Generally, more testing is required! This will be explored further in the bayesian statistics section.</p>

</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
