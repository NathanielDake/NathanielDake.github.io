
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../Projects.html">Projects</a>
</li>
        
<li>
  <a href="../Book_Reviews.html">Book Reviews</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="3.-Introduction-to-Probability-Theory">3. Introduction to Probability Theory<a class="anchor-link" href="#3.-Introduction-to-Probability-Theory">&#182;</a></h1><p>This is a post that I have been excited to get to for over a year now. Probability theory plays an incredibly interesting and unique role in the studying of machine learning and articiail intelligence techniques. It gives us a wonderful way of dealing with <strong>uncertainty</strong>, and shows up in everything from <strong>Hidden Markov Models</strong>, <strong>Bayesian Networks</strong>, <strong>Causal Path Analysis</strong>, <strong>Bayesian A/B</strong> testing, and many other areas.</p>
<p>There are several things that make probability so interesting, and I am going to try and cover all of them in this post and several others. The main points are as follows:</p>
<ul>
<li>Probability is rather intertwined with statistics; we will dissect the differences and also how they fit together.</li>
<li>Many paradox's arise from probability, which makes it rather unintuitive to understand. <strong>Simpson's Paradox</strong> and <strong>the Monty Hall</strong> problem are two hallmark probability paradox problems. We will go through each in detail to discuss why they are paradoxical, and how to remedy it.</li>
<li>There are many different ways to visualize and conceptualize probability.</li>
<li><strong>Discrete</strong> vs. <strong>Continuous</strong> probability distributions cause certain visualizations to break down, causing a gap in understanding.</li>
</ul>
<p>This post will not cover all of the above, but it will certainly help us build a base from which we can climb to higher levels of understanding. To begin, I want to start from a historical perspective, digging into how probability first came to be.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.1-Historical-Background-and-Definitions">3.1 Historical Background and Definitions<a class="anchor-link" href="#3.1-Historical-Background-and-Definitions">&#182;</a></h2><p>At is core, probability theory was defined incredibly well by <strong>Pierre-Simon Laplace</strong> in 1814:</p>
<blockquote><p>Probability theory is nothing but common sense reduced to calculation. ... [Probability] is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible ... when nothing leads us to expect that any one of these cases should occur more than any other.</p>
</blockquote>
<p>This simple summary should always be kept in mind when working with probability. The following terms must also be defined:</p>
<ul>
<li><strong>Trial</strong>: A single occurrence with an outcome that is uncertain until we observe it. <ul>
<li>For example, rolling a single die.</li>
</ul>
</li>
<li><strong>Outcome</strong>: A possible result of a trial; one particular state of the world. What Laplace calls a case. <ul>
<li>For example: 4.</li>
</ul>
</li>
<li><strong>Sample Space</strong>: The set of all possible outcomes for the trial. <ul>
<li>For example, {1, 2, 3, 4, 5, 6}.</li>
</ul>
</li>
<li><strong>Event</strong>: A subset of outcomes that together have some property we are interested in. <ul>
<li>For example, the event "even die roll" is the set of outcomes {2, 4, 6}.</li>
</ul>
</li>
<li><strong>Probability</strong>: As Laplace said, the probability of an event with respect to a sample space is the "number of favorable cases" (outcomes from the sample space that are in the event) divided by the "number of all the cases" in the sample space (assuming "nothing leads us to expect that any one of these cases should occur more than any other"). Since this is a proper fraction, probability will always be a number between 0 (representing an impossible event) and 1 (representing a certain event). <ul>
<li>For example, the probability of an even die roll is 3/6 = 1/2.</li>
</ul>
</li>
</ul>
<p>There is one more term that I would like to discuss before moving onto the general rules of probability; that term is <strong>probabilistic</strong>. The term probabilistic is thrown around frequently without many people having a sound definition for what it really entails. Probabilistic can be defined as:</p>
<blockquote><p><strong>Probabilistic:</strong> Subject to or involving chance variation</p>
</blockquote>
<p>Another way of looking at it is that it deals with <strong>uncertainty</strong>. Now, uncertainty can come about in the real world in a variety of ways (no, I am not going to talk about rounds of cards):</p>
<ol>
<li>We have a partial knowledge of the state of the world.</li>
<li>Noisy observations.</li>
<li><em>Phenomena</em> not covered by our model.</li>
<li>Inherent <strong>Stochasticity</strong> </li>
</ol>
<p>The entire goal of probability theory is to allow us to allow us to deal with uncertainty in ways that are principled and proven. Now, these definitions must be understood and internalized before moving on. One of the troubles with probability is the new vocabularly that it introduces, so be sure to look back on these definitions if anything is unclear as we move forward.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.2-Probability-Rules">3.2 Probability Rules<a class="anchor-link" href="#3.2-Probability-Rules">&#182;</a></h2><p>Now, in general there are three very commonly used probabilities: <strong>marginal</strong>, <strong>joint</strong> and <strong>conditional</strong> probability. I like to do this via looking at an example, as I feel it will help keep the concepts more concrete. Suppose we have the situation shown in the table below:</p>
<p><img src="https://drive.google.com/uc?id=1aNGxEap1BWNCnBbQlo8VJn8kp1RKtHs8" width="600"></p>
<p>Here we are looking at historical data surrounding an ecommerce site and purchases from different countries. Each cell corresponds to the number of people from a specific country who either purchased or did not purchase something from the site. This is known as a <strong>discrete distribution</strong>, which we will cover in more depth later in this post. For now, let's try and and answer some questions about this distribution, and in the process, get a feel for the main types of probability.</p>
<p>One final thing before we get going here. I want to quickly give an informal defintion for the term <strong>random variable</strong>. It comes up very frequently in the discussion of probability, and while we don't need to get into the technicalities surrounding it yet, we should have a general idea of what it represents:</p>
<blockquote><p>A <strong>random variable</strong> is a quantitative variable whose value depends on chance in some way.</p>
</blockquote>
<p>This is a very broad and informal definition, however it should at least convey the general idea. A random variable is not known until its value is observed. We can think of the roll of a die as a random variable, or if we were to flip a coin 3 times and set $X$ to be the number of heads, $X$ would then be a random variable that could take on the values: $\{1,2,3,4\}$</p>
<h3 id="3.2.1-Marginal-Probabilities">3.2.1 Marginal Probabilities<a class="anchor-link" href="#3.2.1-Marginal-Probabilities">&#182;</a></h3><p>Say for a moment that we wanted to know:</p>
<blockquote><p>What is the probability that a user is from a specific country?</p>
</blockquote>
<p>Mathematically, that can be written as:</p>
<p>$$P(Country = c)$$</p>
<p>Where $c$ is the specific country you are interested in. Now, without knowing anything about marginal probabilities, I would guess that most people would end up with the following equation:</p>
<p>$$P(Country = c) = \frac{\text{users from } c}{\text{total number of users}}$$</p>
<p>This is entirely correct! Again, keeping in mind the original quote from Laplace, probability can very often be reduced to common sense and counting (at least in the discrete cases). So, we can do just that:</p>
<p>$$P(Country = Canada) = \frac{300 + 20}{300 + 20 + 50 + 500 + 10 + 200} = 0.30$$</p>
<p>$$P(Country = USA) = \frac{500 + 50}{300 + 20 + 50 + 500 + 10 + 200} = 0.51$$</p>
<p>$$P(Country = Mexia) = \frac{200 + 10}{300 + 20 + 50 + 500 + 10 + 200} = 0.19$$</p>
<p>Above we have just found the distribution $P(Country)$ over all countries. We can ensure that it is a correct and legal probability distribution by checking that it sums to 1:</p>
<p>$$\sum_{c} P(Country = c) = 0.3 + 0.51 + 0.19 = 1$$</p>
<p>Now, as I said earlier, I would have expected that even with <em>no knowledge</em> of marginal probabilities most people would come to that conclusion. However, I want to give a way of thinking about the problem as it relates to marginal probabilities and <strong>marginalization</strong> (we will go over marginalization in much greater detail soon).</p>
<p>When dealing with a distribution that contains multiple variables of interest, such as <em>purchase</em> and <em>country</em> in this case, we often want to acquire information about a single variable (such as country in the above example). To do that, we need to <strong>marginalize</strong> out the variable that we are not interested in (purchase in the above example). We intuitively did it above, but there is a great way to visualy think about this. Let's reconsider our table, but with a different shading of our cells:</p>
<p><img src="https://drive.google.com/uc?id=11fqKDqAHHhFRFEaIcMncbZZETLjVMvR-" width="600"></p>
<p>Here, I have shaded the <em>purchased</em> cell's gray since we are not interested in them. What we are interested in is the country information in the red and blue rows. We want to get rid of the purchase information, which by definition is what is means to <strong>marginalize out</strong> a variable. The visual trick here is to think of <em>collapsing</em> the rows, so that the <em>purchase</em> is erased and we are only left with our variable of interest: <em>country</em>.</p>
<p><img src="https://drive.google.com/uc?id=1xh5D-zTZUZkneKF8w2FoPC7omc1Eu0en" width="600"></p>
<p>Now this visual collapsing is really just a summation along each individual column, but the ability to think about a visual collapse helps in understanding how we get rid of the variables that aren't of interest. Now, there is one issue though that I am sure you are wondering about at this point: This final table isn't a probability distrubtion is it? No, it is not! The row does not sum to 1, which actually brings us to our last step: Dividing each entry by the sum of the entire row to ensure it is a valid distribution:</p>
<p><img src="https://drive.google.com/uc?id=1I_Y8LWwRx7cWUGwFIfXkDRRUW1Qw1KPx" width="600"></p>
<p>I hope that that visual has helped in understanding what is going on here, but if it still hasn't quite set in fear not! We will be going over a few more examples shortly. The main thing to take away is that on an intuitive level, a marginal distribution is used when trying to dissociate certain variables so you can gain insight into only the variable of interest.</p>
<p>For those interested in more "text-book defintion", we can define a marginal distribution as:</p>
<blockquote><p>The marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.</p>
</blockquote>
<p>A marginal distribution gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. <strong>Marginal variables</strong> are those variables in the subset of variables being <em>retained</em>. So, in our above case that would be <em>country</em>. These concepts are "marginal" because they can be found by summing values in a table along rows or columns, and writing the sum in the margins of the table. I don't like that visualization quite as much, but for reference that would look like:</p>
<p><img src="https://drive.google.com/uc?id=1yxysUkkytZavgqZKv6xomXocRWJWOtAr" width="600"></p>
<p>The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing – that is, focusing on the sums in the margin – over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out.</p>
<h3 id="3.2.2-Joint-Distribution">3.2.2 Joint Distribution<a class="anchor-link" href="#3.2.2-Joint-Distribution">&#182;</a></h3><p>Now let's move on to another question all together:</p>
<blockquote><p>What is the probability that someone will make a purchase and that they are from a certain country?</p>
</blockquote>
<p>Mathematically we can write that as:</p>
<p>$$P(Buy=b, Country=c)$$</p>
<p>Where $b$ is either 0 (they did not buy something) or 1 (they did buy something), and $c$ is again country. This visually looks like our original table:</p>
<p><img src="https://drive.google.com/uc?id=1aNGxEap1BWNCnBbQlo8VJn8kp1RKtHs8" width="600"></p>
<p>Now, as before, I would guess that most people could probably reason about this problem based on general intuition. We know that there are two outcomes if a users buys (yes or no) and there are three countries. So, there are 6 total combinations of $b$ and $c$ that the above equation could be filled with. We can easily solve for each of the probabilities as follows:</p>
<p>$$P(Buy=1, Country=Canada)= \frac{20}{1080} = 0.019$$</p>
<p>$$P(Buy=0, Country=Canada)= \frac{300}{1080} = 0.28$$</p>
<p>$$P(Buy=1, Country=USA)= \frac{50}{1080} = 0.046$$</p>
<p>$$P(Buy=0, Country=USA)= \frac{500}{1080} = 0.46$$</p>
<p>$$P(Buy=1, Country=Mexico)= \frac{10}{1080} = 0.0093$$</p>
<p>$$P(Buy=0, Country=Mexico)= \frac{200}{1080} = 0.19$$</p>
<p>Above, we simply take each cell corresponding to $b$ and $c$ and divide its value by the total number of outcomes. Now, we just calculated the joint probabilities by what amounts to gut instinct. However, let's try and put <em>what just felt right</em> into a more defined framework. For instance, what steps did we naturally take in order to calculate $P(Buy=1, Country=Canada)$?</p>
<p>Well, first and foremost we honed in on the column specifically associated with Canada. Because the table was already filled out we may not have even thought about it, but we actively <em>focused</em> on a certain column. Well, that means that we needed to know how probable it was for a user to be from canada in the first place (that is a consequence of our focusing). This can be visualized as:</p>
<p><img src="https://drive.google.com/uc?id=1OUBHqzdXcfMTyLHXtydL6Wz51uu4f_Sk" width="600"></p>
<p>Now, as just mentioned, this focusing must be accounted for in our calculation! By selecting the canada row, we must define how probable a user is to be from canada in the first place. Mathematically this looks like:</p>
<p>$$P(Country=Canada)$$</p>
<p>And from earlier, we know that is equal to:</p>
<p>$$P(Country=Canada) = \frac{320}{1080}$$</p>
<p>So, in defining a more robust way to reason about joint probabilities, we must ensure that the above is included! Now, after this focusing has occured, we are only looking at:</p>
<p><img src="https://drive.google.com/uc?id=1Dzu8tM-qRcIEcAFk6NNFmsVh_MsbULTO" width="400"></p>
<p>Remember, the only way that we are able to ignore the rest of the table is if <em>we account for it in our equation</em>. That is why we need to make sure we include $P(Country=Canada)$ in our final calculation.</p>
<p>So, with this focused view, what is the probability that someone from Canada buys something? Well, that can be written as:</p>
<p>$$P(Buy=1 \mid Country=Canada) = \frac{20}{320}$$</p>
<p>The way that we <em>encode</em> the focusing that we had just done is via the $\mid$ symbol. In english, it means <em>given</em>. So, the entire equation above can be interpreted as: "The probability that a user does buy something, given that they are from canada". This idea of focusing will be defined more thoroughly in the next section concerning <strong>conditional probability</strong> and <strong>conditioning</strong> on certain variables.</p>
<p>At this point, we have everything that we need to calculate $P(Buy=1, Country=Canada)$, via a more robust framework. Specifically, our equation is:</p>
<p>$$P(Buy=1, Country=Canada) = P(Buy=1 \mid Country=Canada) P(Country=Canada)$$</p>
<p>$$P(Buy=1, Country=Canada) = \frac{20}{320} *\frac{320}{1080} = 0.019$$</p>
<p>The equation above is the standard definition for joint probability. Written more generally, we have:</p>
<p>$$P(A,B) = P(A \mid B)P(B)$$</p>
<p>There is an additional visualization that can be rather helpful to further our understanding here:</p>
<p><img src="https://drive.google.com/uc?id=1-yonC3jkzjoJ6pmm4M7MkfvMKpyKzYma" width="400"></p>
<p>We can think of the grey area above as our total probability space, of which there is also a space the represents the probability of being from canada, and another that represents the probability of buying something. Where those two areas overlap represents the probability of being from Canada and buying something.</p>
<p>With that we have just gone through an intuitive derivation of the formula for joint probability (in the discrete case). A few things are worth noting at this point:</p>
<ul>
<li>The table that we worked with above was the joint distribution</li>
<li>As we add more variables it can be shown to increase the space of probabilities exponentially. For instance, we had 2 variables-one that had 2 potential values, the other 3 potential values, leaving us with a space of 6. If we added an additional variable that could have 2 potential values, our space would increase to 12. In the simplest case where each new variable is binary (and only holds 2 values), our space grows as: $2^n$, where $n$ is the number of variables in our distribution. This is known as the <strong>curse of dimensionality</strong></li>
<li>Notice that the final probabilities we found above are much smaller than the marginal probabilities. This is due to the fact that our distribution must sum to one, and since our total possibilities grows exponentially with each additional variable, the actual probabilities will shrink exponentially</li>
</ul>
<p>Now, just as we did with marginal probabilities, we can close with some dry definitions for those who are interested:</p>
<blockquote><p>Given random variables $X$, $Y$, that are defined on a probability space, the joint probability distribution for $X$, $Y$, is a probability distribution that gives the probability that each of $X$, $Y$, falls in any particular range or discrete set of values specified for that variable. In the case of only two random variables, this is called a bivariate distribution, but the concept generalizes to any number of random variables, giving a multivariate distribution.</p>
</blockquote>
<h3 id="3.2.3-Conditional-Probabilities">3.2.3 Conditional Probabilities<a class="anchor-link" href="#3.2.3-Conditional-Probabilities">&#182;</a></h3><p>As before, we can transition to another question of interest:</p>
<blockquote><p>What is the probability that a user buys or doesn't buy, given that they are from a certain country?</p>
</blockquote>
<p>Again, just going off of gut instinct we can see that we start with the base table:</p>
<p><img src="https://drive.google.com/uc?id=1aNGxEap1BWNCnBbQlo8VJn8kp1RKtHs8" width="600"></p>
<p>We again perform the focusing that we talked about earlier, only now we have a new word for it: <strong>conditioning</strong>. Conditioning simply means that we are going to focus in on a subset of variables in the distributions in some way. Intuively, after we have focused our calculations look like:</p>
<p>$$P(Buy=1 \mid Country=Canada)= \frac{20}{320} = 0.07$$</p>
<p>$$P(Buy=0 \mid Country=Canada)= \frac{300}{320} = 0.93$$</p>
<p>$$P(Buy=1 \mid Country=USA)= \frac{50}{550} = 0.09$$</p>
<p>$$P(Buy=0 \mid Country=USA)= \frac{500}{550} = 0.91$$</p>
<p>$$P(Buy=1 \mid Country=Mexico)= \frac{10}{210} = 0.04$$</p>
<p>$$P(Buy=0 \mid Country=Mexico)= \frac{200}{210} = 0.96$$</p>
<p>Notice that the sum of the above results is three, not one. This is because country is no longer random at this point-it is given! So, the results for each given country sum to one, and since there are three countries all of the results sum to three.</p>
<p>Now if we looked at a specific example, where we are trying to determine the probability that a user does or doesn't buy given they are from the united states, that would look like:</p>
<p><img src="https://drive.google.com/uc?id=1mu2pIDnrEN-YZOWRyAQb5xadyNsH40Oj" width="600"></p>
<p>Now recall the equation for the joint distribution:</p>
<p>$$P(Buy=1, Country=USA) = P(Buy=1 \mid Country=USA) P(Country=USA)$$</p>
<p>In our current situation, we are not trying to solve for the joint, but rather the conditional probability above:</p>
<p>$$ P(Buy=1 \mid Country=USA) = \frac{P(Buy=1, Country=USA)}{P(Country=USA)}$$</p>
<p>The above was just some basic algebraic manipulation, but it allows us to express our conditional probability in a way that may not be quite as intuitive. However, the nice thing about it is that we already have values for the two probabilities on the right hand side of the equation!</p>
<p>$$ P(Buy=1 \mid Country=USA) = \frac{0.046}{0.51} = 0.09$$</p>
<p>And that is exactly what we had found above via our slightly more intuitive, gut-instinct approach.</p>
<p>Now, using our visualization from before, we can think of finding $P(Buy=1 \mid Country=USA)$ as follows:</p>
<p><img src="https://drive.google.com/uc?id=1XpW1AGgUDGC_GliqeZ8xgZx-FyleKYhd" width="600"></p>
<p>We are specifically being <em>given</em> information, in this case that the country is the USA. What that essentially means is that we are <em>focusing</em> in on only the space where country is the USA. That space has a probability of 0.51. We then look at, within that space, what fraction of users buy something. In other words, we know that our total (focused) space has a size of 0.51 while our subspace where a user buys something is 0.046. We simply divide the latter by the former in order to compute our desired probability!</p>
<p>So, we are (as always) looking for a specific event and then dividing by the total number of events. In this case, our total number of events is constrained (conditioned) based on the focusing around the country being the USA.</p>
<p>These two visualizations are very helpful because they provide two unique viewpoints with the same information conveyed, but through a different medium (you could even say they are isomorphic, in the informal sense of the word).</p>
<p>We can close our discussion of conditional probability by discussing a formal definition.</p>
<blockquote><p>Conditional probability is a measure of the probability of an event (some particular situation occurring) given that (by assumption, presumption, assertion or evidence) another event has occurred. The conditional probability of A given B is written as $P(A \mid B)$.</p>
</blockquote>
<p>As another example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person has a cold, then they are much more likely to be coughing. The conditional probability of coughing given that you have a cold might be a much higher 75%.</p>
<p>In the most general case, we define conditional probability as:</p>
<p>$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$</p>
<h3 id="3.2.4-Bayes-Rule">3.2.4 Bayes Rule<a class="anchor-link" href="#3.2.4-Bayes-Rule">&#182;</a></h3><p>Now, I would like to give a quick introduction to the increasingly popularized <strong>Bayes Rule</strong>. At it's core, Bayes Rule is just a unification of conditional probability and the idea that two things that each equal a third thing must also equal each other (as shown by Euclid over 2,000 years ago). We just showed above the idea of conditional probability in one direction:</p>
<p>$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$</p>
<p>But, the reverse is equally true:</p>
<p>$$P(B \mid A) = \frac{P(B \cap A)}{P(A)}$$</p>
<p>Because the order of a joint probability does not matter, we know that:</p>
<p>$$P(B \cap A) = P(A \cap B)$$</p>
<p>If we then multiply each of our conditionals by the respective marginal:</p>
<p>$$P(A \mid B)P(B) = P(A \cap B)$$</p>
<p>$$P(B \mid A)P(A) = P(A \cap B)$$</p>
<p>We see that in each of the two above equations, the left hand side equals $P(A \cap B)$. So, finally let's set the two left hand sides equal to each other:</p>
<p>$$P(A \mid B)P(B) = P(B \mid A)P(A)$$</p>
<p>The above relationship is Baye's rule! Now, I am dedicating the entire next post in this series to an in depth look at Bayes Rule, so we won't go into much more detail now. The purpose of the above was just to show that Bayes Rule is only a slight extension to our understanding of basic conditional probabilities.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.3-Discrete-vs.-Continuous-Distributions">3.3 Discrete vs. Continuous Distributions<a class="anchor-link" href="#3.3-Discrete-vs.-Continuous-Distributions">&#182;</a></h2><h3 id="3.3.1-Discrete-Distributions">3.3.1 Discrete Distributions<a class="anchor-link" href="#3.3.1-Discrete-Distributions">&#182;</a></h3><h3 id="3.3.2-Continuous-Distributions">3.3.2 Continuous Distributions<a class="anchor-link" href="#3.3.2-Continuous-Distributions">&#182;</a></h3><h3 id="Probability-Density-Functions">Probability Density Functions<a class="anchor-link" href="#Probability-Density-Functions">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.4-Marginalization">3.4 Marginalization<a class="anchor-link" href="#3.4-Marginalization">&#182;</a></h2><p>The final introductory probability concept that I wanted to highlight was that of <strong>marginalization</strong>. We have seen marginalization before when determining our marginal probabilities. We can informally define marginalization as follows:</p>
<blockquote><p><strong>Marginalization</strong>: A way to safely ignore variables.</p>
</blockquote>
<p>And, as a quick reminder of what we just discussed earlier:</p>
<ul>
<li><strong>Marginal probability</strong>: is the probability of any single event occurring unconditioned on any other events. Whenever someone asks you whether the weather is going to be rainy or sunny today, you are computing a marginal probability. </li>
<li><strong> Joint probability</strong>: probability of more than one event occurring simultaneously. If I ask you whether the weather is going to be rainy and the temperature is going to be above a certain number, you are computing a joint probability.</li>
<li><strong>Conditional probability</strong>:  probability of an event occurring given some events that you have already observed. When I ask you what’s the probability that today is rainy or sunny given that I noticed the temperature is going to be above 80 degrees, you are computing a conditional probability.</li>
</ul>
<p>Okay, we are now ready to dig into the discrete case of marginalization.</p>
<h3 id="3.4.1-Marginalization-$\rightarrow$-Discrete">3.4.1 Marginalization $\rightarrow$ Discrete<a class="anchor-link" href="#3.4.1-Marginalization-$\rightarrow$-Discrete">&#182;</a></h3><p>Let's start with a simple discrete example. Say we have a the following table:</p>
<p><img src="https://drive.google.com/uc?id=1rfE1rxv-Rn_7lgM_G1su_vGOYRL-FugF" width="400"></p>
<p>Now, let's ask: What is the probability that someone is experiencing symptoms? In order to solve this, intuitively we say:</p>
<blockquote><p>What are all of the ways in which someone can be experience symptoms?</p>
</blockquote>
<p>In other words, what is the probability that $Y=1$?</p>
<p>$$p(Y=1)$$</p>
<p>In order to find that we can just sum up the different ways in which $Y=1$! Visually that looks like:</p>
<p><img src="https://drive.google.com/uc?id=10skKfKglkRI9mjs3R5Ulks0Lr5t7DOm0" width="400"></p>
<p>And we can write it as:</p>
<p>$$p(Y=1) = p(Y=1, X=0) + p(Y=1, X=1) = 0.1 + 0.3 = 0.4$$</p>
<p>This can be rewritten with a summation for consciseness:</p>
<p>$$p(Y=1) = \sum_{x}p(Y=1, X=x)$$</p>
<p>Now, our general case above can be expanded for the general formula of marginalization:</p>
<p>$$P(Y=y) = \sum_{x}p(Y=y, X=x)$$</p>
<p>Now, keep in mind that we can do the exact same thing to the $X$ variable if we'd like; that is, we can marginalize out $Y$ and be left with only $X$. For instance, say we want to know:</p>
<blockquote><p>What is the probability that someone has the disease?</p>
</blockquote>
<p>Intuitvely, we know that is just <em>the probability someone has the disease and shows symptons</em>, plus <em>the probability someone has the disease and doesn't show symptoms</em>.</p>
<p><img src="https://drive.google.com/uc?id=1VzQ_X9PmN3xXgbz6i4r4GnWtyueG26qy" width="400"></p>
<p>And we can write it as:</p>
<p>$$p(X=1) = p(Y=0, X=1) + p(Y=1, X=1) = 0.1 + 0.3 = 0.4$$</p>
<h3 id="3.4.1.1-Discrete-Case-Intuition">3.4.1.1 Discrete Case Intuition<a class="anchor-link" href="#3.4.1.1-Discrete-Case-Intuition">&#182;</a></h3><p>A way to think about this in the discrete case (when we are dealing with tables), is that we are <em>collapsing</em> the dimension that we are marginalizing out. For instance, our table above represents the <em><strong>joint distribution</strong></em> between <em>disease</em> and <em>symptoms</em>. Now, if we wanted to find just the distribution of <em>symptoms</em>, we would need to marginalize out <em>disease</em>:</p>
<p>$$P(Symptoms) = \sum_{disease={yes,no}}p(Symptoms, Disease=disease)$$</p>
<p>$$P(Y) = \sum_{x}p(Y, X=x)$$</p>
<p>The key is to remember how this looks visually:</p>
<p><img src="https://drive.google.com/uc?id=18HW5nckCyi3_zOLSzl7b7LOhzwwFGa5T" width="700"></p>
<p>We can see that out columns in $X$ (the disease) were collapsed into a single probability column for $Y$, and the $X$ variable no longer remained.</p>
<h3 id="3.4.1.2-A-Confusing-Convention">3.4.1.2 A Confusing Convention<a class="anchor-link" href="#3.4.1.2-A-Confusing-Convention">&#182;</a></h3><p>You may notice that if we are trying to find $p(Y)$ for all $y$, the equation to do so is written as:</p>
<p>$$P(Y) = \sum_{x}p(Y, X=x)$$</p>
<p>What may seem strange if you think about this from the mechanical perspective (or if you think about implementing it in code), is that we seem to be missing an iterator. In other words, what we are trying to do is sum over all $x$ in $X$, <em>for each</em> $y$. In code we could write it as:</p>

<pre><code>P(y,x) # Joint Distribution
P(y) # Marginalized Distribution
for y in Y:
    for x in X:
        P(y) += P(y,x)</code></pre>
<p>However, when we write the actual mathematical equation, that first iterator that occurs over all $y$ is missing. That is simply a convention, nothing more.</p>
<h3 id="3.4.2-Marginalization-$\rightarrow$-Continuous">3.4.2 Marginalization $\rightarrow$ Continuous<a class="anchor-link" href="#3.4.2-Marginalization-$\rightarrow$-Continuous">&#182;</a></h3>
</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
