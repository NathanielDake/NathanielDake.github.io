
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="10.-Momentum-and-Adaptive-Learning-Rates">10. Momentum and Adaptive Learning Rates<a class="anchor-link" href="#10.-Momentum-and-Adaptive-Learning-Rates">&#182;</a></h1><p>We will now take a look at one of the most effective methods at improving plain gradient descent, called <strong>momentum</strong>. This can be thought of as the 80% factor to improve your learning procedure.</p>
<p>A way to think of this is as follows: Gradient descent without momentum requires a <em>force</em> or <em>push</em> each time we want to get the weights to move. In other words, each time we want to move, there has to a be a gradient so that we can move in the direction of the gradient. If we had <strong>momentum</strong>, we can imagine that our update could keep moving, even without the gradient being present.</p>
<p>This can be thought of as pushing a box on ice vs. pushing a box on gravel. If we are pushing the box on gravel, the minute we stop applying force, the box will also stop moving. This is analogous to gradient descent without momentum. However, if we were pushing the box on ice we could and then let go and it would continue moving for a period of time, before stopping. This is analogous to gradient descent with momentum. Another way to phrase this is as follows:</p>
<blockquote><p>With momentum included in our update, our weight vector will build up a velocity in any direction that has a <em>consistent gradient</em>.</p>
</blockquote>
<p>Let's put this into math.</p>
<h2 id="1.1-Gradient-Descent,-without-Momentum">1.1 Gradient Descent, <em>without</em> Momentum<a class="anchor-link" href="#1.1-Gradient-Descent,-without-Momentum">&#182;</a></h2><p>Our update for $\theta$ can be described as:
$$\theta_t \leftarrow \theta_{t-1} - \eta g_{t-1}$$
This says that $\theta_t$ is equal to the previous value of $theta$, minus the learning rate, times the gradient $g_t$. From this we can see that if the gradient is 0, nothing will happen to $\theta_t$. It just gets updated to it's old value and doesn't change.</p>
<h2 id="1.2-Gradient-Descent,-with-Momentum">1.2 Gradient Descent, <em>with</em> Momentum<a class="anchor-link" href="#1.2-Gradient-Descent,-with-Momentum">&#182;</a></h2><p>Now let's say that we add in <strong>momentum</strong>. Note that the term momentum is used very loosely here, since it has nothing to do with actual physical momentum. What we do is create a new variable, $v$, which stands for the velocity. It is equal to $\mu$ (the momentum term) times its old velocity, minus the learning rate times the gradient. Notice that now, the gradient only directly influences the velocity, which in turn has an effect on the position (our weight vector), $\theta$.</p>
$$v_t \leftarrow \mu v_{t-1} - \eta g_{t-1}$$<p>This new term, $\mu v_{t-1}$, gives us the ability to "slide on ice" if you will. In other words, it allows us to continue to move in the same direction that we were going before. Now, we talked about how if a box is sliding on ice, it will still stop eventually. That means that we are going to want our updated $v$ to be a fraction of the prior $v$, and hence $\mu$ should be a fraction. Typical values of $\mu$ are 0.9, 0.95, 0.99, etc. This means that without any $g$, the equation will still eventually "slow down". Our update rule for $\theta_t$ then becomes:</p>
$$\theta_t \leftarrow \theta_{t-1} + v_t$$<p>Now, if we combine these two equations we can see that our total update rule is:</p>
$$\theta_t \leftarrow \theta_{t-1} + \mu v_{t-1} -\eta g_{t-1} $$<p>And we can see that if we set the momentum term, $\mu$, equal to zero, we end up with the same update rule we originally had for gradient descent.</p>
<h2 id="1.3-The-Effect-of-Momentum">1.3 The Effect of Momentum<a class="anchor-link" href="#1.3-The-Effect-of-Momentum">&#182;</a></h2><p>You may be wondering, what is the effect of using momentum? Well we can see below that by using momentum, the cost converges to its minimum value much faster. This significantly speeds up training!</p>
<p><img src="https://drive.google.com/uc?id=1-poOdEvq6lpTJjcOAV0-9MNbQ54ofBHt" width="500"></p>
<p>From another perspective, we can think of a situation where we have unequal gradients in different directions. In the image below, we have a very large gradient that creates the valley (each side is very steep), and then in the other direction (the stream flowing down), it is a very small gradient.</p>
<p><img src="https://drive.google.com/uc?id=1g3v2TWK7BfgRbfLUzonPjSi7kJkeWhcB" width="400"></p>
<p>For visualization purposes, lets assume we have 2 parameters to optimize: the vertical and horizontal parameter. The gradient in one direction is very steep, and the gradient in the other direction is very shallow. The idea is that if you don't have momentum, then you rely purely on the gradient, which points more in the steep direction than in the shallow direction-this is just a property of the gradient, it is the direction of steepest descent. Since this gradient vector points more in the steep direction, we are going to zigzag back and forth across the valley. That is a very inefficient way of reaching the minimum.</p>
<p><img src="https://drive.google.com/uc?id=1s4OisuSJPCOqLWjnThmGIXP7fle2qLON" width="300"></p>
<p>Once we add momentum, however, things change. Because in the shallow direction, we move in the same direction every time, those velocities are going to accumulate, so we will have a portion of our old velocity, added to our new velocity to help us along in that direction. The result is that we get there faster by taking bigger steps in the shallow direction of the gradient.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></br></p>
<h1 id="2.-Nesterov-Momentum">2. Nesterov Momentum<a class="anchor-link" href="#2.-Nesterov-Momentum">&#182;</a></h1><p>Nesterov momentum was coined by <strong>Y Nesterov</strong> in 1983. It is described as:</p>
<blockquote><p>"A method for unconstrained convex minimization problem with rate of convergence O(1/$k^2$)"</p>
</blockquote>
<p>The core idea is that when the current weight vector is at some position, lets say $w$, then looking at original momentum update from earlier, we know that the momentum term alone (ignoring the term with the gradient), is about to nudge the parameter vector by $\mu v_{t-1}$. Therefore, if we are about to compute the gradient, we can treat the future approximate position of $w$, $w + \mu v_{t-1}$, as a "lookahead" - as in this is a point in the vicinity of where we are going to end up. Hence, it makes sense to compute the gradient at the $w + \mu v_{t-1}$, instead of the old/stale position $w$.</p>
<p><br></br>
<img src="https://drive.google.com/uc?id=1ivr45vrPNtKmal7_qhjKlSvqIkEUqCCh" width="600"></p>
<p>The image above makes it clear that instead of evaluating the gradient at the current position of $w$, (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this "looked-ahead" position. Also, keep in mind that in the image above the blue vector is referring to our update to the velocity, $v_t$, and not to the update of $w_t$.</p>
<p>Okay, so we have a basic idea of <strong>nesterov momentum</strong> now, but let's just try and reiterate from a few different perspectives, to help is sink in. So, instead of just using momentum to blindly keep going in the direction that we were already going, let's instead peak ahead, by taking a big jump in the direction of the previous velocity, and calculate the gradient from there. We can think of it as though you are gambling, and if you are going to gamble it is better to take a big jump and then make a correction, than to make a correction and then gamble.</p>
<p><img src="https://drive.google.com/uc?id=1O7b1eNCQdaYB9mGemuT7HrKn3ZgJtN3r" width="600"></p>
<p>So first we peak ahead, jumping in the direction of the previous velocity (accumulated gradient):</p>
<p><img src="https://drive.google.com/uc?id=1VaEu1Vw4L9MVDDn9_VQ_a95qOp4egrnF" width="500"></p>
<p>We then measure the gradient, and go downhill in the direction of the gradient. We use that gradient to update our velocity (accumulated gradient). In other words, we combine the big jump with our gradient to get the accumulated gradient. So in a way, its peaking ahead and then course correcting based on where we would have ended up.</p>
<p><img src="https://drive.google.com/uc?id=1uMkN0mXVC7tPpO8v4110zSjE_iorHzVZ" width="500"></p>
<p>We then take that accumulated gradient (first green vector), multiply by some momentum constant, $\mu$, and then we take the next big jump in the direction of that accumulated gradient. Again, at the place where we end up (head of second brown vector), we measure the gradient, we go downhill (second red vector) to correct any errors we have made, and we get a new accumulated gradient (second green vector)</p>
<p>We can see that the blue vectors represent where we would go if we were using standard momentum, where we first measure the gradient where it currently is (small blue vector), and it adds that to the brown vector, and ends up making a jump by the big blue vector (first brown vector plus small blue vector, i.e. the current gradient). The brown vector represents our peak ahead value. Notice that it is in the same direction as the blue vector. The red vector is the gradient at the peak ahead value. The green vector is just the vector of the brown vector and the red vector.</p>
<hr>
<p><br></br></p>
<h2 id="2.1-Nesterov-Equations">2.1 Nesterov Equations<a class="anchor-link" href="#2.1-Nesterov-Equations">&#182;</a></h2><p>So, with the visuals discussed, what do the equations look like? First, we are going to use $w$ to represent our weights instead of $\theta$. Also, the majority if this is looking at how we will update $v_t$, and the last step covers $w_t$. Now, lets start with the vector that represents the previous value of our weights, $w_{t-1}$, and the previous velocity, $v_{t-1}$:</p>
<p><img src="https://drive.google.com/uc?id=1YmBMpV5vWFwIfLDt-G5_prqSD8ha_pGp" width="250"></p>
<p>Now, we have this jump ahead, which we can call $w'_{t-1}$. We can also think of it as just our previous weight position, plus the momentum step. It is in the same direction of our previous velocity vector (because remember, the first part of updating $v_t$ was the term $\mu v_{t-1}$, but it is slightly smaller since the jump is scaled by $\mu$:</p>
<p><img src="https://drive.google.com/uc?id=1fJUGXjdcE6NC1W83c0xLE2g1ihxtmgfP" width="350"></p>
$$look \; ahead\; value: \; w'_{t-1} = w_{t-1} +\mu v_{t-1}$$$$look \; ahead\; value: \; w'_{t-1} = v_{t-1} +\mu v_{t-1}$$<p>The above equations are equivalent because both $w_{t-1}$ and $v_{t-1}$ both have the same position (head each vector). Also, note that as seen in the image above, the jump ahead is just the previous value of the velocity (or previous weight position, $w_{t-1}$), plus the momentum term multiplied by the previous velocity. Next, we calculate the gradient at this jump ahead point, and then use that to update $v$:</p>
<p><img src="https://drive.google.com/uc?id=19igElK8e3DBAle_jYMx17be38ADdL5d0" width="350"></p>
<p><img src="https://drive.google.com/uc?id=1aRcNTcrqDAPA51g5tOaqvzD9cScrYPiw" width="350"></p>
$$v_t \leftarrow \mu v_{t-1} - \eta \nabla J(w'_{t-1})$$<p>Which is equal to:</p>
$$v_t \leftarrow \mu v_{t-1} - \eta \nabla J(w_{t-1} +\mu v_{t-1})$$<p>And then the last step is to update $w_t$, the accumulated gradient, which is the same as it was for standard momentum:</p>
<p><img src="https://drive.google.com/uc?id=1XQ6Q-veJkvf3EWLD23OnkBvxt867iV2E" width="350"></p>
$$w_t \leftarrow w_{t-1} + v_t$$<p>The main difference to note is that in the standard method we are taking the gradient from the current position of $w$, and also making our momentum step from the current position of $w$, whereas in the Nesterov method we first take our momentum step from the current position of $w$, then correct by taking the gradient from that position. For a great link that goes over this topic in more detail, checkout out the follow: <a href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></p>
<hr>
<p><br></br></p>
<h2 id="2.3-Reformulation">2.3 Reformulation<a class="anchor-link" href="#2.3-Reformulation">&#182;</a></h2><p>However, in practice, this is not how nesterov momentum is usually implemented. Instead, we will reformulate the equations. Let's try and express everything only in terms of $w'$, our lookahead value of $w$, and it is where we want to calculate the gradient from. So, we can define $w'_t$ and $w'_{t-1}$ using the same definition:</p>
<h3 id="2.3.1-Redefine-in-terms-of-$w'$">2.3.1 Redefine in terms of $w'$<a class="anchor-link" href="#2.3.1-Redefine-in-terms-of-$w'$">&#182;</a></h3>$$w'_t = w_t + \mu v_t$$$$w'_{t-1} = w_{t-1} + \mu v_{t-1}$$<p>In other words, these are the lookahead values of $w$ at two consecutive steps. The second equation is seen in the below:</p>
<p><img src="https://drive.google.com/uc?id=1U4RFAtJD5YFOqPxHtDWe3CyYrbyHfhvt" width="350"></p>
<p>So, remember, the look ahead value can just be thought of as the step in the direction of the previous velocity, from the current weight vector postion. Next lets recall our updates for $v$ and $w$.</p>
<h3 id="2.3.2-Recall-updates-for-$v$-and-$w$">2.3.2 Recall updates for $v$ and $w$<a class="anchor-link" href="#2.3.2-Recall-updates-for-$v$-and-$w$">&#182;</a></h3>$$v_t = \mu v_{t-1} - \eta \nabla J(w'_{t-1})$$$$w_t = w_{t-1} + v_t$$<p>From here we can substitute $w'$ for $w$</p>
<h3 id="2.3.3-Substitute-$w'$-for-$w$">2.3.3 Substitute $w'$ for $w$<a class="anchor-link" href="#2.3.3-Substitute-$w'$-for-$w$">&#182;</a></h3>$$w'_t - \mu v_t = w'_{t-1} - \mu v_{t-1} + v_t$$<h3 id="2.3.4-Combine-like-terms-on-the-right">2.3.4 Combine like terms on the right<a class="anchor-link" href="#2.3.4-Combine-like-terms-on-the-right">&#182;</a></h3>$$w'_t = w'_{t-1} -\mu v_{t-1} + (1 + \mu)v_t$$<h3 id="2.3.5-Get-rid-of-$v_t$-term">2.3.5 Get rid of $v_t$ term<a class="anchor-link" href="#2.3.5-Get-rid-of-$v_t$-term">&#182;</a></h3><p>We can get ride of the $v_t$ term on the right by replacing it with an expression in terms of $v_{t-1}$. 
$$w'_t = w'_{t-1} - \mu v_{t-1} + (1+\mu) \Big[\mu v_{t-1} - \eta \nabla J (w'_{t-1})\Big]$$</p>
<h3 id="2.3.6-Combine-like-terms,-arrive-at-Nesterov-momentum-update">2.3.6 Combine like terms, arrive at Nesterov momentum update<a class="anchor-link" href="#2.3.6-Combine-like-terms,-arrive-at-Nesterov-momentum-update">&#182;</a></h3>$$w'_t = w'_{t-1} + \mu^2 v_{t-1} - (1+\mu)\eta \nabla J (w'_{t-1})$$<p>This equation is the one that you will most often see when people are discussing nesterov momentum.</p>
<hr>
<p><br></br></p>
<h2 id="2.4-Regular-vs.-Nesterov-Momentum">2.4 Regular vs. Nesterov Momentum<a class="anchor-link" href="#2.4-Regular-vs.-Nesterov-Momentum">&#182;</a></h2><p>So, to quickly recap, the equation for <strong>regular momentum</strong> is:</p>
$$w_t = w_{t-1} + \mu v_{t-1} - \eta \nabla J (w_{t-1})$$<p>And <strong>Nesterov Momentum</strong> is defined as:</p>
$$w'_t = w'_{t-1} + \mu^2 v_{t-1} - (1+ \mu) \eta \nabla (w'_{t-1})$$<p>We can see that they each have a similar form. There is a previous $w$ term, a previous velocity term, and a previous gradient term. And if we take the nesterov equation, and plug phrase our $w'_t$ update in terms of $v_t$, we can see that the equations have an even greater resemblance:</p>
$$v_t = \mu v_{t-1} - \eta \nabla J (w'_{t-1})$$$$w'_t = w'_{t-1} + \mu v_t - \eta \nabla J (w'_{t-1})$$<p>The question may arise, why is this useful? What is the point of all of this algebraic manuipulation? Well, we have expressed the nesterov momentum update entirely in terms of all the look ahead $w'$s, so the actual non look ahead $w$s are never needed. We can just treat $w$, whatever it may be, as if they were the lookahead values. In other words, lets just drop the prime symbols!</p>
$$v_t = \mu v_{t-1} - \eta \nabla J (w_{t-1})$$$$w_t = w_{t-1} + \mu v_t - \eta \nabla J (w_{t-1})$$<p>So we can see that similar to regular momentum, we just proceed in two steps.</p>
<ol>
<li>First we calculate the new $v$ from the old $v$ and the gradient</li>
<li>Then we update $w$ using the new $v$</li>
</ol>
<hr>
<p><br></br></p>
<h2 id="2.5-Conclusion">2.5 Conclusion<a class="anchor-link" href="#2.5-Conclusion">&#182;</a></h2><p>The final updates for <strong>regular momentum</strong> are:
$$v_t \leftarrow \mu v_{t-1} - \eta \nabla J(w_{t-1})$$
$$w_t \leftarrow w_{t-1} + v_t$$</p>
<p>And for <strong>nesterov momentum</strong>:
$$v_t = \mu v_{t-1} - \eta \nabla J (w_{t-1})$$
$$w_t = w_{t-1} + \mu v_t - \eta \nabla J (w_{t-1})$$</p>
<p>So the main difference be clearly distinguished: The update rule for $w_t$ is slightly different. However, in practice the lost per iteration tends to be rather similar, so choosing between the two is not a huge deal.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></br></p>
<h1 id="3.-Momentum-in-Code">3. Momentum in Code<a class="anchor-link" href="#3.-Momentum-in-Code">&#182;</a></h1><p>Let's now implement 3 different scenarios:</p>
<ol>
<li>Batch Gradient descent, no momentum</li>
<li>Batch Gradient descent, with momentum</li>
<li>Batch Gradient descent, with Nesterov Momentum</li>
</ol>
<p>We can start with our imports:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">shuffle</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">util</span> <span class="k">import</span> <span class="n">get_normalized_data</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y2indicator</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now lets quickly define our <code>forward</code> and derivative functions. Note that I have commented out the sigmoid code, but they can be performed with the sigmoid instead of the ReLU.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
    <span class="c1"># sigmoid</span>
    <span class="c1"># Z = 1 / (1 + np.exp(-( X.dot(W1) + b1 )))</span>

    <span class="c1"># relu</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">Z</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">A</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">expA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">expA</span> <span class="o">/</span> <span class="n">expA</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span>

<span class="k">def</span> <span class="nf">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivative_b2</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivative_w1</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W2</span><span class="p">):</span>
    <span class="c1"># return X.T.dot( ( ( Y-T ).dot(W2.T) * ( Z*(1 - Z) ) ) ) # for sigmoid</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="p">(</span> <span class="p">(</span> <span class="n">Y</span><span class="o">-</span><span class="n">T</span> <span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="c1"># for relu</span>

<span class="k">def</span> <span class="nf">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W2</span><span class="p">):</span>
    <span class="c1"># return (( Y-T ).dot(W2.T) * ( Z*(1 - Z) )).sum(axis=0) # for sigmoid</span>
    <span class="k">return</span> <span class="p">((</span> <span class="n">Y</span><span class="o">-</span><span class="n">T</span> <span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># for relu</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this example we have imported <code>get_normalized_data</code>, which means that we will be using the full 784 dimensionality data set. Now lets define our setup function, where the goal is to prep our data for the 3 scenarios discussed above:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_iter</span> <span class="o">=</span> <span class="mi">20</span>         <span class="c1"># 20 iterations, 1 batch per iteration </span>
<span class="n">print_period</span> <span class="o">=</span> <span class="mi">10</span> 

<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_normalized_data</span><span class="p">()</span>      <span class="c1"># grab our normalized X and Y data</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00004</span>                      <span class="c1"># precomputed learning and reg rates</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span>            

<span class="c1"># create train and test set (data is already shuffled), as well as one hot matrix</span>
<span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">,]</span>          <span class="c1"># grabbing everything up until the last 100</span>
<span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">]</span>           <span class="c1"># grabbing last 1000, making it test set</span>
<span class="n">Xtest</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:,]</span>
<span class="n">Ytest</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:]</span>
<span class="n">Ytrain_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytrain</span><span class="p">)</span>      <span class="c1"># turn targets into one hot encoded matrices</span>
<span class="n">Ytest_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytest</span><span class="p">)</span>

<span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span> 
<span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">500</span>                        <span class="c1"># set batch size to 500</span>
<span class="n">n_batches</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">batch_sz</span>              <span class="c1"># get number of batches</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">300</span>                               <span class="c1"># number hidden units, we found this in prev demo</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>              <span class="c1"># number of output classes </span>

<span class="c1"># intialize weights to random small values </span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

<span class="c1"># and let&#39;s save initial weights so we can compare all methods!</span>
<span class="n">W1_0</span> <span class="o">=</span> <span class="n">W1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">b1_0</span> <span class="o">=</span> <span class="n">b1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">W2_0</span> <span class="o">=</span> <span class="n">W2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">b2_0</span> <span class="o">=</span> <span class="n">b2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Reading in and transforming data...
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Batch-Gradient-Descent,-no-Momentum">1. Batch Gradient Descent, no Momentum<a class="anchor-link" href="#1.-Batch-Gradient-Descent,-no-Momentum">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ------------------ test number 1, batch GD without momentum ------------------</span>
<span class="n">losses_batch</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">errors_batch</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>                  <span class="c1"># iterate through all batches</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>             <span class="c1"># iterate through each specific batch</span>

        <span class="c1"># get batch size, make predictions </span>
        <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
        <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>    
        <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>

        <span class="c1"># perform updates</span>
        <span class="n">W2</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span><span class="p">)</span>
        <span class="n">b2</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span><span class="p">)</span>
        <span class="n">W1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span><span class="p">)</span>
        <span class="n">b1</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span><span class="p">)</span>

        <span class="c1"># if print period:</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
            <span class="n">losses_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>

            <span class="n">e</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
            <span class="n">errors_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

<span class="c1"># print final error rate </span>
<span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------------------------------&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cost at iteration i=0, j=0: 2358.115395
Error rate: 0.849
Cost at iteration i=0, j=10: 1820.148615
Error rate: 0.504
Cost at iteration i=0, j=20: 1474.576347
Error rate: 0.362
Cost at iteration i=0, j=30: 1243.957286
Error rate: 0.275
Cost at iteration i=0, j=40: 1081.230910
Error rate: 0.225
Cost at iteration i=0, j=50: 961.015850
Error rate: 0.205
Cost at iteration i=0, j=60: 870.533621
Error rate: 0.189
Cost at iteration i=0, j=70: 802.442843
Error rate: 0.184
Cost at iteration i=0, j=80: 747.330643
Error rate: 0.173
Cost at iteration i=1, j=0: 737.331648
Error rate: 0.171
Cost at iteration i=1, j=10: 693.893523
Error rate: 0.158
Cost at iteration i=1, j=20: 659.032960
Error rate: 0.151
Cost at iteration i=1, j=30: 630.053802
Error rate: 0.149
Cost at iteration i=1, j=40: 603.948116
Error rate: 0.146
Cost at iteration i=1, j=50: 580.252973
Error rate: 0.141
Cost at iteration i=1, j=60: 560.473756
Error rate: 0.135
Cost at iteration i=1, j=70: 544.425582
Error rate: 0.134
Cost at iteration i=1, j=80: 529.500168
Error rate: 0.133
Cost at iteration i=2, j=0: 526.346451
Error rate: 0.133
Cost at iteration i=2, j=10: 513.001293
Error rate: 0.131
Cost at iteration i=2, j=20: 501.751851
Error rate: 0.127
Cost at iteration i=2, j=30: 491.714107
Error rate: 0.126
Cost at iteration i=2, j=40: 481.438257
Error rate: 0.124
Cost at iteration i=2, j=50: 471.809626
Error rate: 0.123
Cost at iteration i=2, j=60: 463.143653
Error rate: 0.122
Cost at iteration i=2, j=70: 456.677179
Error rate: 0.122
Cost at iteration i=2, j=80: 449.773327
Error rate: 0.12
Cost at iteration i=3, j=0: 448.071399
Error rate: 0.12
Cost at iteration i=3, j=10: 441.632975
Error rate: 0.117
Cost at iteration i=3, j=20: 436.031093
Error rate: 0.117
Cost at iteration i=3, j=30: 430.798084
Error rate: 0.116
Cost at iteration i=3, j=40: 425.237641
Error rate: 0.113
Cost at iteration i=3, j=50: 419.982296
Error rate: 0.113
Cost at iteration i=3, j=60: 414.592662
Error rate: 0.11
Cost at iteration i=3, j=70: 411.291087
Error rate: 0.112
Cost at iteration i=3, j=80: 407.147075
Error rate: 0.111
Cost at iteration i=4, j=0: 405.911467
Error rate: 0.109
Cost at iteration i=4, j=10: 402.114912
Error rate: 0.109
Cost at iteration i=4, j=20: 398.514426
Error rate: 0.109
Cost at iteration i=4, j=30: 395.225017
Error rate: 0.109
Cost at iteration i=4, j=40: 391.661767
Error rate: 0.108
Cost at iteration i=4, j=50: 388.305647
Error rate: 0.107
Cost at iteration i=4, j=60: 384.321366
Error rate: 0.105
Cost at iteration i=4, j=70: 382.328567
Error rate: 0.108
Cost at iteration i=4, j=80: 379.459660
Error rate: 0.107
Cost at iteration i=5, j=0: 378.436303
Error rate: 0.108
Cost at iteration i=5, j=10: 375.832235
Error rate: 0.106
Cost at iteration i=5, j=20: 373.341741
Error rate: 0.104
Cost at iteration i=5, j=30: 371.051750
Error rate: 0.104
Cost at iteration i=5, j=40: 368.486367
Error rate: 0.104
Cost at iteration i=5, j=50: 366.149952
Error rate: 0.104
Cost at iteration i=5, j=60: 362.952477
Error rate: 0.105
Cost at iteration i=5, j=70: 361.652147
Error rate: 0.104
Cost at iteration i=5, j=80: 359.483816
Error rate: 0.103
Cost at iteration i=6, j=0: 358.553982
Error rate: 0.102
Cost at iteration i=6, j=10: 356.734489
Error rate: 0.103
Cost at iteration i=6, j=20: 354.861656
Error rate: 0.101
Cost at iteration i=6, j=30: 353.144264
Error rate: 0.105
Cost at iteration i=6, j=40: 351.187604
Error rate: 0.101
Cost at iteration i=6, j=50: 349.470387
Error rate: 0.101
Cost at iteration i=6, j=60: 346.752728
Error rate: 0.099
Cost at iteration i=6, j=70: 345.843766
Error rate: 0.098
Cost at iteration i=6, j=80: 344.088416
Error rate: 0.098
Cost at iteration i=7, j=0: 343.211922
Error rate: 0.098
Cost at iteration i=7, j=10: 341.899679
Error rate: 0.098
Cost at iteration i=7, j=20: 340.409559
Error rate: 0.099
Cost at iteration i=7, j=30: 339.055293
Error rate: 0.099
Cost at iteration i=7, j=40: 337.489669
Error rate: 0.097
Cost at iteration i=7, j=50: 336.181431
Error rate: 0.094
Cost at iteration i=7, j=60: 333.816026
Error rate: 0.095
Cost at iteration i=7, j=70: 333.157868
Error rate: 0.093
Cost at iteration i=7, j=80: 331.675235
Error rate: 0.095
Cost at iteration i=8, j=0: 330.813709
Error rate: 0.094
Cost at iteration i=8, j=10: 329.853963
Error rate: 0.094
Cost at iteration i=8, j=20: 328.658382
Error rate: 0.094
Cost at iteration i=8, j=30: 327.575400
Error rate: 0.094
Cost at iteration i=8, j=40: 326.307378
Error rate: 0.095
Cost at iteration i=8, j=50: 325.288462
Error rate: 0.092
Cost at iteration i=8, j=60: 323.184212
Error rate: 0.09
Cost at iteration i=8, j=70: 322.722467
Error rate: 0.09
Cost at iteration i=8, j=80: 321.426969
Error rate: 0.09
Cost at iteration i=9, j=0: 320.582716
Error rate: 0.09
Cost at iteration i=9, j=10: 319.862921
Error rate: 0.09
Cost at iteration i=9, j=20: 318.876274
Error rate: 0.089
Cost at iteration i=9, j=30: 318.002236
Error rate: 0.09
Cost at iteration i=9, j=40: 316.963850
Error rate: 0.089
Cost at iteration i=9, j=50: 316.149146
Error rate: 0.089
Cost at iteration i=9, j=60: 314.241733
Error rate: 0.085
Cost at iteration i=9, j=70: 313.923676
Error rate: 0.089
Cost at iteration i=9, j=80: 312.771857
Error rate: 0.088
Cost at iteration i=10, j=0: 311.935290
Error rate: 0.087
Cost at iteration i=10, j=10: 311.366564
Error rate: 0.087
Cost at iteration i=10, j=20: 310.536952
Error rate: 0.088
Cost at iteration i=10, j=30: 309.825779
Error rate: 0.087
Cost at iteration i=10, j=40: 308.977331
Error rate: 0.087
Cost at iteration i=10, j=50: 308.315288
Error rate: 0.085
Cost at iteration i=10, j=60: 306.557321
Error rate: 0.084
Cost at iteration i=10, j=70: 306.331580
Error rate: 0.083
Cost at iteration i=10, j=80: 305.289440
Error rate: 0.084
Cost at iteration i=11, j=0: 304.460711
Error rate: 0.083
Cost at iteration i=11, j=10: 303.997669
Error rate: 0.085
Cost at iteration i=11, j=20: 303.282862
Error rate: 0.084
Cost at iteration i=11, j=30: 302.702055
Error rate: 0.083
Cost at iteration i=11, j=40: 301.999597
Error rate: 0.082
Cost at iteration i=11, j=50: 301.450670
Error rate: 0.082
Cost at iteration i=11, j=60: 299.821759
Error rate: 0.082
Cost at iteration i=11, j=70: 299.676282
Error rate: 0.082
Cost at iteration i=11, j=80: 298.718653
Error rate: 0.083
Cost at iteration i=12, j=0: 297.904338
Error rate: 0.082
Cost at iteration i=12, j=10: 297.503951
Error rate: 0.083
Cost at iteration i=12, j=20: 296.877420
Error rate: 0.082
Cost at iteration i=12, j=30: 296.411239
Error rate: 0.081
Cost at iteration i=12, j=40: 295.845029
Error rate: 0.08
Cost at iteration i=12, j=50: 295.401997
Error rate: 0.081
Cost at iteration i=12, j=60: 293.871915
Error rate: 0.081
Cost at iteration i=12, j=70: 293.801903
Error rate: 0.082
Cost at iteration i=12, j=80: 292.894167
Error rate: 0.082
Cost at iteration i=13, j=0: 292.097619
Error rate: 0.082
Cost at iteration i=13, j=10: 291.749244
Error rate: 0.082
Cost at iteration i=13, j=20: 291.193732
Error rate: 0.081
Cost at iteration i=13, j=30: 290.829059
Error rate: 0.079
Cost at iteration i=13, j=40: 290.378943
Error rate: 0.079
Cost at iteration i=13, j=50: 290.004520
Error rate: 0.08
Cost at iteration i=13, j=60: 288.530719
Error rate: 0.08
Cost at iteration i=13, j=70: 288.511178
Error rate: 0.08
Cost at iteration i=13, j=80: 287.653073
Error rate: 0.078
Cost at iteration i=14, j=0: 286.878971
Error rate: 0.077
Cost at iteration i=14, j=10: 286.571015
Error rate: 0.079
Cost at iteration i=14, j=20: 286.070437
Error rate: 0.077
Cost at iteration i=14, j=30: 285.795053
Error rate: 0.077
Cost at iteration i=14, j=40: 285.442202
Error rate: 0.078
Cost at iteration i=14, j=50: 285.126412
Error rate: 0.077
Cost at iteration i=14, j=60: 283.725768
Error rate: 0.078
Cost at iteration i=14, j=70: 283.755896
Error rate: 0.078
Cost at iteration i=14, j=80: 282.932285
Error rate: 0.077
Cost at iteration i=15, j=0: 282.164853
Error rate: 0.078
Cost at iteration i=15, j=10: 281.890993
Error rate: 0.079
Cost at iteration i=15, j=20: 281.445383
Error rate: 0.078
Cost at iteration i=15, j=30: 281.241730
Error rate: 0.077
Cost at iteration i=15, j=40: 280.976362
Error rate: 0.077
Cost at iteration i=15, j=50: 280.715675
Error rate: 0.077
Cost at iteration i=15, j=60: 279.368006
Error rate: 0.078
Cost at iteration i=15, j=70: 279.428484
Error rate: 0.078
Cost at iteration i=15, j=80: 278.627562
Error rate: 0.078
Cost at iteration i=16, j=0: 277.871408
Error rate: 0.078
Cost at iteration i=16, j=10: 277.604198
Error rate: 0.077
Cost at iteration i=16, j=20: 277.194839
Error rate: 0.077
Cost at iteration i=16, j=30: 277.059178
Error rate: 0.077
Cost at iteration i=16, j=40: 276.874558
Error rate: 0.077
Cost at iteration i=16, j=50: 276.650836
Error rate: 0.077
Cost at iteration i=16, j=60: 275.354274
Error rate: 0.078
Cost at iteration i=16, j=70: 275.433760
Error rate: 0.076
Cost at iteration i=16, j=80: 274.668592
Error rate: 0.077
Cost at iteration i=17, j=0: 273.921165
Error rate: 0.077
Cost at iteration i=17, j=10: 273.668718
Error rate: 0.076
Cost at iteration i=17, j=20: 273.292740
Error rate: 0.077
Cost at iteration i=17, j=30: 273.215803
Error rate: 0.077
Cost at iteration i=17, j=40: 273.088433
Error rate: 0.077
Cost at iteration i=17, j=50: 272.902852
Error rate: 0.076
Cost at iteration i=17, j=60: 271.644681
Error rate: 0.077
Cost at iteration i=17, j=70: 271.740917
Error rate: 0.076
Cost at iteration i=17, j=80: 270.990657
Error rate: 0.076
Cost at iteration i=18, j=0: 270.252998
Error rate: 0.076
Cost at iteration i=18, j=10: 270.003885
Error rate: 0.076
Cost at iteration i=18, j=20: 269.662307
Error rate: 0.076
Cost at iteration i=18, j=30: 269.643838
Error rate: 0.074
Cost at iteration i=18, j=40: 269.572626
Error rate: 0.075
Cost at iteration i=18, j=50: 269.423572
Error rate: 0.076
Cost at iteration i=18, j=60: 268.203335
Error rate: 0.077
Cost at iteration i=18, j=70: 268.323119
Error rate: 0.076
Cost at iteration i=18, j=80: 267.589593
Error rate: 0.076
Cost at iteration i=19, j=0: 266.855792
Error rate: 0.076
Cost at iteration i=19, j=10: 266.602952
Error rate: 0.077
Cost at iteration i=19, j=20: 266.295556
Error rate: 0.075
Cost at iteration i=19, j=30: 266.329657
Error rate: 0.075
Cost at iteration i=19, j=40: 266.308509
Error rate: 0.075
Cost at iteration i=19, j=50: 266.190909
Error rate: 0.075
Cost at iteration i=19, j=60: 265.004141
Error rate: 0.077
Cost at iteration i=19, j=70: 265.148844
Error rate: 0.076
Cost at iteration i=19, j=80: 264.441284
Error rate: 0.076
Final error rate: 0.076
--------------------------------------------------
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Batch-Gradient-Descent,-Regular-Momentum">2. Batch Gradient Descent, Regular Momentum<a class="anchor-link" href="#2.-Batch-Gradient-Descent,-Regular-Momentum">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># --------------- test number 2, batch GD with regular momentum ------------------</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">W1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">b1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">W2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">b2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">losses_momentum</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">errors_momentum</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.9</span>                    <span class="c1"># momentum parameter, think viscosity </span>
<span class="n">dW2</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># need to keep track of the previous weight changes (gradients)</span>
<span class="n">db2</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># think of these as velocity</span>
<span class="n">dW1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">db1</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>                  <span class="c1"># iterate through all batches</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>             <span class="c1"># iterate through each specific batch</span>

        <span class="c1"># get batch size, make predictions </span>
        <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
        <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>    
        <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        
        <span class="c1"># calculate the gradients</span>
        <span class="n">gW2</span> <span class="o">=</span> <span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span>
        <span class="n">gb2</span> <span class="o">=</span> <span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span>
        <span class="n">gW1</span> <span class="o">=</span> <span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span>
        <span class="n">gb1</span> <span class="o">=</span> <span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span>
        
        <span class="c1"># update our velocities - based on regular momentum equation</span>
        <span class="n">dW2</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">dW2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gW2</span>
        <span class="n">db2</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">db2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gb2</span>
        <span class="n">dW1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">dW1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gW1</span>
        <span class="n">db1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">db1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gb1</span>
        
        <span class="c1"># update weights</span>
        <span class="n">W2</span> <span class="o">+=</span> <span class="n">dW2</span>
        <span class="n">b2</span> <span class="o">+=</span> <span class="n">db2</span>
        <span class="n">W1</span> <span class="o">+=</span> <span class="n">dW1</span> 
        <span class="n">b1</span> <span class="o">+=</span> <span class="n">db1</span> 
        
        <span class="c1"># if print period:</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
            <span class="n">losses_momentum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>

            <span class="n">e</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
            <span class="n">errors_momentum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

<span class="c1"># print final error rate </span>
<span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------------------------------&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cost at iteration i=0, j=0: 2358.081866
Error rate: 0.848
Cost at iteration i=0, j=10: 903.875582
Error rate: 0.23
Cost at iteration i=0, j=20: 533.583958
Error rate: 0.15
Cost at iteration i=0, j=30: 449.737845
Error rate: 0.126
Cost at iteration i=0, j=40: 404.674774
Error rate: 0.117
Cost at iteration i=0, j=50: 383.271078
Error rate: 0.108
Cost at iteration i=0, j=60: 360.054545
Error rate: 0.102
Cost at iteration i=0, j=70: 348.575284
Error rate: 0.095
Cost at iteration i=0, j=80: 329.370020
Error rate: 0.094
Cost at iteration i=1, j=0: 324.092778
Error rate: 0.093
Cost at iteration i=1, j=10: 320.172692
Error rate: 0.089
Cost at iteration i=1, j=20: 319.276610
Error rate: 0.087
Cost at iteration i=1, j=30: 308.224811
Error rate: 0.086
Cost at iteration i=1, j=40: 299.440787
Error rate: 0.08
Cost at iteration i=1, j=50: 293.822271
Error rate: 0.079
Cost at iteration i=1, j=60: 287.202975
Error rate: 0.081
Cost at iteration i=1, j=70: 281.112211
Error rate: 0.08
Cost at iteration i=1, j=80: 272.114777
Error rate: 0.079
Cost at iteration i=2, j=0: 269.308716
Error rate: 0.078
Cost at iteration i=2, j=10: 267.260909
Error rate: 0.075
Cost at iteration i=2, j=20: 270.740562
Error rate: 0.074
Cost at iteration i=2, j=30: 266.754194
Error rate: 0.073
Cost at iteration i=2, j=40: 262.147681
Error rate: 0.071
Cost at iteration i=2, j=50: 260.063565
Error rate: 0.073
Cost at iteration i=2, j=60: 255.839661
Error rate: 0.072
Cost at iteration i=2, j=70: 254.427726
Error rate: 0.072
Cost at iteration i=2, j=80: 246.804034
Error rate: 0.074
Cost at iteration i=3, j=0: 244.592102
Error rate: 0.073
Cost at iteration i=3, j=10: 243.219317
Error rate: 0.069
Cost at iteration i=3, j=20: 247.622247
Error rate: 0.07
Cost at iteration i=3, j=30: 245.988386
Error rate: 0.069
Cost at iteration i=3, j=40: 242.024506
Error rate: 0.067
Cost at iteration i=3, j=50: 241.747465
Error rate: 0.071
Cost at iteration i=3, j=60: 237.834349
Error rate: 0.064
Cost at iteration i=3, j=70: 237.441477
Error rate: 0.066
Cost at iteration i=3, j=80: 230.809184
Error rate: 0.065
Cost at iteration i=4, j=0: 228.750599
Error rate: 0.066
Cost at iteration i=4, j=10: 227.067083
Error rate: 0.067
Cost at iteration i=4, j=20: 232.059501
Error rate: 0.066
Cost at iteration i=4, j=30: 230.322145
Error rate: 0.065
Cost at iteration i=4, j=40: 227.859838
Error rate: 0.063
Cost at iteration i=4, j=50: 228.423433
Error rate: 0.063
Cost at iteration i=4, j=60: 225.180980
Error rate: 0.06
Cost at iteration i=4, j=70: 224.949132
Error rate: 0.061
Cost at iteration i=4, j=80: 218.676082
Error rate: 0.064
Cost at iteration i=5, j=0: 216.793937
Error rate: 0.063
Cost at iteration i=5, j=10: 214.973800
Error rate: 0.063
Cost at iteration i=5, j=20: 220.093287
Error rate: 0.061
Cost at iteration i=5, j=30: 219.585164
Error rate: 0.063
Cost at iteration i=5, j=40: 216.716283
Error rate: 0.061
Cost at iteration i=5, j=50: 217.849188
Error rate: 0.061
Cost at iteration i=5, j=60: 214.877194
Error rate: 0.058
Cost at iteration i=5, j=70: 214.549906
Error rate: 0.059
Cost at iteration i=5, j=80: 208.903129
Error rate: 0.061
Cost at iteration i=6, j=0: 207.173266
Error rate: 0.06
Cost at iteration i=6, j=10: 205.074829
Error rate: 0.061
Cost at iteration i=6, j=20: 210.253192
Error rate: 0.057
Cost at iteration i=6, j=30: 210.986922
Error rate: 0.061
Cost at iteration i=6, j=40: 208.452352
Error rate: 0.058
Cost at iteration i=6, j=50: 209.211260
Error rate: 0.059
Cost at iteration i=6, j=60: 206.833627
Error rate: 0.057
Cost at iteration i=6, j=70: 206.423508
Error rate: 0.057
Cost at iteration i=6, j=80: 201.117613
Error rate: 0.06
Cost at iteration i=7, j=0: 199.523282
Error rate: 0.06
Cost at iteration i=7, j=10: 197.419311
Error rate: 0.061
Cost at iteration i=7, j=20: 202.305156
Error rate: 0.056
Cost at iteration i=7, j=30: 203.929185
Error rate: 0.061
Cost at iteration i=7, j=40: 201.238200
Error rate: 0.056
Cost at iteration i=7, j=50: 201.826729
Error rate: 0.057
Cost at iteration i=7, j=60: 200.092663
Error rate: 0.054
Cost at iteration i=7, j=70: 199.631335
Error rate: 0.056
Cost at iteration i=7, j=80: 194.832428
Error rate: 0.059
Cost at iteration i=8, j=0: 193.412496
Error rate: 0.058
Cost at iteration i=8, j=10: 191.336341
Error rate: 0.058
Cost at iteration i=8, j=20: 195.948932
Error rate: 0.056
Cost at iteration i=8, j=30: 198.361817
Error rate: 0.056
Cost at iteration i=8, j=40: 194.730227
Error rate: 0.055
Cost at iteration i=8, j=50: 195.450168
Error rate: 0.056
Cost at iteration i=8, j=60: 194.353066
Error rate: 0.055
Cost at iteration i=8, j=70: 193.825581
Error rate: 0.055
Cost at iteration i=8, j=80: 189.605602
Error rate: 0.057
Cost at iteration i=9, j=0: 188.387385
Error rate: 0.056
Cost at iteration i=9, j=10: 186.465580
Error rate: 0.056
Cost at iteration i=9, j=20: 190.792631
Error rate: 0.05
Cost at iteration i=9, j=30: 193.855244
Error rate: 0.051
Cost at iteration i=9, j=40: 190.962731
Error rate: 0.053
Cost at iteration i=9, j=50: 190.567898
Error rate: 0.053
Cost at iteration i=9, j=60: 189.819025
Error rate: 0.053
Cost at iteration i=9, j=70: 189.823642
Error rate: 0.052
Cost at iteration i=9, j=80: 185.617225
Error rate: 0.052
Cost at iteration i=10, j=0: 184.506506
Error rate: 0.051
Cost at iteration i=10, j=10: 182.677449
Error rate: 0.052
Cost at iteration i=10, j=20: 186.595550
Error rate: 0.047
Cost at iteration i=10, j=30: 190.286247
Error rate: 0.048
Cost at iteration i=10, j=40: 187.418156
Error rate: 0.051
Cost at iteration i=10, j=50: 186.542071
Error rate: 0.052
Cost at iteration i=10, j=60: 185.994211
Error rate: 0.052
Cost at iteration i=10, j=70: 186.306546
Error rate: 0.048
Cost at iteration i=10, j=80: 182.475927
Error rate: 0.049
Cost at iteration i=11, j=0: 181.469354
Error rate: 0.049
Cost at iteration i=11, j=10: 179.665505
Error rate: 0.047
Cost at iteration i=11, j=20: 183.334934
Error rate: 0.046
Cost at iteration i=11, j=30: 187.546132
Error rate: 0.047
Cost at iteration i=11, j=40: 184.803848
Error rate: 0.049
Cost at iteration i=11, j=50: 183.479621
Error rate: 0.051
Cost at iteration i=11, j=60: 183.028955
Error rate: 0.05
Cost at iteration i=11, j=70: 183.584345
Error rate: 0.047
Cost at iteration i=11, j=80: 180.053438
Error rate: 0.045
Cost at iteration i=12, j=0: 179.119469
Error rate: 0.045
Cost at iteration i=12, j=10: 177.340556
Error rate: 0.046
Cost at iteration i=12, j=20: 180.886763
Error rate: 0.046
Cost at iteration i=12, j=30: 185.546039
Error rate: 0.047
Cost at iteration i=12, j=40: 181.808010
Error rate: 0.047
Cost at iteration i=12, j=50: 180.755480
Error rate: 0.049
Cost at iteration i=12, j=60: 180.713466
Error rate: 0.048
Cost at iteration i=12, j=70: 181.079420
Error rate: 0.048
Cost at iteration i=12, j=80: 178.036656
Error rate: 0.045
Cost at iteration i=13, j=0: 177.217364
Error rate: 0.045
Cost at iteration i=13, j=10: 175.765931
Error rate: 0.046
Cost at iteration i=13, j=20: 179.823356
Error rate: 0.047
Cost at iteration i=13, j=30: 183.923750
Error rate: 0.048
Cost at iteration i=13, j=40: 180.525981
Error rate: 0.046
Cost at iteration i=13, j=50: 179.286364
Error rate: 0.046
Cost at iteration i=13, j=60: 179.134737
Error rate: 0.047
Cost at iteration i=13, j=70: 179.542708
Error rate: 0.047
Cost at iteration i=13, j=80: 176.758084
Error rate: 0.045
Cost at iteration i=14, j=0: 175.991566
Error rate: 0.045
Cost at iteration i=14, j=10: 174.501739
Error rate: 0.047
Cost at iteration i=14, j=20: 178.372794
Error rate: 0.048
Cost at iteration i=14, j=30: 182.798071
Error rate: 0.047
Cost at iteration i=14, j=40: 179.478458
Error rate: 0.046
Cost at iteration i=14, j=50: 177.823751
Error rate: 0.046
Cost at iteration i=14, j=60: 177.758454
Error rate: 0.047
Cost at iteration i=14, j=70: 178.342559
Error rate: 0.047
Cost at iteration i=14, j=80: 175.758415
Error rate: 0.045
Cost at iteration i=15, j=0: 175.008925
Error rate: 0.045
Cost at iteration i=15, j=10: 173.518510
Error rate: 0.048
Cost at iteration i=15, j=20: 177.316940
Error rate: 0.048
Cost at iteration i=15, j=30: 181.987632
Error rate: 0.048
Cost at iteration i=15, j=40: 178.878044
Error rate: 0.047
Cost at iteration i=15, j=50: 176.899754
Error rate: 0.045
Cost at iteration i=15, j=60: 176.807546
Error rate: 0.046
Cost at iteration i=15, j=70: 177.449498
Error rate: 0.044
Cost at iteration i=15, j=80: 175.137672
Error rate: 0.045
Cost at iteration i=16, j=0: 174.425890
Error rate: 0.044
Cost at iteration i=16, j=10: 172.905482
Error rate: 0.046
Cost at iteration i=16, j=20: 176.532915
Error rate: 0.048
Cost at iteration i=16, j=30: 181.387923
Error rate: 0.048
Cost at iteration i=16, j=40: 178.581385
Error rate: 0.047
Cost at iteration i=16, j=50: 176.169284
Error rate: 0.046
Cost at iteration i=16, j=60: 175.956559
Error rate: 0.046
Cost at iteration i=16, j=70: 176.660711
Error rate: 0.044
Cost at iteration i=16, j=80: 174.549157
Error rate: 0.043
Cost at iteration i=17, j=0: 173.866521
Error rate: 0.043
Cost at iteration i=17, j=10: 172.374758
Error rate: 0.043
Cost at iteration i=17, j=20: 175.953872
Error rate: 0.047
Cost at iteration i=17, j=30: 180.975641
Error rate: 0.048
Cost at iteration i=17, j=40: 178.501825
Error rate: 0.047
Cost at iteration i=17, j=50: 175.687885
Error rate: 0.046
Cost at iteration i=17, j=60: 175.404130
Error rate: 0.046
Cost at iteration i=17, j=70: 176.167960
Error rate: 0.043
Cost at iteration i=17, j=80: 174.273254
Error rate: 0.044
Cost at iteration i=18, j=0: 173.625275
Error rate: 0.044
Cost at iteration i=18, j=10: 172.099503
Error rate: 0.042
Cost at iteration i=18, j=20: 175.551501
Error rate: 0.046
Cost at iteration i=18, j=30: 180.736241
Error rate: 0.048
Cost at iteration i=18, j=40: 178.559111
Error rate: 0.046
Cost at iteration i=18, j=50: 175.452848
Error rate: 0.046
Cost at iteration i=18, j=60: 175.042356
Error rate: 0.047
Cost at iteration i=18, j=70: 175.819518
Error rate: 0.043
Cost at iteration i=18, j=80: 174.185438
Error rate: 0.043
Cost at iteration i=19, j=0: 173.582440
Error rate: 0.044
Cost at iteration i=19, j=10: 172.058631
Error rate: 0.04
Cost at iteration i=19, j=20: 175.290283
Error rate: 0.046
Cost at iteration i=19, j=30: 180.516692
Error rate: 0.048
Cost at iteration i=19, j=40: 178.767806
Error rate: 0.045
Cost at iteration i=19, j=50: 175.409518
Error rate: 0.045
Cost at iteration i=19, j=60: 174.853181
Error rate: 0.047
Cost at iteration i=19, j=70: 175.597245
Error rate: 0.043
Cost at iteration i=19, j=80: 174.221705
Error rate: 0.043
Final error rate: 0.043
--------------------------------------------------
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Batch-Gradient-Descent,-Nesterov-Momentum">3. Batch Gradient Descent, Nesterov Momentum<a class="anchor-link" href="#3.-Batch-Gradient-Descent,-Nesterov-Momentum">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># --------------- test number 3, batch GD with regular momentum ------------------</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">W1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">b1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">W2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">b2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">losses_nesterov</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">errors_nesterov</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.9</span>                    <span class="c1"># momentum parameter, think viscosity </span>
<span class="n">vW2</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># need to keep track of the previous weight changes (gradients)</span>
<span class="n">vb2</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># think of these as velocity</span>
<span class="n">vW1</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">vb1</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>                  <span class="c1"># iterate through all batches</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>             <span class="c1"># iterate through each specific batch</span>

        <span class="c1"># get batch size, make predictions </span>
        <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
        <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>    
        <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        
        <span class="c1"># calculate the gradients</span>
        <span class="n">gW2</span> <span class="o">=</span> <span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span>
        <span class="n">gb2</span> <span class="o">=</span> <span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span>
        <span class="n">gW1</span> <span class="o">=</span> <span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span>
        <span class="n">gb1</span> <span class="o">=</span> <span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span>
        
        <span class="c1"># update our velocites - based on nesterov momentum equations </span>
        <span class="n">vW2</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vW2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gW2</span>
        <span class="n">vb2</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vb2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gb2</span>
        <span class="n">vW1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vW1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gW1</span>
        <span class="n">vb1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vb1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gb1</span>
        
        <span class="c1"># update our weights - based on nesterov momentum equations </span>
        <span class="n">W2</span> <span class="o">+=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vW2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gW2</span>
        <span class="n">b2</span> <span class="o">+=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vb2</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gb2</span>
        <span class="n">W1</span> <span class="o">+=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vW1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gW1</span>
        <span class="n">b1</span> <span class="o">+=</span> <span class="n">mu</span><span class="o">*</span><span class="n">vb1</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">gb1</span>
        
        <span class="c1"># if print period:</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
            <span class="n">losses_nesterov</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>

            <span class="n">e</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
            <span class="n">errors_nesterov</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

<span class="c1"># print final error rate </span>
<span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------------------------------&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cost at iteration i=0, j=0: 2295.050709
Error rate: 0.82
Cost at iteration i=0, j=10: 850.938970
Error rate: 0.216
Cost at iteration i=0, j=20: 521.677466
Error rate: 0.147
Cost at iteration i=0, j=30: 441.274946
Error rate: 0.125
Cost at iteration i=0, j=40: 398.910493
Error rate: 0.112
Cost at iteration i=0, j=50: 377.277626
Error rate: 0.102
Cost at iteration i=0, j=60: 355.943904
Error rate: 0.1
Cost at iteration i=0, j=70: 345.044062
Error rate: 0.093
Cost at iteration i=0, j=80: 327.048570
Error rate: 0.093
Cost at iteration i=1, j=0: 321.954840
Error rate: 0.091
Cost at iteration i=1, j=10: 318.705830
Error rate: 0.088
Cost at iteration i=1, j=20: 315.078551
Error rate: 0.087
Cost at iteration i=1, j=30: 306.442573
Error rate: 0.086
Cost at iteration i=1, j=40: 297.799286
Error rate: 0.081
Cost at iteration i=1, j=50: 292.476657
Error rate: 0.079
Cost at iteration i=1, j=60: 285.157606
Error rate: 0.079
Cost at iteration i=1, j=70: 279.357931
Error rate: 0.081
Cost at iteration i=1, j=80: 270.542753
Error rate: 0.078
Cost at iteration i=2, j=0: 267.401154
Error rate: 0.077
Cost at iteration i=2, j=10: 266.824718
Error rate: 0.077
Cost at iteration i=2, j=20: 268.832865
Error rate: 0.075
Cost at iteration i=2, j=30: 265.667322
Error rate: 0.073
Cost at iteration i=2, j=40: 261.275809
Error rate: 0.07
Cost at iteration i=2, j=50: 258.182152
Error rate: 0.072
Cost at iteration i=2, j=60: 253.872444
Error rate: 0.071
Cost at iteration i=2, j=70: 252.502799
Error rate: 0.071
Cost at iteration i=2, j=80: 245.298157
Error rate: 0.075
Cost at iteration i=3, j=0: 242.977783
Error rate: 0.073
Cost at iteration i=3, j=10: 242.831265
Error rate: 0.069
Cost at iteration i=3, j=20: 245.276315
Error rate: 0.068
Cost at iteration i=3, j=30: 245.015818
Error rate: 0.068
Cost at iteration i=3, j=40: 242.165216
Error rate: 0.068
Cost at iteration i=3, j=50: 240.806187
Error rate: 0.07
Cost at iteration i=3, j=60: 236.540121
Error rate: 0.065
Cost at iteration i=3, j=70: 236.232807
Error rate: 0.066
Cost at iteration i=3, j=80: 229.879066
Error rate: 0.065
Cost at iteration i=4, j=0: 227.644612
Error rate: 0.066
Cost at iteration i=4, j=10: 227.120486
Error rate: 0.067
Cost at iteration i=4, j=20: 230.416289
Error rate: 0.063
Cost at iteration i=4, j=30: 229.679541
Error rate: 0.065
Cost at iteration i=4, j=40: 228.148811
Error rate: 0.064
Cost at iteration i=4, j=50: 228.009191
Error rate: 0.065
Cost at iteration i=4, j=60: 224.281862
Error rate: 0.06
Cost at iteration i=4, j=70: 223.998878
Error rate: 0.061
Cost at iteration i=4, j=80: 218.047437
Error rate: 0.064
Cost at iteration i=5, j=0: 215.910698
Error rate: 0.063
Cost at iteration i=5, j=10: 215.135899
Error rate: 0.062
Cost at iteration i=5, j=20: 218.868496
Error rate: 0.06
Cost at iteration i=5, j=30: 218.185185
Error rate: 0.064
Cost at iteration i=5, j=40: 216.606524
Error rate: 0.061
Cost at iteration i=5, j=50: 217.060520
Error rate: 0.063
Cost at iteration i=5, j=60: 213.911347
Error rate: 0.058
Cost at iteration i=5, j=70: 213.723448
Error rate: 0.06
Cost at iteration i=5, j=80: 208.203357
Error rate: 0.061
Cost at iteration i=6, j=0: 206.246946
Error rate: 0.061
Cost at iteration i=6, j=10: 205.240186
Error rate: 0.061
Cost at iteration i=6, j=20: 209.263942
Error rate: 0.057
Cost at iteration i=6, j=30: 209.620457
Error rate: 0.062
Cost at iteration i=6, j=40: 208.531215
Error rate: 0.059
Cost at iteration i=6, j=50: 208.471934
Error rate: 0.059
Cost at iteration i=6, j=60: 205.859014
Error rate: 0.058
Cost at iteration i=6, j=70: 205.776474
Error rate: 0.058
Cost at iteration i=6, j=80: 200.599328
Error rate: 0.06
Cost at iteration i=7, j=0: 198.802431
Error rate: 0.059
Cost at iteration i=7, j=10: 197.715409
Error rate: 0.06
Cost at iteration i=7, j=20: 201.805965
Error rate: 0.057
Cost at iteration i=7, j=30: 202.854579
Error rate: 0.06
Cost at iteration i=7, j=40: 201.678974
Error rate: 0.056
Cost at iteration i=7, j=50: 201.514702
Error rate: 0.058
Cost at iteration i=7, j=60: 199.348444
Error rate: 0.056
Cost at iteration i=7, j=70: 199.242580
Error rate: 0.056
Cost at iteration i=7, j=80: 194.363938
Error rate: 0.058
Cost at iteration i=8, j=0: 192.708563
Error rate: 0.058
Cost at iteration i=8, j=10: 191.454832
Error rate: 0.059
Cost at iteration i=8, j=20: 195.565930
Error rate: 0.056
Cost at iteration i=8, j=30: 197.212288
Error rate: 0.056
Cost at iteration i=8, j=40: 195.688913
Error rate: 0.055
Cost at iteration i=8, j=50: 195.410401
Error rate: 0.056
Cost at iteration i=8, j=60: 193.706306
Error rate: 0.055
Cost at iteration i=8, j=70: 193.721313
Error rate: 0.056
Cost at iteration i=8, j=80: 189.410913
Error rate: 0.056
Cost at iteration i=9, j=0: 187.922696
Error rate: 0.053
Cost at iteration i=9, j=10: 186.569590
Error rate: 0.054
Cost at iteration i=9, j=20: 190.574046
Error rate: 0.051
Cost at iteration i=9, j=30: 192.846757
Error rate: 0.051
Cost at iteration i=9, j=40: 191.018419
Error rate: 0.051
Cost at iteration i=9, j=50: 190.655309
Error rate: 0.053
Cost at iteration i=9, j=60: 189.372065
Error rate: 0.053
Cost at iteration i=9, j=70: 189.415843
Error rate: 0.052
Cost at iteration i=9, j=80: 185.527676
Error rate: 0.052
Cost at iteration i=10, j=0: 184.209907
Error rate: 0.052
Cost at iteration i=10, j=10: 182.847606
Error rate: 0.05
Cost at iteration i=10, j=20: 186.713161
Error rate: 0.048
Cost at iteration i=10, j=30: 189.537910
Error rate: 0.048
Cost at iteration i=10, j=40: 187.874033
Error rate: 0.05
Cost at iteration i=10, j=50: 186.888459
Error rate: 0.051
Cost at iteration i=10, j=60: 185.813637
Error rate: 0.051
Cost at iteration i=10, j=70: 186.124446
Error rate: 0.051
Cost at iteration i=10, j=80: 182.598171
Error rate: 0.048
Cost at iteration i=11, j=0: 181.390407
Error rate: 0.049
Cost at iteration i=11, j=10: 179.902682
Error rate: 0.048
Cost at iteration i=11, j=20: 183.551531
Error rate: 0.046
Cost at iteration i=11, j=30: 186.840411
Error rate: 0.047
Cost at iteration i=11, j=40: 184.704607
Error rate: 0.05
Cost at iteration i=11, j=50: 183.819678
Error rate: 0.05
Cost at iteration i=11, j=60: 182.895176
Error rate: 0.05
Cost at iteration i=11, j=70: 183.191257
Error rate: 0.049
Cost at iteration i=11, j=80: 180.057407
Error rate: 0.046
Cost at iteration i=12, j=0: 178.969525
Error rate: 0.046
Cost at iteration i=12, j=10: 177.529986
Error rate: 0.046
Cost at iteration i=12, j=20: 181.080413
Error rate: 0.045
Cost at iteration i=12, j=30: 184.827745
Error rate: 0.048
Cost at iteration i=12, j=40: 183.223829
Error rate: 0.047
Cost at iteration i=12, j=50: 181.749129
Error rate: 0.05
Cost at iteration i=12, j=60: 180.861449
Error rate: 0.048
Cost at iteration i=12, j=70: 181.409880
Error rate: 0.049
Cost at iteration i=12, j=80: 178.560023
Error rate: 0.045
Cost at iteration i=13, j=0: 177.536875
Error rate: 0.045
Cost at iteration i=13, j=10: 175.983894
Error rate: 0.046
Cost at iteration i=13, j=20: 179.278433
Error rate: 0.047
Cost at iteration i=13, j=30: 183.350944
Error rate: 0.048
Cost at iteration i=13, j=40: 181.790290
Error rate: 0.046
Cost at iteration i=13, j=50: 180.135155
Error rate: 0.046
Cost at iteration i=13, j=60: 179.231137
Error rate: 0.047
Cost at iteration i=13, j=70: 179.819030
Error rate: 0.048
Cost at iteration i=13, j=80: 177.258460
Error rate: 0.045
Cost at iteration i=14, j=0: 176.300448
Error rate: 0.045
Cost at iteration i=14, j=10: 174.655081
Error rate: 0.046
Cost at iteration i=14, j=20: 177.764011
Error rate: 0.047
Cost at iteration i=14, j=30: 182.164916
Error rate: 0.047
Cost at iteration i=14, j=40: 179.467837
Error rate: 0.046
Cost at iteration i=14, j=50: 178.434296
Error rate: 0.046
Cost at iteration i=14, j=60: 177.749812
Error rate: 0.047
Cost at iteration i=14, j=70: 178.101383
Error rate: 0.048
Cost at iteration i=14, j=80: 175.964112
Error rate: 0.045
Cost at iteration i=15, j=0: 175.137622
Error rate: 0.045
Cost at iteration i=15, j=10: 174.055675
Error rate: 0.046
Cost at iteration i=15, j=20: 177.491874
Error rate: 0.048
Cost at iteration i=15, j=30: 181.462279
Error rate: 0.048
Cost at iteration i=15, j=40: 179.109621
Error rate: 0.048
Cost at iteration i=15, j=50: 177.926512
Error rate: 0.045
Cost at iteration i=15, j=60: 177.062015
Error rate: 0.046
Cost at iteration i=15, j=70: 177.377292
Error rate: 0.046
Cost at iteration i=15, j=80: 175.442784
Error rate: 0.045
Cost at iteration i=16, j=0: 174.669743
Error rate: 0.043
Cost at iteration i=16, j=10: 173.582715
Error rate: 0.045
Cost at iteration i=16, j=20: 176.815602
Error rate: 0.047
Cost at iteration i=16, j=30: 180.917445
Error rate: 0.048
Cost at iteration i=16, j=40: 178.591729
Error rate: 0.047
Cost at iteration i=16, j=50: 177.244643
Error rate: 0.044
Cost at iteration i=16, j=60: 176.396782
Error rate: 0.046
Cost at iteration i=16, j=70: 176.655206
Error rate: 0.044
Cost at iteration i=16, j=80: 174.868368
Error rate: 0.042
Cost at iteration i=17, j=0: 174.145824
Error rate: 0.042
Cost at iteration i=17, j=10: 173.084567
Error rate: 0.044
Cost at iteration i=17, j=20: 176.226205
Error rate: 0.047
Cost at iteration i=17, j=30: 180.484942
Error rate: 0.047
Cost at iteration i=17, j=40: 178.334320
Error rate: 0.046
Cost at iteration i=17, j=50: 176.768539
Error rate: 0.043
Cost at iteration i=17, j=60: 175.926128
Error rate: 0.045
Cost at iteration i=17, j=70: 176.240793
Error rate: 0.043
Cost at iteration i=17, j=80: 174.724951
Error rate: 0.042
Cost at iteration i=18, j=0: 174.057592
Error rate: 0.042
Cost at iteration i=18, j=10: 173.017742
Error rate: 0.043
Cost at iteration i=18, j=20: 176.044278
Error rate: 0.047
Cost at iteration i=18, j=30: 180.377109
Error rate: 0.048
Cost at iteration i=18, j=40: 178.412676
Error rate: 0.046
Cost at iteration i=18, j=50: 176.615129
Error rate: 0.043
Cost at iteration i=18, j=60: 175.731898
Error rate: 0.044
Cost at iteration i=18, j=70: 175.990691
Error rate: 0.042
Cost at iteration i=18, j=80: 174.594331
Error rate: 0.042
Cost at iteration i=19, j=0: 173.973139
Error rate: 0.042
Cost at iteration i=19, j=10: 172.899862
Error rate: 0.041
Cost at iteration i=19, j=20: 175.744304
Error rate: 0.045
Cost at iteration i=19, j=30: 180.149565
Error rate: 0.047
Cost at iteration i=19, j=40: 178.430161
Error rate: 0.047
Cost at iteration i=19, j=50: 176.515080
Error rate: 0.045
Cost at iteration i=19, j=60: 175.560622
Error rate: 0.043
Cost at iteration i=19, j=70: 175.770776
Error rate: 0.042
Cost at iteration i=19, j=80: 174.593999
Error rate: 0.042
Final error rate: 0.042
--------------------------------------------------
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Plot-all-likelihoods">4. Plot all likelihoods<a class="anchor-link" href="#4.-Plot-all-likelihoods">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_batch</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_momentum</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;momentum&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_nesterov</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;nesterov&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>





<div id="9379dfd7-dea7-4359-af0c-90dfbeb74218"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#9379dfd7-dea7-4359-af0c-90dfbeb74218');
/* Put everything inside the global mpl namespace */
window.mpl = {};


mpl.get_websocket_type = function() {
    if (typeof(WebSocket) !== 'undefined') {
        return WebSocket;
    } else if (typeof(MozWebSocket) !== 'undefined') {
        return MozWebSocket;
    } else {
        alert('Your browser does not have WebSocket support.' +
              'Please try Chrome, Safari or Firefox ≥ 6. ' +
              'Firefox 4 and 5 are also supported but you ' +
              'have to enable WebSockets in about:config.');
    };
}

mpl.figure = function(figure_id, websocket, ondownload, parent_element) {
    this.id = figure_id;

    this.ws = websocket;

    this.supports_binary = (this.ws.binaryType != undefined);

    if (!this.supports_binary) {
        var warnings = document.getElementById("mpl-warnings");
        if (warnings) {
            warnings.style.display = 'block';
            warnings.textContent = (
                "This browser does not support binary websocket messages. " +
                    "Performance may be slow.");
        }
    }

    this.imageObj = new Image();

    this.context = undefined;
    this.message = undefined;
    this.canvas = undefined;
    this.rubberband_canvas = undefined;
    this.rubberband_context = undefined;
    this.format_dropdown = undefined;

    this.image_mode = 'full';

    this.root = $('<div/>');
    this._root_extra_style(this.root)
    this.root.attr('style', 'display: inline-block');

    $(parent_element).append(this.root);

    this._init_header(this);
    this._init_canvas(this);
    this._init_toolbar(this);

    var fig = this;

    this.waiting = false;

    this.ws.onopen =  function () {
            fig.send_message("supports_binary", {value: fig.supports_binary});
            fig.send_message("send_image_mode", {});
            if (mpl.ratio != 1) {
                fig.send_message("set_dpi_ratio", {'dpi_ratio': mpl.ratio});
            }
            fig.send_message("refresh", {});
        }

    this.imageObj.onload = function() {
            if (fig.image_mode == 'full') {
                // Full images could contain transparency (where diff images
                // almost always do), so we need to clear the canvas so that
                // there is no ghosting.
                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);
            }
            fig.context.drawImage(fig.imageObj, 0, 0);
        };

    this.imageObj.onunload = function() {
        this.ws.close();
    }

    this.ws.onmessage = this._make_on_message_function(this);

    this.ondownload = ondownload;
}

mpl.figure.prototype._init_header = function() {
    var titlebar = $(
        '<div class="ui-dialog-titlebar ui-widget-header ui-corner-all ' +
        'ui-helper-clearfix"/>');
    var titletext = $(
        '<div class="ui-dialog-title" style="width: 100%; ' +
        'text-align: center; padding: 3px;"/>');
    titlebar.append(titletext)
    this.root.append(titlebar);
    this.header = titletext[0];
}



mpl.figure.prototype._canvas_extra_style = function(canvas_div) {

}


mpl.figure.prototype._root_extra_style = function(canvas_div) {

}

mpl.figure.prototype._init_canvas = function() {
    var fig = this;

    var canvas_div = $('<div/>');

    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');

    function canvas_keyboard_event(event) {
        return fig.key_event(event, event['data']);
    }

    canvas_div.keydown('key_press', canvas_keyboard_event);
    canvas_div.keyup('key_release', canvas_keyboard_event);
    this.canvas_div = canvas_div
    this._canvas_extra_style(canvas_div)
    this.root.append(canvas_div);

    var canvas = $('<canvas/>');
    canvas.addClass('mpl-canvas');
    canvas.attr('style', "left: 0; top: 0; z-index: 0; outline: 0")

    this.canvas = canvas[0];
    this.context = canvas[0].getContext("2d");

    var backingStore = this.context.backingStorePixelRatio ||
	this.context.webkitBackingStorePixelRatio ||
	this.context.mozBackingStorePixelRatio ||
	this.context.msBackingStorePixelRatio ||
	this.context.oBackingStorePixelRatio ||
	this.context.backingStorePixelRatio || 1;

    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;

    var rubberband = $('<canvas/>');
    rubberband.attr('style', "position: absolute; left: 0; top: 0; z-index: 1;")

    var pass_mouse_events = true;

    canvas_div.resizable({
        start: function(event, ui) {
            pass_mouse_events = false;
        },
        resize: function(event, ui) {
            fig.request_resize(ui.size.width, ui.size.height);
        },
        stop: function(event, ui) {
            pass_mouse_events = true;
            fig.request_resize(ui.size.width, ui.size.height);
        },
    });

    function mouse_event_fn(event) {
        if (pass_mouse_events)
            return fig.mouse_event(event, event['data']);
    }

    rubberband.mousedown('button_press', mouse_event_fn);
    rubberband.mouseup('button_release', mouse_event_fn);
    // Throttle sequential mouse events to 1 every 20ms.
    rubberband.mousemove('motion_notify', mouse_event_fn);

    rubberband.mouseenter('figure_enter', mouse_event_fn);
    rubberband.mouseleave('figure_leave', mouse_event_fn);

    canvas_div.on("wheel", function (event) {
        event = event.originalEvent;
        event['data'] = 'scroll'
        if (event.deltaY < 0) {
            event.step = 1;
        } else {
            event.step = -1;
        }
        mouse_event_fn(event);
    });

    canvas_div.append(canvas);
    canvas_div.append(rubberband);

    this.rubberband = rubberband;
    this.rubberband_canvas = rubberband[0];
    this.rubberband_context = rubberband[0].getContext("2d");
    this.rubberband_context.strokeStyle = "#000000";

    this._resize_canvas = function(width, height) {
        // Keep the size of the canvas, canvas container, and rubber band
        // canvas in synch.
        canvas_div.css('width', width)
        canvas_div.css('height', height)

        canvas.attr('width', width * mpl.ratio);
        canvas.attr('height', height * mpl.ratio);
        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');

        rubberband.attr('width', width);
        rubberband.attr('height', height);
    }

    // Set the figure to an initial 600x600px, this will subsequently be updated
    // upon first draw.
    this._resize_canvas(600, 600);

    // Disable right mouse context menu.
    $(this.rubberband_canvas).bind("contextmenu",function(e){
        return false;
    });

    function set_focus () {
        canvas.focus();
        canvas_div.focus();
    }

    window.setTimeout(set_focus, 100);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            // put a spacer in here.
            continue;
        }
        var button = $('<button/>');
        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +
                        'ui-button-icon-only');
        button.attr('role', 'button');
        button.attr('aria-disabled', 'false');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);

        var icon_img = $('<span/>');
        icon_img.addClass('ui-button-icon-primary ui-icon');
        icon_img.addClass(image);
        icon_img.addClass('ui-corner-all');

        var tooltip_span = $('<span/>');
        tooltip_span.addClass('ui-button-text');
        tooltip_span.html(tooltip);

        button.append(icon_img);
        button.append(tooltip_span);

        nav_element.append(button);
    }

    var fmt_picker_span = $('<span/>');

    var fmt_picker = $('<select/>');
    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');
    fmt_picker_span.append(fmt_picker);
    nav_element.append(fmt_picker_span);
    this.format_dropdown = fmt_picker[0];

    for (var ind in mpl.extensions) {
        var fmt = mpl.extensions[ind];
        var option = $(
            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);
        fmt_picker.append(option)
    }

    // Add hover states to the ui-buttons
    $( ".ui-button" ).hover(
        function() { $(this).addClass("ui-state-hover");},
        function() { $(this).removeClass("ui-state-hover");}
    );

    var status_bar = $('<span class="mpl-message"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];
}

mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {
    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,
    // which will in turn request a refresh of the image.
    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});
}

mpl.figure.prototype.send_message = function(type, properties) {
    properties['type'] = type;
    properties['figure_id'] = this.id;
    this.ws.send(JSON.stringify(properties));
}

mpl.figure.prototype.send_draw_message = function() {
    if (!this.waiting) {
        this.waiting = true;
        this.ws.send(JSON.stringify({type: "draw", figure_id: this.id}));
    }
}


mpl.figure.prototype.handle_save = function(fig, msg) {
    var format_dropdown = fig.format_dropdown;
    var format = format_dropdown.options[format_dropdown.selectedIndex].value;
    fig.ondownload(fig, format);
}


mpl.figure.prototype.handle_resize = function(fig, msg) {
    var size = msg['size'];
    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {
        fig._resize_canvas(size[0], size[1]);
        fig.send_message("refresh", {});
    };
}

mpl.figure.prototype.handle_rubberband = function(fig, msg) {
    var x0 = msg['x0'] / mpl.ratio;
    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;
    var x1 = msg['x1'] / mpl.ratio;
    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;
    x0 = Math.floor(x0) + 0.5;
    y0 = Math.floor(y0) + 0.5;
    x1 = Math.floor(x1) + 0.5;
    y1 = Math.floor(y1) + 0.5;
    var min_x = Math.min(x0, x1);
    var min_y = Math.min(y0, y1);
    var width = Math.abs(x1 - x0);
    var height = Math.abs(y1 - y0);

    fig.rubberband_context.clearRect(
        0, 0, fig.canvas.width, fig.canvas.height);

    fig.rubberband_context.strokeRect(min_x, min_y, width, height);
}

mpl.figure.prototype.handle_figure_label = function(fig, msg) {
    // Updates the figure title.
    fig.header.textContent = msg['label'];
}

mpl.figure.prototype.handle_cursor = function(fig, msg) {
    var cursor = msg['cursor'];
    switch(cursor)
    {
    case 0:
        cursor = 'pointer';
        break;
    case 1:
        cursor = 'default';
        break;
    case 2:
        cursor = 'crosshair';
        break;
    case 3:
        cursor = 'move';
        break;
    }
    fig.rubberband_canvas.style.cursor = cursor;
}

mpl.figure.prototype.handle_message = function(fig, msg) {
    fig.message.textContent = msg['message'];
}

mpl.figure.prototype.handle_draw = function(fig, msg) {
    // Request the server to send over a new figure.
    fig.send_draw_message();
}

mpl.figure.prototype.handle_image_mode = function(fig, msg) {
    fig.image_mode = msg['mode'];
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Called whenever the canvas gets updated.
    this.send_message("ack", {});
}

// A function to construct a web socket function for onmessage handling.
// Called in the figure constructor.
mpl.figure.prototype._make_on_message_function = function(fig) {
    return function socket_on_message(evt) {
        if (evt.data instanceof Blob) {
            /* FIXME: We get "Resource interpreted as Image but
             * transferred with MIME type text/plain:" errors on
             * Chrome.  But how to set the MIME type?  It doesn't seem
             * to be part of the websocket stream */
            evt.data.type = "image/png";

            /* Free the memory for the previous frames */
            if (fig.imageObj.src) {
                (window.URL || window.webkitURL).revokeObjectURL(
                    fig.imageObj.src);
            }

            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(
                evt.data);
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }
        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == "data:image/png;base64") {
            fig.imageObj.src = evt.data;
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }

        var msg = JSON.parse(evt.data);
        var msg_type = msg['type'];

        // Call the  "handle_{type}" callback, which takes
        // the figure and JSON message as its only arguments.
        try {
            var callback = fig["handle_" + msg_type];
        } catch (e) {
            console.log("No handler for the '" + msg_type + "' message type: ", msg);
            return;
        }

        if (callback) {
            try {
                // console.log("Handling '" + msg_type + "' message: ", msg);
                callback(fig, msg);
            } catch (e) {
                console.log("Exception inside the 'handler_" + msg_type + "' callback:", e, e.stack, msg);
            }
        }
    };
}

// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas
mpl.findpos = function(e) {
    //this section is from http://www.quirksmode.org/js/events_properties.html
    var targ;
    if (!e)
        e = window.event;
    if (e.target)
        targ = e.target;
    else if (e.srcElement)
        targ = e.srcElement;
    if (targ.nodeType == 3) // defeat Safari bug
        targ = targ.parentNode;

    // jQuery normalizes the pageX and pageY
    // pageX,Y are the mouse positions relative to the document
    // offset() returns the position of the element relative to the document
    var x = e.pageX - $(targ).offset().left;
    var y = e.pageY - $(targ).offset().top;

    return {"x": x, "y": y};
};

/*
 * return a copy of an object with only non-object keys
 * we need this to avoid circular references
 * http://stackoverflow.com/a/24161582/3208463
 */
function simpleKeys (original) {
  return Object.keys(original).reduce(function (obj, key) {
    if (typeof original[key] !== 'object')
        obj[key] = original[key]
    return obj;
  }, {});
}

mpl.figure.prototype.mouse_event = function(event, name) {
    var canvas_pos = mpl.findpos(event)

    if (name === 'button_press')
    {
        this.canvas.focus();
        this.canvas_div.focus();
    }

    var x = canvas_pos.x * mpl.ratio;
    var y = canvas_pos.y * mpl.ratio;

    this.send_message(name, {x: x, y: y, button: event.button,
                             step: event.step,
                             guiEvent: simpleKeys(event)});

    /* This prevents the web browser from automatically changing to
     * the text insertion cursor when the button is pressed.  We want
     * to control all of the cursor setting manually through the
     * 'cursor' event from matplotlib */
    event.preventDefault();
    return false;
}

mpl.figure.prototype._key_event_extra = function(event, name) {
    // Handle any extra behaviour associated with a key event
}

mpl.figure.prototype.key_event = function(event, name) {

    // Prevent repeat events
    if (name == 'key_press')
    {
        if (event.which === this._key)
            return;
        else
            this._key = event.which;
    }
    if (name == 'key_release')
        this._key = null;

    var value = '';
    if (event.ctrlKey && event.which != 17)
        value += "ctrl+";
    if (event.altKey && event.which != 18)
        value += "alt+";
    if (event.shiftKey && event.which != 16)
        value += "shift+";

    value += 'k';
    value += event.which.toString();

    this._key_event_extra(event, name);

    this.send_message(name, {key: value,
                             guiEvent: simpleKeys(event)});
    return false;
}

mpl.figure.prototype.toolbar_button_onclick = function(name) {
    if (name == 'download') {
        this.handle_save(this, null);
    } else {
        this.send_message("toolbar_button", {name: name});
    }
};

mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {
    this.message.textContent = tooltip;
};
mpl.toolbar_items = [["Home", "Reset original view", "fa fa-home icon-home", "home"], ["Back", "Back to  previous view", "fa fa-arrow-left icon-arrow-left", "back"], ["Forward", "Forward to next view", "fa fa-arrow-right icon-arrow-right", "forward"], ["", "", "", ""], ["Pan", "Pan axes with left mouse, zoom with right", "fa fa-arrows icon-move", "pan"], ["Zoom", "Zoom to rectangle", "fa fa-square-o icon-check-empty", "zoom"], ["", "", "", ""], ["Download", "Download plot", "fa fa-floppy-o icon-save", "download"]];

mpl.extensions = ["eps", "jpeg", "pdf", "png", "ps", "raw", "svg", "tif"];

mpl.default_extension = "png";var comm_websocket_adapter = function(comm) {
    // Create a "websocket"-like object which calls the given IPython comm
    // object with the appropriate methods. Currently this is a non binary
    // socket, so there is still some room for performance tuning.
    var ws = {};

    ws.close = function() {
        comm.close()
    };
    ws.send = function(m) {
        //console.log('sending', m);
        comm.send(m);
    };
    // Register the callback with on_msg.
    comm.on_msg(function(msg) {
        //console.log('receiving', msg['content']['data'], msg);
        // Pass the mpl event to the overriden (by mpl) onmessage function.
        ws.onmessage(msg['content']['data'])
    });
    return ws;
}

mpl.mpl_figure_comm = function(comm, msg) {
    // This is the function which gets called when the mpl process
    // starts-up an IPython Comm through the "matplotlib" channel.

    var id = msg.content.data.id;
    // Get hold of the div created by the display call when the Comm
    // socket was opened in Python.
    var element = $("#" + id);
    var ws_proxy = comm_websocket_adapter(comm)

    function ondownload(figure, format) {
        window.open(figure.imageObj.src);
    }

    var fig = new mpl.figure(id, ws_proxy,
                           ondownload,
                           element.get(0));

    // Call onopen now - mpl needs it, as it is assuming we've passed it a real
    // web socket which is closed, not our websocket->open comm proxy.
    ws_proxy.onopen();

    fig.parent_element = element.get(0);
    fig.cell_info = mpl.find_output_cell("<div id='" + id + "'></div>");
    if (!fig.cell_info) {
        console.error("Failed to find cell for figure", id, fig);
        return;
    }

    var output_index = fig.cell_info[2]
    var cell = fig.cell_info[0];

};

mpl.figure.prototype.handle_close = function(fig, msg) {
    var width = fig.canvas.width/mpl.ratio
    fig.root.unbind('remove')

    // Update the output cell to use the data from the current canvas.
    fig.push_to_output();
    var dataURL = fig.canvas.toDataURL();
    // Re-enable the keyboard manager in IPython - without this line, in FF,
    // the notebook keyboard shortcuts fail.
    IPython.keyboard_manager.enable()
    $(fig.parent_element).html('<img src="' + dataURL + '" width="' + width + '">');
    fig.close_ws(fig, msg);
}

mpl.figure.prototype.close_ws = function(fig, msg){
    fig.send_message('closing', msg);
    // fig.ws.close()
}

mpl.figure.prototype.push_to_output = function(remove_interactive) {
    // Turn the data on the canvas into data in the output cell.
    var width = this.canvas.width/mpl.ratio
    var dataURL = this.canvas.toDataURL();
    this.cell_info[1]['text/html'] = '<img src="' + dataURL + '" width="' + width + '">';
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Tell IPython that the notebook contents must change.
    IPython.notebook.set_dirty(true);
    this.send_message("ack", {});
    var fig = this;
    // Wait a second, then push the new image to the DOM so
    // that it is saved nicely (might be nice to debounce this).
    setTimeout(function () { fig.push_to_output() }, 1000);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items){
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) { continue; };

        var button = $('<button class="btn btn-default" href="#" title="' + name + '"><i class="fa ' + image + ' fa-lg"></i></button>');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);
        nav_element.append(button);
    }

    // Add the status bar.
    var status_bar = $('<span class="mpl-message" style="text-align:right; float: right;"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];

    // Add the close button to the window.
    var buttongrp = $('<div class="btn-group inline pull-right"></div>');
    var button = $('<button class="btn btn-mini btn-primary" href="#" title="Stop Interaction"><i class="fa fa-power-off icon-remove icon-large"></i></button>');
    button.click(function (evt) { fig.handle_close(fig, {}); } );
    button.mouseover('Stop Interaction', toolbar_mouse_event);
    buttongrp.append(button);
    var titlebar = this.root.find($('.ui-dialog-titlebar'));
    titlebar.prepend(buttongrp);
}

mpl.figure.prototype._root_extra_style = function(el){
    var fig = this
    el.on("remove", function(){
	fig.close_ws(fig, {});
    });
}

mpl.figure.prototype._canvas_extra_style = function(el){
    // this is important to make the div 'focusable
    el.attr('tabindex', 0)
    // reach out to IPython and tell the keyboard manager to turn it's self
    // off when our div gets focus

    // location in version 3
    if (IPython.notebook.keyboard_manager) {
        IPython.notebook.keyboard_manager.register_events(el);
    }
    else {
        // location in version 2
        IPython.keyboard_manager.register_events(el);
    }

}

mpl.figure.prototype._key_event_extra = function(event, name) {
    var manager = IPython.notebook.keyboard_manager;
    if (!manager)
        manager = IPython.keyboard_manager;

    // Check for shift+enter
    if (event.shiftKey && event.which == 13) {
        this.canvas_div.blur();
        // select the cell after this one
        var index = IPython.notebook.find_cell_index(this.cell_info[0]);
        IPython.notebook.select(index + 1);
    }
}

mpl.figure.prototype.handle_save = function(fig, msg) {
    fig.ondownload(fig, null);
}


mpl.find_output_cell = function(html_output) {
    // Return the cell and output element which can be found *uniquely* in the notebook.
    // Note - this is a bit hacky, but it is done because the "notebook_saving.Notebook"
    // IPython event is triggered only after the cells have been serialised, which for
    // our purposes (turning an active figure into a static one), is too late.
    var cells = IPython.notebook.get_cells();
    var ncells = cells.length;
    for (var i=0; i<ncells; i++) {
        var cell = cells[i];
        if (cell.cell_type === 'code'){
            for (var j=0; j<cell.output_area.outputs.length; j++) {
                var data = cell.output_area.outputs[j];
                if (data.data) {
                    // IPython >= 3 moved mimebundle to data attribute of output
                    data = data.data;
                }
                if (data['text/html'] == html_output) {
                    return [cell, data, j];
                }
            }
        }
    }
}

// Register the function which deals with the matplotlib target/channel.
// The kernel may be null if the page has been refreshed.
if (IPython.notebook.kernel != null) {
    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);
}

</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<img src="data:image/png;base64,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" width="640">
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></br></p>
<h1 id="4.-Variable-and-Adaptive-Learning-Rates">4. Variable and Adaptive Learning Rates<a class="anchor-link" href="#4.-Variable-and-Adaptive-Learning-Rates">&#182;</a></h1><p>Momentum is generally looked at as the most impactful method when it comes to speeding up training. If you compare the loss per iteration between standard gradient descent, and gradient descent with momentum, the difference is huge!</p>
<p>Additionally, momentum is nice because you don't have to play with the hyper parameters very much. You can just set it to 0.9 and it is usually fine, resulting in huge performance gains. However, some adaptive learning techniques are very powerful, so let's take a look!</p>
<h2 id="4.1-Variable-Learning-Rates">4.1 Variable Learning Rates<a class="anchor-link" href="#4.1-Variable-Learning-Rates">&#182;</a></h2><p>The first new learning rate we will look at is a <strong>variable learning rate</strong>, e.g. a learning rate that is a function of time, $\eta(t)$. This is sometimes referred to as "Learning rate scheduling". The first one worth mentioning is:</p>
<h3 id="4.1.1-Step-Decay">4.1.1 Step Decay<a class="anchor-link" href="#4.1.1-Step-Decay">&#182;</a></h3><p>Periodically, say ever 100 iterations, we will reduce the learning rate by a constant factor. For example, if we constantly divide by 2, we will half it each time. <br></p>
<h3 id="4.1.2-Exponential-Decay">4.1.2 Exponential Decay<a class="anchor-link" href="#4.1.2-Exponential-Decay">&#182;</a></h3><p>In this method the learning rate follows and exponential curve:</p>
$$\eta(t) = A * e^{-kt}$$<p><img src="https://drive.google.com/uc?id=1-8lpPA4qhPejHlLpAMZ9nyfLSwvY9QqX"></p>
<h3 id="4.1.3-$\frac{1}{t}$-decay">4.1.3 $\frac{1}{t}$ decay<a class="anchor-link" href="#4.1.3-$\frac{1}{t}$-decay">&#182;</a></h3><p>Here the learning rate will decay proportionally to $\frac{1}{t}$:
$$\eta(t) = \frac{A}{kt + 1 }$$
In this case the dropoff is slower than exponential decay.</p>
<p><img src="https://drive.google.com/uc?id=1hlgb8CBJmkY1Odc3q2TqOQ80c0FQQfmA"></p>
<h3 id="4.1.4-What-do-all-of-these-methods-have-in-common?">4.1.4 What do all of these methods have in common?<a class="anchor-link" href="#4.1.4-What-do-all-of-these-methods-have-in-common?">&#182;</a></h3><p>Well, each of these methods will decrease the learning rate as time goes on (i.e. the number of iterations increases). The question comes up, why would be want to do that? Well, generally speaking, when we initialize the weights of a neural network, they are going to be very far from the optimal weights, so it is good to start with a large learning rate so that we can take bigger steps towards the goal. This is the motivation behind momentum as well: We want to pick up speed by accumulating past gradients, because we know that if we are very far away from our goal, then those gradients should be large. However, as we get close to the goal, the gradient is going to shrink. In fact, by definition, the minimum of a function means the gradient must be 0-that is how we solve for the minimum of a function in calculus.</p>
<p>But why may we want to slow down as we get close to the minimum? Well, when you are too close to the minimum and you take too big of a step, you are going to overshoot the minimum. So what ends up happening is that you just bounce back and forth. In fact, if you learning rate is too large then you will just bounce right of the valley and your loss may increase! So, in order to reduce all of this bouncing around, we would like to take smaller steps.</p>
<p>One other things to think about is the following caveat: all of these methods mean that there are more <strong>hyperparameters</strong> to optimize. This adds more work to your plate as a data scientist.</p>
<hr>
<h2 id="4.2-Adaptive-Learning-Rates">4.2 Adaptive Learning Rates<a class="anchor-link" href="#4.2-Adaptive-Learning-Rates">&#182;</a></h2><p><br></p>
<p><img src="https://drive.google.com/uc?id=1NCJNJNvEgtONMNTRCvkjEOT7k2r7Z_n3" width="500"></p>
<p><br></p>
<h3 id="4.2.1-AdaGrad">4.2.1 AdaGrad<a class="anchor-link" href="#4.2.1-AdaGrad">&#182;</a></h3><p>The first adaptive learning technique we will go over is <strong>AdaGrad</strong>. The main idea behind it is this: we cannot expect the dependence of the cost on each of the parameters to be the same. In other words, in one direction the gradient may be very steep, but in another direction the gradient may be very flat. So, perhaps it may be beneficial to adapt the learning rate for each parameter individually, based on how much it has changed in the past.</p>
<p>So, in AdaGrad what we do is introduce a variable called the <strong>cache</strong>.</p>
$$cache = cache + gradient^2$$$$w \leftarrow w - \eta \frac{\nabla J}{\sqrt{cache + \epsilon }}$$<p>Each parameter of the neural network has its own cache, so for example if you have one weight matrix of size (3 x 4), you will also have a cache matrix of size (3 x 4). The same thing applies to the bias vectors.</p>
<p>The idea behind the cache is that it is going to accumulate the squared gradients. Because we are squaring the gradients, the cache is always going to be positive. And because each parameter has its own cache, then if one parameter has had a lot of large gradients in the past, then its cache will be very large, and its effective learning rate will be very small, and it will change more slowly in the future. On the other hand, if a parameter has had a lot of small gradients in the past then it's cache will be small, so its respective learning rate will remain large, and it will have more opportunity to change in the future.</p>
<p>One small thing to keep in mind is that we usually add a small number $\epsilon$ to the denominator in order to prevent dividing by 0. This is generally set to $10^{-8}$ or $10^{-10}$.</p>
<p>One important thing to stress about AdaGrad is that everything we are doing is an element-wise operation. In other words, each scalar parameter and its learning rate is effectively updated independently of the others. So you can look at the rules that we presented as scalar updates that apply to all of the parameters, or you can think of one huge parameter vector that contains all of the neural network parameters, and that each of the operations is an element wise operation.</p>
<h3 id="4.2.2-RMSProp">4.2.2 RMSProp<a class="anchor-link" href="#4.2.2-RMSProp">&#182;</a></h3><p>This next technique builds on the fact that researchers have found that AdaGrad decreases the learning rate too aggresively. In this case, the learning rate would approach 0 too quickly, when in fact there was still learning to be done. The method introduced to fix this is called <strong>RMSProp</strong>, and it was introduced by Geoff Hinton and team. It works as follows:</p>
<p>The reason that AdaGrad decrease the learning rate too quickly is because the cache is growing too fast. So in order to make the cache grow more slowly, we actually decrease it each time we update it. This is done by taking a weighted average of the old cache and the new squared gradient. We call this weight the decay weight, and we can see that the two weights add up to 1.</p>
$$cache = decay*cache +(1 - decay)*gradient^2$$<p>Typical values for the decay are 0.99, 0.999, etc. The intuition for these choices will be discussed in later lectures. By doing this, we say that we are making the cache "leaky". The reasoning it is leaking is because we can imagine that if we had 0 gradient for a long time, eventually the cache would shrink back down to 0, because it would be decreased by the decay rate on each round</p>
<p>Note that there is some ambiguity in the RMSProp and AdaGrad algorithms. Specifically, this can be seen in the context of RMSProp. The ambiguity arises from the initial value of the cache. You may automatically assume that 0 is a good value, but this actually has a very strange effect. We can let:
$$decay = 0.99$$ 
And that means our initial update for the cache is:</p>
$$0.001g^2$$<p>And hence our initial update is (ignoring epsilon): 
$$\frac{\eta g}{\sqrt{0.001g^2}}$$
Which ends up being very large, because the denominator is very small. What you would have to do is compensate by making you initial learning rate smaller than usual. One solution to this however, is just to set the cache to 1 instead. By manipulating the RMSProp update, we can show that this is what we would get if we were not doing RMSProp at all:</p>
$$\nabla w = \eta \frac{g}{\sqrt{1 + 0.001 g^2}} \approx \eta g$$<p>You may still be wondering which way is the correct way to go about things? Well, in <strong>TensorFlow</strong> the cache is initialized to 1. In <strong>Keras</strong> the cache is initialized to 0.</p>
<h2 id="4.3-Summary-Pseudocode">4.3 Summary Pseudocode<a class="anchor-link" href="#4.3-Summary-Pseudocode">&#182;</a></h2><p><strong>AdaGrad</strong></p>

<pre><code># at every batch
cache += gradient ** 2
param = param - learning_rate * gradient / sqrt(cache + epsilon)</code></pre>
<p><strong>RMSProp</strong></p>

<pre><code># at every batch
cache = decay * cache + (1 - decay) * gradient **2
param = param - learning_rate * gradient / sqrt(cache + epsilon)</code></pre>
<p><strong>Note:</strong>
Epsilon is generally 10e-8, 10e-9, 10e-10, and decay is generally 0.9, 0.99, 0.999</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="5.-Constant-Learning-Rate-vs.-RMSProp-in-Code">5. Constant Learning Rate vs. RMSProp in Code<a class="anchor-link" href="#5.-Constant-Learning-Rate-vs.-RMSProp-in-Code">&#182;</a></h1><p>Here we are going to be comparing RMSProp against a constant learning rate. We can start by defining our standard multi-layer perceptron functions:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">Z</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>               <span class="c1"># using ReLU function</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">expA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">expA</span> <span class="o">/</span> <span class="n">expA</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span>

<span class="k">def</span> <span class="nf">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivative_b2</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivative_w1</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span> <span class="p">(</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>      <span class="c1"># for relu</span>

<span class="k">def</span> <span class="nf">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">W2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>       <span class="c1"># for relu</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now for our imports.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">shuffle</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">util</span> <span class="k">import</span> <span class="n">get_normalized_data</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y2indicator</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can start our main function:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">20</span>    
    <span class="n">print_period</span> <span class="o">=</span> <span class="mi">10</span>
    
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_normalized_data</span><span class="p">()</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00004</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span>
    
    <span class="c1"># get train/test data</span>
    <span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">,]</span>
    <span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">]</span>
    <span class="n">Xtest</span>  <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:,]</span>
    <span class="n">Ytest</span>  <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:]</span>
    <span class="n">Ytrain_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytrain</span><span class="p">)</span>
    <span class="n">Ytest_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytest</span><span class="p">)</span>
    
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">batch_sz</span>
    
    <span class="c1"># 300 hidden units, 10 output classes, then initialize our weights</span>
    <span class="n">M</span> <span class="o">=</span> <span class="mi">300</span>
    <span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="o">/</span> <span class="mi">28</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    
    <span class="c1"># --------------------- 1. Constant Learning Rate -----------------------------</span>
    <span class="c1"># cost = -16</span>
    <span class="n">LL_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">CR_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>         <span class="c1"># looping through total iterations</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>    <span class="c1"># looping through number of batches </span>
            <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
            <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
            <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>

            <span class="c1"># weight and bias updates</span>
            <span class="n">W2</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span><span class="p">)</span>
            <span class="n">b2</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span><span class="p">)</span>
            <span class="n">W1</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span><span class="p">)</span>
            <span class="n">b1</span> <span class="o">-=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span><span class="p">)</span>
            
            <span class="c1"># print/debugging step</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate just for LL</span>
                <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
                <span class="c1"># print &quot;pY:&quot;, pY</span>
                <span class="n">ll</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
                <span class="n">LL_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">ll</span><span class="p">))</span>

                <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
                <span class="n">CR_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>

    <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------- END --------------------------&#39;</span><span class="p">)</span>


    <span class="c1"># --------------------- 2. RMSProp -----------------------------</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="o">/</span> <span class="mi">28</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">LL_rms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">CR_rms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lr0</span> <span class="o">=</span> <span class="mf">0.001</span>         <span class="c1"># Initial Learning Rate. If you set this too high you&#39;ll get NaN!</span>
    <span class="n">cache_W2</span> <span class="o">=</span> <span class="mi">1</span>        <span class="c1"># storing cache for each of our weights</span>
    <span class="n">cache_b2</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">cache_W1</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">cache_b1</span> <span class="o">=</span> <span class="mi">1</span>  
    <span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.999</span>        <span class="c1"># our decay rate parameter and epsilon</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-10</span>
    
    <span class="c1"># calculate batches in same way </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
            <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
            <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
            <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>              <span class="c1"># forward pass</span>
            
            <span class="c1"># weight and bias updates, WITH RMSProp</span>
            <span class="n">gW2</span> <span class="o">=</span> <span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span>
            <span class="n">cache_W2</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_W2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gW2</span><span class="o">*</span><span class="n">gW2</span> <span class="c1"># elem by elem mult</span>
            <span class="n">W2</span> <span class="o">-=</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gW2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    
            <span class="n">gb2</span> <span class="o">=</span> <span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span>
            <span class="n">cache_b2</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_b2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gb2</span><span class="o">*</span><span class="n">gb2</span>
            <span class="n">b2</span> <span class="o">-=</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gb2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_b2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

            <span class="n">gW1</span> <span class="o">=</span> <span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span>
            <span class="n">cache_W1</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_W1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gW1</span><span class="o">*</span><span class="n">gW1</span>
            <span class="n">W1</span> <span class="o">-=</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gW1</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

            <span class="n">gb1</span> <span class="o">=</span> <span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span>
            <span class="n">cache_b1</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_b1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gb1</span><span class="o">*</span><span class="n">gb1</span>
            <span class="n">b1</span> <span class="o">-=</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gb1</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_b1</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># calculate just for LL</span>
                <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
                <span class="c1"># print &quot;pY:&quot;, pY</span>
                <span class="n">ll</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
                <span class="n">LL_rms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">ll</span><span class="p">))</span>

                <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
                <span class="n">CR_rms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>

    <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------- END --------------------------&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">LL_batch</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;const&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">LL_rms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;rms&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">main</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Reading in and transforming data...
Cost at iteration i=0, j=0: 2349.120009
Error rate: 0.845
Cost at iteration i=0, j=10: 1754.519175
Error rate: 0.471
Cost at iteration i=0, j=20: 1385.450313
Error rate: 0.309
Cost at iteration i=0, j=30: 1148.961005
Error rate: 0.237
Cost at iteration i=0, j=40: 992.222950
Error rate: 0.197
Cost at iteration i=0, j=50: 879.998396
Error rate: 0.172
Cost at iteration i=0, j=60: 798.245900
Error rate: 0.16
Cost at iteration i=0, j=70: 731.509100
Error rate: 0.147
Cost at iteration i=0, j=80: 678.656807
Error rate: 0.136
Cost at iteration i=1, j=0: 669.401911
Error rate: 0.136
Cost at iteration i=1, j=10: 628.291571
Error rate: 0.129
Cost at iteration i=1, j=20: 594.981241
Error rate: 0.126
Cost at iteration i=1, j=30: 566.042122
Error rate: 0.121
Cost at iteration i=1, j=40: 541.672213
Error rate: 0.116
Cost at iteration i=1, j=50: 521.289063
Error rate: 0.113
Cost at iteration i=1, j=60: 505.045671
Error rate: 0.113
Cost at iteration i=1, j=70: 487.926051
Error rate: 0.107
Cost at iteration i=1, j=80: 473.050332
Error rate: 0.105
Cost at iteration i=2, j=0: 470.455385
Error rate: 0.104
Cost at iteration i=2, j=10: 458.069777
Error rate: 0.1
Cost at iteration i=2, j=20: 447.364195
Error rate: 0.098
Cost at iteration i=2, j=30: 437.065791
Error rate: 0.097
Cost at iteration i=2, j=40: 427.512263
Error rate: 0.095
Cost at iteration i=2, j=50: 419.510999
Error rate: 0.096
Cost at iteration i=2, j=60: 413.146741
Error rate: 0.094
Cost at iteration i=2, j=70: 405.276953
Error rate: 0.093
Cost at iteration i=2, j=80: 397.614660
Error rate: 0.092
Cost at iteration i=3, j=0: 396.398931
Error rate: 0.091
Cost at iteration i=3, j=10: 390.339230
Error rate: 0.09
Cost at iteration i=3, j=20: 385.187041
Error rate: 0.091
Cost at iteration i=3, j=30: 379.791654
Error rate: 0.092
Cost at iteration i=3, j=40: 374.399247
Error rate: 0.088
Cost at iteration i=3, j=50: 369.950091
Error rate: 0.089
Cost at iteration i=3, j=60: 366.506478
Error rate: 0.092
Cost at iteration i=3, j=70: 361.983055
Error rate: 0.088
Cost at iteration i=3, j=80: 356.767986
Error rate: 0.087
Cost at iteration i=4, j=0: 356.031076
Error rate: 0.087
Cost at iteration i=4, j=10: 352.154249
Error rate: 0.087
Cost at iteration i=4, j=20: 349.076297
Error rate: 0.082
Cost at iteration i=4, j=30: 345.712119
Error rate: 0.084
Cost at iteration i=4, j=40: 342.068517
Error rate: 0.081
Cost at iteration i=4, j=50: 339.084530
Error rate: 0.084
Cost at iteration i=4, j=60: 336.841254
Error rate: 0.083
Cost at iteration i=4, j=70: 333.645503
Error rate: 0.081
Cost at iteration i=4, j=80: 329.657069
Error rate: 0.08
Cost at iteration i=5, j=0: 329.163244
Error rate: 0.081
Cost at iteration i=5, j=10: 326.244010
Error rate: 0.079
Cost at iteration i=5, j=20: 324.280580
Error rate: 0.077
Cost at iteration i=5, j=30: 321.963438
Error rate: 0.076
Cost at iteration i=5, j=40: 319.203727
Error rate: 0.076
Cost at iteration i=5, j=50: 317.086903
Error rate: 0.077
Cost at iteration i=5, j=60: 315.568431
Error rate: 0.075
Cost at iteration i=5, j=70: 313.234177
Error rate: 0.077
Cost at iteration i=5, j=80: 309.942642
Error rate: 0.077
Cost at iteration i=6, j=0: 309.573070
Error rate: 0.076
Cost at iteration i=6, j=10: 307.227866
Error rate: 0.074
Cost at iteration i=6, j=20: 305.849738
Error rate: 0.072
Cost at iteration i=6, j=30: 304.106693
Error rate: 0.072
Cost at iteration i=6, j=40: 301.783900
Error rate: 0.071
Cost at iteration i=6, j=50: 300.217864
Error rate: 0.07
Cost at iteration i=6, j=60: 299.164516
Error rate: 0.07
Cost at iteration i=6, j=70: 297.368127
Error rate: 0.07
Cost at iteration i=6, j=80: 294.527322
Error rate: 0.069
Cost at iteration i=7, j=0: 294.229376
Error rate: 0.069
Cost at iteration i=7, j=10: 292.225742
Error rate: 0.069
Cost at iteration i=7, j=20: 291.221258
Error rate: 0.069
Cost at iteration i=7, j=30: 289.825023
Error rate: 0.068
Cost at iteration i=7, j=40: 287.797451
Error rate: 0.068
Cost at iteration i=7, j=50: 286.544210
Error rate: 0.067
Cost at iteration i=7, j=60: 285.812886
Error rate: 0.066
Cost at iteration i=7, j=70: 284.383791
Error rate: 0.067
Cost at iteration i=7, j=80: 281.858567
Error rate: 0.066
Cost at iteration i=8, j=0: 281.620037
Error rate: 0.067
Cost at iteration i=8, j=10: 279.833578
Error rate: 0.066
Cost at iteration i=8, j=20: 279.075078
Error rate: 0.066
Cost at iteration i=8, j=30: 277.902222
Error rate: 0.066
Cost at iteration i=8, j=40: 276.081607
Error rate: 0.066
Cost at iteration i=8, j=50: 275.027323
Error rate: 0.065
Cost at iteration i=8, j=60: 274.535374
Error rate: 0.064
Cost at iteration i=8, j=70: 273.375743
Error rate: 0.065
Cost at iteration i=8, j=80: 271.077846
Error rate: 0.065
Cost at iteration i=9, j=0: 270.874604
Error rate: 0.065
Cost at iteration i=9, j=10: 269.256345
Error rate: 0.064
Cost at iteration i=9, j=20: 268.659399
Error rate: 0.065
Cost at iteration i=9, j=30: 267.641776
Error rate: 0.065
Cost at iteration i=9, j=40: 265.974517
Error rate: 0.065
Cost at iteration i=9, j=50: 265.114531
Error rate: 0.064
Cost at iteration i=9, j=60: 264.781330
Error rate: 0.064
Cost at iteration i=9, j=70: 263.823098
Error rate: 0.065
Cost at iteration i=9, j=80: 261.703984
Error rate: 0.064
Cost at iteration i=10, j=0: 261.532235
Error rate: 0.064
Cost at iteration i=10, j=10: 260.047354
Error rate: 0.064
Cost at iteration i=10, j=20: 259.581783
Error rate: 0.064
Cost at iteration i=10, j=30: 258.692815
Error rate: 0.063
Cost at iteration i=10, j=40: 257.132174
Error rate: 0.063
Cost at iteration i=10, j=50: 256.431424
Error rate: 0.063
Cost at iteration i=10, j=60: 256.227024
Error rate: 0.062
Cost at iteration i=10, j=70: 255.414449
Error rate: 0.063
Cost at iteration i=10, j=80: 253.445438
Error rate: 0.063
Cost at iteration i=11, j=0: 253.309405
Error rate: 0.063
Cost at iteration i=11, j=10: 251.914861
Error rate: 0.063
Cost at iteration i=11, j=20: 251.538425
Error rate: 0.062
Cost at iteration i=11, j=30: 250.758485
Error rate: 0.062
Cost at iteration i=11, j=40: 249.292771
Error rate: 0.062
Cost at iteration i=11, j=50: 248.730466
Error rate: 0.062
Cost at iteration i=11, j=60: 248.621070
Error rate: 0.062
Cost at iteration i=11, j=70: 247.909256
Error rate: 0.063
Cost at iteration i=11, j=80: 246.072094
Error rate: 0.063
Cost at iteration i=12, j=0: 245.959406
Error rate: 0.062
Cost at iteration i=12, j=10: 244.651365
Error rate: 0.062
Cost at iteration i=12, j=20: 244.369165
Error rate: 0.062
Cost at iteration i=12, j=30: 243.672257
Error rate: 0.063
Cost at iteration i=12, j=40: 242.287605
Error rate: 0.063
Cost at iteration i=12, j=50: 241.832745
Error rate: 0.062
Cost at iteration i=12, j=60: 241.807813
Error rate: 0.062
Cost at iteration i=12, j=70: 241.186960
Error rate: 0.062
Cost at iteration i=12, j=80: 239.443943
Error rate: 0.062
Cost at iteration i=13, j=0: 239.350245
Error rate: 0.062
Cost at iteration i=13, j=10: 238.121796
Error rate: 0.062
Cost at iteration i=13, j=20: 237.907680
Error rate: 0.063
Cost at iteration i=13, j=30: 237.280906
Error rate: 0.063
Cost at iteration i=13, j=40: 235.971844
Error rate: 0.063
Cost at iteration i=13, j=50: 235.608783
Error rate: 0.063
Cost at iteration i=13, j=60: 235.655442
Error rate: 0.064
Cost at iteration i=13, j=70: 235.113416
Error rate: 0.062
Cost at iteration i=13, j=80: 233.443128
Error rate: 0.063
Cost at iteration i=14, j=0: 233.362237
Error rate: 0.063
Cost at iteration i=14, j=10: 232.192020
Error rate: 0.063
Cost at iteration i=14, j=20: 232.028394
Error rate: 0.061
Cost at iteration i=14, j=30: 231.462431
Error rate: 0.063
Cost at iteration i=14, j=40: 230.213531
Error rate: 0.062
Cost at iteration i=14, j=50: 229.922751
Error rate: 0.062
Cost at iteration i=14, j=60: 230.033767
Error rate: 0.063
Cost at iteration i=14, j=70: 229.556634
Error rate: 0.062
Cost at iteration i=14, j=80: 227.953639
Error rate: 0.062
Cost at iteration i=15, j=0: 227.887345
Error rate: 0.062
Cost at iteration i=15, j=10: 226.764677
Error rate: 0.063
Cost at iteration i=15, j=20: 226.646583
Error rate: 0.062
Cost at iteration i=15, j=30: 226.132476
Error rate: 0.062
Cost at iteration i=15, j=40: 224.935946
Error rate: 0.062
Cost at iteration i=15, j=50: 224.704294
Error rate: 0.061
Cost at iteration i=15, j=60: 224.861129
Error rate: 0.062
Cost at iteration i=15, j=70: 224.449084
Error rate: 0.063
Cost at iteration i=15, j=80: 222.907183
Error rate: 0.062
Cost at iteration i=16, j=0: 222.851999
Error rate: 0.061
Cost at iteration i=16, j=10: 221.777287
Error rate: 0.061
Cost at iteration i=16, j=20: 221.706764
Error rate: 0.059
Cost at iteration i=16, j=30: 221.234214
Error rate: 0.059
Cost at iteration i=16, j=40: 220.073308
Error rate: 0.06
Cost at iteration i=16, j=50: 219.887304
Error rate: 0.06
Cost at iteration i=16, j=60: 220.102443
Error rate: 0.06
Cost at iteration i=16, j=70: 219.746461
Error rate: 0.06
Cost at iteration i=16, j=80: 218.241344
Error rate: 0.059
Cost at iteration i=17, j=0: 218.201380
Error rate: 0.06
Cost at iteration i=17, j=10: 217.168483
Error rate: 0.06
Cost at iteration i=17, j=20: 217.131525
Error rate: 0.058
Cost at iteration i=17, j=30: 216.694513
Error rate: 0.059
Cost at iteration i=17, j=40: 215.575683
Error rate: 0.06
Cost at iteration i=17, j=50: 215.423832
Error rate: 0.059
Cost at iteration i=17, j=60: 215.680775
Error rate: 0.06
Cost at iteration i=17, j=70: 215.369086
Error rate: 0.059
Cost at iteration i=17, j=80: 213.899064
Error rate: 0.058
Cost at iteration i=18, j=0: 213.865263
Error rate: 0.057
Cost at iteration i=18, j=10: 212.873672
Error rate: 0.057
Cost at iteration i=18, j=20: 212.870456
Error rate: 0.055
Cost at iteration i=18, j=30: 212.466931
Error rate: 0.055
Cost at iteration i=18, j=40: 211.387176
Error rate: 0.056
Cost at iteration i=18, j=50: 211.272768
Error rate: 0.056
Cost at iteration i=18, j=60: 211.560197
Error rate: 0.055
Cost at iteration i=18, j=70: 211.281060
Error rate: 0.055
Cost at iteration i=18, j=80: 209.848217
Error rate: 0.056
Cost at iteration i=19, j=0: 209.825195
Error rate: 0.056
Cost at iteration i=19, j=10: 208.869018
Error rate: 0.054
Cost at iteration i=19, j=20: 208.894951
Error rate: 0.054
Cost at iteration i=19, j=30: 208.524304
Error rate: 0.054
Cost at iteration i=19, j=40: 207.467373
Error rate: 0.054
Cost at iteration i=19, j=50: 207.382990
Error rate: 0.054
Cost at iteration i=19, j=60: 207.699016
Error rate: 0.053
Cost at iteration i=19, j=70: 207.456084
Error rate: 0.055
Cost at iteration i=19, j=80: 206.060349
Error rate: 0.055
Final error rate: 0.054
-------------------- END --------------------------
Cost at iteration i=0, j=0: 1302.765016
Error rate: 0.421
Cost at iteration i=0, j=10: 398.127106
Error rate: 0.102
Cost at iteration i=0, j=20: 332.985303
Error rate: 0.077
Cost at iteration i=0, j=30: 291.236274
Error rate: 0.076
Cost at iteration i=0, j=40: 273.482729
Error rate: 0.073
Cost at iteration i=0, j=50: 254.258390
Error rate: 0.066
Cost at iteration i=0, j=60: 246.139238
Error rate: 0.063
Cost at iteration i=0, j=70: 228.230492
Error rate: 0.061
Cost at iteration i=0, j=80: 200.113072
Error rate: 0.057
Cost at iteration i=1, j=0: 203.806167
Error rate: 0.057
Cost at iteration i=1, j=10: 192.321276
Error rate: 0.051
Cost at iteration i=1, j=20: 199.572118
Error rate: 0.053
Cost at iteration i=1, j=30: 192.806075
Error rate: 0.056
Cost at iteration i=1, j=40: 189.398028
Error rate: 0.049
Cost at iteration i=1, j=50: 183.727744
Error rate: 0.045
Cost at iteration i=1, j=60: 180.697464
Error rate: 0.05
Cost at iteration i=1, j=70: 176.467620
Error rate: 0.047
Cost at iteration i=1, j=80: 158.250659
Error rate: 0.043
Cost at iteration i=2, j=0: 162.233567
Error rate: 0.042
Cost at iteration i=2, j=10: 156.353189
Error rate: 0.042
Cost at iteration i=2, j=20: 163.194722
Error rate: 0.047
Cost at iteration i=2, j=30: 168.083572
Error rate: 0.044
Cost at iteration i=2, j=40: 165.132412
Error rate: 0.041
Cost at iteration i=2, j=50: 162.424448
Error rate: 0.041
Cost at iteration i=2, j=60: 165.214867
Error rate: 0.039
Cost at iteration i=2, j=70: 160.444840
Error rate: 0.039
Cost at iteration i=2, j=80: 143.728678
Error rate: 0.037
Cost at iteration i=3, j=0: 147.690818
Error rate: 0.035
Cost at iteration i=3, j=10: 145.251354
Error rate: 0.035
Cost at iteration i=3, j=20: 151.492707
Error rate: 0.038
Cost at iteration i=3, j=30: 155.619508
Error rate: 0.039
Cost at iteration i=3, j=40: 153.681399
Error rate: 0.037
Cost at iteration i=3, j=50: 152.523027
Error rate: 0.036
Cost at iteration i=3, j=60: 155.854670
Error rate: 0.036
Cost at iteration i=3, j=70: 152.538494
Error rate: 0.035
Cost at iteration i=3, j=80: 137.580983
Error rate: 0.032
Cost at iteration i=4, j=0: 140.964466
Error rate: 0.029
Cost at iteration i=4, j=10: 140.112148
Error rate: 0.033
Cost at iteration i=4, j=20: 145.345904
Error rate: 0.033
Cost at iteration i=4, j=30: 148.307599
Error rate: 0.036
Cost at iteration i=4, j=40: 145.573409
Error rate: 0.036
Cost at iteration i=4, j=50: 147.326050
Error rate: 0.033
Cost at iteration i=4, j=60: 151.733384
Error rate: 0.036
Cost at iteration i=4, j=70: 146.105656
Error rate: 0.032
Cost at iteration i=4, j=80: 132.565915
Error rate: 0.031
Cost at iteration i=5, j=0: 135.485671
Error rate: 0.027
Cost at iteration i=5, j=10: 136.681437
Error rate: 0.031
Cost at iteration i=5, j=20: 141.686916
Error rate: 0.031
Cost at iteration i=5, j=30: 145.246070
Error rate: 0.034
Cost at iteration i=5, j=40: 142.282458
Error rate: 0.033
Cost at iteration i=5, j=50: 145.199844
Error rate: 0.032
Cost at iteration i=5, j=60: 149.838376
Error rate: 0.034
Cost at iteration i=5, j=70: 143.731109
Error rate: 0.033
Cost at iteration i=5, j=80: 131.304493
Error rate: 0.031
Cost at iteration i=6, j=0: 133.990625
Error rate: 0.027
Cost at iteration i=6, j=10: 136.101741
Error rate: 0.031
Cost at iteration i=6, j=20: 139.453595
Error rate: 0.032
Cost at iteration i=6, j=30: 142.583843
Error rate: 0.034
Cost at iteration i=6, j=40: 139.901206
Error rate: 0.033
Cost at iteration i=6, j=50: 142.986914
Error rate: 0.031
Cost at iteration i=6, j=60: 148.503604
Error rate: 0.034
Cost at iteration i=6, j=70: 141.828301
Error rate: 0.03
Cost at iteration i=6, j=80: 131.230851
Error rate: 0.033
Cost at iteration i=7, j=0: 133.468160
Error rate: 0.028
Cost at iteration i=7, j=10: 136.035457
Error rate: 0.03
Cost at iteration i=7, j=20: 138.249200
Error rate: 0.03
Cost at iteration i=7, j=30: 141.600186
Error rate: 0.035
Cost at iteration i=7, j=40: 139.708721
Error rate: 0.032
Cost at iteration i=7, j=50: 142.390000
Error rate: 0.03
Cost at iteration i=7, j=60: 147.842725
Error rate: 0.033
Cost at iteration i=7, j=70: 140.495995
Error rate: 0.029
Cost at iteration i=7, j=80: 132.204980
Error rate: 0.033
Cost at iteration i=8, j=0: 134.597511
Error rate: 0.026
Cost at iteration i=8, j=10: 136.793355
Error rate: 0.03
Cost at iteration i=8, j=20: 138.872168
Error rate: 0.03
Cost at iteration i=8, j=30: 141.508976
Error rate: 0.033
Cost at iteration i=8, j=40: 140.661221
Error rate: 0.032
Cost at iteration i=8, j=50: 142.625252
Error rate: 0.031
Cost at iteration i=8, j=60: 148.025390
Error rate: 0.032
Cost at iteration i=8, j=70: 140.605185
Error rate: 0.03
Cost at iteration i=8, j=80: 133.831230
Error rate: 0.034
Cost at iteration i=9, j=0: 136.054808
Error rate: 0.028
Cost at iteration i=9, j=10: 136.983022
Error rate: 0.03
Cost at iteration i=9, j=20: 139.948096
Error rate: 0.031
Cost at iteration i=9, j=30: 142.363093
Error rate: 0.033
Cost at iteration i=9, j=40: 140.889875
Error rate: 0.031
Cost at iteration i=9, j=50: 142.732904
Error rate: 0.03
Cost at iteration i=9, j=60: 148.271596
Error rate: 0.032
Cost at iteration i=9, j=70: 142.106092
Error rate: 0.03
Cost at iteration i=9, j=80: 135.531783
Error rate: 0.033
Cost at iteration i=10, j=0: 137.577871
Error rate: 0.028
Cost at iteration i=10, j=10: 138.409637
Error rate: 0.029
Cost at iteration i=10, j=20: 141.259732
Error rate: 0.03
Cost at iteration i=10, j=30: 143.665281
Error rate: 0.033
Cost at iteration i=10, j=40: 142.344639
Error rate: 0.031
Cost at iteration i=10, j=50: 144.153445
Error rate: 0.03
Cost at iteration i=10, j=60: 149.317771
Error rate: 0.032
Cost at iteration i=10, j=70: 143.069698
Error rate: 0.031
Cost at iteration i=10, j=80: 137.945913
Error rate: 0.033
Cost at iteration i=11, j=0: 139.848358
Error rate: 0.028
Cost at iteration i=11, j=10: 140.281712
Error rate: 0.029
Cost at iteration i=11, j=20: 142.715404
Error rate: 0.03
Cost at iteration i=11, j=30: 144.672019
Error rate: 0.032
Cost at iteration i=11, j=40: 143.112871
Error rate: 0.031
Cost at iteration i=11, j=50: 144.926515
Error rate: 0.03
Cost at iteration i=11, j=60: 149.914620
Error rate: 0.031
Cost at iteration i=11, j=70: 144.072747
Error rate: 0.03
Cost at iteration i=11, j=80: 139.421155
Error rate: 0.033
Cost at iteration i=12, j=0: 141.475706
Error rate: 0.029
Cost at iteration i=12, j=10: 141.570552
Error rate: 0.029
Cost at iteration i=12, j=20: 143.844635
Error rate: 0.029
Cost at iteration i=12, j=30: 145.925681
Error rate: 0.032
Cost at iteration i=12, j=40: 143.641218
Error rate: 0.031
Cost at iteration i=12, j=50: 145.667904
Error rate: 0.03
Cost at iteration i=12, j=60: 150.673502
Error rate: 0.031
Cost at iteration i=12, j=70: 145.335190
Error rate: 0.029
Cost at iteration i=12, j=80: 141.139299
Error rate: 0.032
Cost at iteration i=13, j=0: 143.365897
Error rate: 0.029
Cost at iteration i=13, j=10: 143.294662
Error rate: 0.028
Cost at iteration i=13, j=20: 145.528835
Error rate: 0.03
Cost at iteration i=13, j=30: 147.546935
Error rate: 0.032
Cost at iteration i=13, j=40: 145.516632
Error rate: 0.031
Cost at iteration i=13, j=50: 147.181662
Error rate: 0.029
Cost at iteration i=13, j=60: 151.938722
Error rate: 0.031
Cost at iteration i=13, j=70: 146.897990
Error rate: 0.029
Cost at iteration i=13, j=80: 143.175131
Error rate: 0.03
Cost at iteration i=14, j=0: 145.320085
Error rate: 0.03
Cost at iteration i=14, j=10: 145.079627
Error rate: 0.028
Cost at iteration i=14, j=20: 146.821527
Error rate: 0.03
Cost at iteration i=14, j=30: 148.798177
Error rate: 0.031
Cost at iteration i=14, j=40: 146.672768
Error rate: 0.031
Cost at iteration i=14, j=50: 148.356526
Error rate: 0.029
Cost at iteration i=14, j=60: 152.971972
Error rate: 0.029
Cost at iteration i=14, j=70: 148.448248
Error rate: 0.03
Cost at iteration i=14, j=80: 144.895337
Error rate: 0.031
Cost at iteration i=15, j=0: 147.180013
Error rate: 0.03
Cost at iteration i=15, j=10: 146.698825
Error rate: 0.028
Cost at iteration i=15, j=20: 147.911150
Error rate: 0.03
Cost at iteration i=15, j=30: 149.885095
Error rate: 0.031
Cost at iteration i=15, j=40: 147.723225
Error rate: 0.031
Cost at iteration i=15, j=50: 149.402505
Error rate: 0.029
Cost at iteration i=15, j=60: 154.061489
Error rate: 0.029
Cost at iteration i=15, j=70: 149.905740
Error rate: 0.03
Cost at iteration i=15, j=80: 146.730451
Error rate: 0.03
Cost at iteration i=16, j=0: 149.111490
Error rate: 0.03
Cost at iteration i=16, j=10: 148.618158
Error rate: 0.028
Cost at iteration i=16, j=20: 149.539371
Error rate: 0.029
Cost at iteration i=16, j=30: 151.431054
Error rate: 0.031
Cost at iteration i=16, j=40: 148.276068
Error rate: 0.028
Cost at iteration i=16, j=50: 150.046638
Error rate: 0.029
Cost at iteration i=16, j=60: 154.935099
Error rate: 0.029
Cost at iteration i=16, j=70: 151.222785
Error rate: 0.032
Cost at iteration i=16, j=80: 148.388571
Error rate: 0.03
Cost at iteration i=17, j=0: 150.779628
Error rate: 0.03
Cost at iteration i=17, j=10: 150.106789
Error rate: 0.028
Cost at iteration i=17, j=20: 152.481972
Error rate: 0.03
Cost at iteration i=17, j=30: 154.359348
Error rate: 0.031
Cost at iteration i=17, j=40: 150.287320
Error rate: 0.029
Cost at iteration i=17, j=50: 151.925365
Error rate: 0.029
Cost at iteration i=17, j=60: 156.586598
Error rate: 0.029
Cost at iteration i=17, j=70: 153.044622
Error rate: 0.032
Cost at iteration i=17, j=80: 150.295396
Error rate: 0.031
Cost at iteration i=18, j=0: 152.682940
Error rate: 0.032
Cost at iteration i=18, j=10: 151.895674
Error rate: 0.029
Cost at iteration i=18, j=20: 154.006675
Error rate: 0.028
Cost at iteration i=18, j=30: 155.919530
Error rate: 0.031
Cost at iteration i=18, j=40: 151.636828
Error rate: 0.029
Cost at iteration i=18, j=50: 153.179341
Error rate: 0.029
Cost at iteration i=18, j=60: 157.916758
Error rate: 0.03
Cost at iteration i=18, j=70: 154.658322
Error rate: 0.032
Cost at iteration i=18, j=80: 152.053474
Error rate: 0.031
Cost at iteration i=19, j=0: 154.517029
Error rate: 0.032
Cost at iteration i=19, j=10: 153.755373
Error rate: 0.029
Cost at iteration i=19, j=20: 155.777852
Error rate: 0.028
Cost at iteration i=19, j=30: 157.701633
Error rate: 0.031
Cost at iteration i=19, j=40: 153.190401
Error rate: 0.029
Cost at iteration i=19, j=50: 154.598358
Error rate: 0.029
Cost at iteration i=19, j=60: 159.039975
Error rate: 0.03
Cost at iteration i=19, j=70: 155.970673
Error rate: 0.032
Cost at iteration i=19, j=80: 153.676532
Error rate: 0.031
Final error rate: 0.03
-------------------- END --------------------------
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="6.-Adam-Optimizer">6. Adam Optimizer<a class="anchor-link" href="#6.-Adam-Optimizer">&#182;</a></h1><p>One of the newest and most common optimization techniques these days is known as the <strong>Adam Optimizer</strong> (link to original paper here: <a href="https://arxiv.org/pdf/1412.6980.pdf">https://arxiv.org/pdf/1412.6980.pdf</a>). It is going to be given it's own section all together, since as of 2017 it is often considered the go to optimizer for those doing deep learning. Sometimes, people will talk about <strong>Adam</strong> by saying it is like RMSprop, but <em>with</em> momentum. This can be slightly confusing though, considering that TensorFlow has RMSprop, and allows you to add momentum to it, which is <em>not</em> Adam. In other words, you can have RMSprop with momentum, but that doesn't give you Adam-it just gives you RMSprop with momentum. Adam does, in a sense, add something like momentum to RMSprop, but in a very specific way. In order to fully understand <strong>Adam</strong>, we first need to look at part of the theory that it is dependent on: <strong>Exponentially-Smoothed Averages</strong>.</p>
<h2 id="6.1-Exponentially-Smoothed-Averages">6.1 Exponentially Smoothed Averages<a class="anchor-link" href="#6.1-Exponentially-Smoothed-Averages">&#182;</a></h2><p>We are going to take a minute to dig into something that may seem straight forward: How to calculate an average. The first thought we all have when being asked to do this is: Why not just add all of the sample data points, and then divide by the number of data points, resulting in the sample mean:</p>
$$\bar{X}_N = \frac{1}{N}\sum_{n=1}^NX_n$$<p>But now let's suppose that you have a large amount of data-so much so that all of your X's cannot fit into memory at the same time. Is it still possible to calculate the sample mean? Yes it is! We can read in one data point at a time, and then delete each data point after we've looked at it. It is shown below that the current sample mean can actually be expressed in terms of the previous sample mean and the current data point.</p>
$$\bar{X}_N =  \frac{1}{N}\sum_{n=1}^NX_n = \frac{1}{N}\Big((N-1)\bar{X}_{N-1} + X_N \Big) = (1 - \frac{1}{N})\bar{X}_{N-1}+\frac{1}{N}X_N$$<p>We can then express this using simpler symbols. We can call $Y$ our output, and we can use $t$ to represent the current time step:</p>
$$Y_t = (1 - \frac{1}{t})Y_{t-1} + \frac{1}{t}X_t$$<p>Great, so we have solved our problem of how to calculate the sample mean when we can't fit all of the data into memory, but we can see that there is this $\frac{1}{t}$ term. This says that as $t$ grows larger, the current sample has less and less of an effect on the total mean. Clearly this makes sense, because as $t$ grows that means the total number of $X$'s we've seen has grown. We also decrease the influence of the previous $Y$ by $1 - \frac{1}{t}$. This means that each new $Y$ is just part of the old $Y$, plus part of the newest $X$. But in the end, it balances out to give us exactly the sample mean of $X$.</p>
<p>For convenience we can call this $\frac{1}{t}$ term $\alpha_t$. What if we were to say that we did not want $\alpha$ to be $\frac{1}{t}$? What if we said that we wanted each data point to matter equally at the time that we see it, so that we can set alpha to be a constant? Of course, $\alpha$ needs to be less than 1 so that we don't end up negating the previous mean.</p>
$$0 &lt; \alpha_t = constant &lt; 1 $$$$Y_t = (1 - \alpha)Y_{t-1} + \alpha X_t$$<p>So what does this give us?</p>
<h3 id="6.1.1-The-Exponentially-smoothed-average">6.1.1 The Exponentially-smoothed average<a class="anchor-link" href="#6.1.1-The-Exponentially-smoothed-average">&#182;</a></h3><p>This gives us what is called the exponentially smoothed average! We can see why it is called exponential when we express it in terms of only $X$'s.</p>
$$Y_t = (1 - \alpha)^tY_0 + \alpha \sum_{\tau = 0}^{t - 1}(1- \alpha)^\tau X(t- \tau)$$<p>If the equation above is not clear, the expansion below should clear up where everything is coming from and <em>why</em> this is called exponential. Let's say we are looking at $Y_{100}$:</p>
$$Y_{100} = (1-\alpha)^{100}Y_0 + \alpha * X_{100} + \alpha * (1 - \alpha)^1*X_{99} + \alpha * (1 - \alpha)^2 * X_{98}+ ...$$<p>We can see the exponential term start to accumulate along the $(1 - alpha)$! Now, does this still give us the mean, aka the expected value of $X$? Well, if you take the expected value of everything, we can see that we arrive at the expected value of $X$:</p>
$$(1 - \alpha)E[y(t-1)] + \alpha E[X(t)] = (1-\alpha)E(X) + \alpha E(X) = E(X)$$<p>We do arrive at the expected value of $X$, so we can see that the math does checkout! Of course, this is assuming that the distribution of $X$ does not change over time. Note that if you have come from a signal processing background, you may recognize this as a <strong>low-pass filter</strong>. Another way to think about this is that you are saying <em>current values matter more</em>, and <em>past values matter less</em> in an exponentially decreasing way. So, if $X$ is not stationary (meaning it's distribution changes over time), then this is actually a better way to estimate the mean (average) then weighting all data points equally over all time.</p>
<h2 id="6.2-Exponentially-Smoothed-Average's-and-Adam">6.2 Exponentially Smoothed Average's and Adam<a class="anchor-link" href="#6.2-Exponentially-Smoothed-Average's-and-Adam">&#182;</a></h2><p>Now, how does this apply to the <strong>Adam Optimizer</strong>? Well, if we call the sample mean of $T$ samples, $M_T$, we write $M_T$ as:
$$M_T = \frac{1}{T} \sum_{t=1}^TX_t = \frac{1}{T}\sum_{t=1}^{T-1}X_t+\frac{1}{T}X_T = (1 - \frac{1}{T})M_{T-1}+\frac{1}{T}X_T$$</p>
<p>Where $X_T$ is the new sample, and $M_{T-1}$ is the previous mean. This means that instead of needing to store all of the $X$'s, all we need to store is the current $X$ and the previous $M$, and then we calculate the current $M$.</p>
<p>Now, you may notice that there is this $\frac{1}{T}$ term that occurs in the equation. It is reasonable to ask: "What is we set this term to be a constant?" Then, we get back <strong>precisely</strong> what looks like the <strong>cache update</strong> from <strong>RMSProp</strong>!</p>
$$M_T = decay*M_{T-1} + (1 - decay)X_T$$<h2 id="6.3-What-is-the-cache-of-RMSProp-estimating?">6.3 What is the cache of RMSProp estimating?<a class="anchor-link" href="#6.3-What-is-the-cache-of-RMSProp-estimating?">&#182;</a></h2><p>At this point, it is a very reasonable thing to ask: "What is the cache of RMSProp really estimating?" Well, it is really estimating the mean of the square of the gradient!</p>
<p><img src="https://drive.google.com/uc?id=16P1WvLt-U6g4ywRjHcFZM-z1ir3IQWYs"></p>
<p><img src="https://drive.google.com/uc?id=1ECbBAnRz0tU9FARYdFlcTBb_YgP13CMo"></p>
$$v_t = decay * v_{t-1} + (1 - decay)*g^2 \approx mean(g^2)$$<p>And because we eventually take the square root of the cache, now you can see where RMSProp gets its name (RMS= "root mean square").</p>
<h2 id="6.4-Second-Moment">6.4 Second Moment<a class="anchor-link" href="#6.4-Second-Moment">&#182;</a></h2><p>Also, it is worth remembering that the <strong>mean</strong> of a <strong>random variable</strong> can also be phrased as an <strong>expected value</strong>, aka: $mean(X) = E(X)$. In particular, when we take the expected value of the square of a random variable, we get the <strong>second moment</strong>:
$$E(X^2) = 2nd \;moment\;of\;X$$</p>
<p>So, $E(g^2)$ is the second moment of the gradient, and we calculate it using this exponentially smoothed average. We are going to refer to it as $v$ from now on, since the <strong>2nd central moment</strong> of a random variable is the definition of variance.</p>
$$v \approx E(g^2)$$$$M_T = (1 - \frac{1}{T})M_{T-1}+\frac{1}{T}X_T$$$$v_t = decay * v_{t-1} + (1 - decay)*g^2$$<h2 id="6.5-First-Moment">6.5 First Moment<a class="anchor-link" href="#6.5-First-Moment">&#182;</a></h2><p>Now, since there is a second moment, it is reasonable to ask: is there a <strong>first moment</strong>? Yes there is! It is just the expected value of $g$.</p>
$$m_t = \mu m_{t-1} + (1 - \mu)g_t$$$$m \approx E(g)$$<p>We will call this $m$ because the first moment of a random variable is the definition of the <strong>mean</strong>. Because the <strong>Adam Update</strong> makes use of the 1st and 2nd moments of $g$ using exponentially smoothed estimates, this also explains the name of the algorithm: <strong>Adaptive Moment Estimation</strong>.</p>
<p>Also, note that the update for $m$ looks a lot like momentum! Hence, why some people refer to this as <strong>RMSProp</strong> with momentum. Usually momentum is just implemented by adding a momentum parameter to the velocity rather than doing this weighted average, as a reminder it usually looks like:</p>
$$m_t = \mu m_{t-1} + g_t$$<h2 id="6.6-One-final-problem">6.6 One final problem<a class="anchor-link" href="#6.6-One-final-problem">&#182;</a></h2><p>So to get to our final destination, which is the <strong>Adam Update</strong>, we are going to combine these two pieces: the first and second moments of $g$, or as we have been calling them, $m$ and $v$ (mean and variance). However, before we do that, there is one last important concept to discuss. The exponentially smoothed average acts very much like a smoothing function, ignoring much of the variation. It will ignore the rapid very random spikes, but it will follow the main trend of the signal, the original sine wave. In the field of signal processing this is known as a low pass filter, because the high frequency changes are being filtered out.</p>
<p><br>
<img src="https://drive.google.com/uc?id=1PdWjYmhekt8kLsowXJLxGt_4iSnVnBFp" width="500"></p>
<p>The problem with exponential smoothing is just like RMSProp, what is the initial value? The output is supposed to depend on the current value of the input plus the last value of the output. But since there is no last value of the output for $t = 0 $, we typically just set that to 0. But in doing so, we make the first value of the output small:
$$Y(0)= 0$$
$$Y(1) = 0.99 * Y(0) + 0.01*X(1) = 0.01*X(1)$$</p>
<p>This means that initially your output will be biased towards 0!</p>
<p><img src="https://drive.google.com/uc?id=1DzymN4mLDVCJiqxZAGmA5bDvmjbTeMma" width="400"></p>
<h2 id="6.7-Bias-Correction">6.7 Bias Correction<a class="anchor-link" href="#6.7-Bias-Correction">&#182;</a></h2><p>Is there anything that we can do about the issue presented above? Well, there is a technique, <strong>bias correction</strong>, that makes up for this.</p>
$$\hat{Y}(t) = \frac{Y(t)}{1 - decay^t}$$<p>We divide by $(1 - decay)^t$. As you can see, when $t$ is small we are dividing by a very small number, which makes the initial value bigger. This is good because the initial value was too small.</p>
<p>Let's go through an example to make things very clear. Given a decay of 0.99, our original output would be:</p>
$$Y(1) = 0.01*X(1)$$<p>And $Y(2)$ would be:</p>
$$Y(2) = 0.0099*X(1) + 0.01*X(2)$$<p>But, if we correct it, then we have:</p>
$$\hat{Y}(1) = \frac{0.01}{1 - 0.99^1}*X(1) = X(1)$$<p>And our corrected $\hat{Y}(2)$:</p>
$$\hat{Y}(2) = \frac{Y(2)}{0.0199} = 0.497*X(1) + 0.503*X(2)$$<p>This also makes sense because it is about half of $X(1)$ plus about half of $X(2)$, with a bit more weight on the current value. We can also see that as $t$ approaches infinity, the bias correction term approaches 1, meaning that we quickly converge to the standard exponentially smooth average. The important part is that our output now has the correct range of values when compared with the input, for all values of the input, including the initial ones.</p>
<h2 id="6.8-Back-to-Adam">6.8 Back to Adam<a class="anchor-link" href="#6.8-Back-to-Adam">&#182;</a></h2><p>Now that we have discussed bias correction for exponentially smoothed averages, you might guess that we are going to apply it to the exponentially smoothed averages that we have been talking about, in particular $m$ and $v$. So, if we call $\hat{m}$ the bias corrected version of $m$, and $\hat{v}$ the bias corrected version of $v$, then we can formulate our <strong>Adam Update</strong>.</p>
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$<p>Note, there are two different decay rates for $m$ and $v$, which we call $\beta_1$ and $\beta_2$. Typical values are in the range of 0.9 to 0.99. $\epsilon$ again has values in the range of $10^{-8}$ to $10^{-10}$. Our final update for $w$ is:</p>
$$w_{t+1} \leftarrow w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t+\epsilon}}$$<h2 id="6.9-Recap">6.9 Recap<a class="anchor-link" href="#6.9-Recap">&#182;</a></h2><p>Now that was a ton of stuff, so lets recap! <strong>Adam</strong> is a new, modern adaptive learning rate technique that is the default go to for many deep learning practioners. It combines 2 proven techniques:</p>
<blockquote><ol>
<li><strong>RMSProp</strong> which is a <strong>cache mechanism</strong> (leaky cache)</li>
<li><strong>Momentum</strong> which speeds up training just by continuing to go in the same direction it was going before (keeping track of old gradients) </li>
</ol>
</blockquote>
<p>We saw that these two methods can be generalized, by observing that they estimate the first and second moments of the gradient. <strong>Adam</strong> also adds bias correction, which makes up for the fact that the exponentially smoothed average starts at zero, and hence the initial estimates are biased towards 0.</p>
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$$$w_{t+1} \leftarrow w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t+\epsilon}}$$<h2 id="6.10-Summary-Pseudocode">6.10 Summary Pseudocode<a class="anchor-link" href="#6.10-Summary-Pseudocode">&#182;</a></h2><p>At every batch (typical $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$):</p>
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t$$$$v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2$$$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$$$w_{t+1} \leftarrow w_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t+\epsilon}}$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="7.-Adam-vs.-RMSProp-with-Momentum,-in-Code">7. Adam vs. RMSProp with Momentum, in Code<a class="anchor-link" href="#7.-Adam-vs.-RMSProp-with-Momentum,-in-Code">&#182;</a></h1><p>We are now going to take the time to compare the <strong>Adam Optimizer</strong> in code, vs <strong>RMSProp with Momentum</strong>.</p>
<p>Recall that the algorithm for RMSProp was:</p>
<p><strong>RMSProp</strong></p>

<pre><code># at every batch
cache = decay * cache + (1 - decay) * gradient **2
param = param - learning_rate * gradient / sqrt(cache + epsilon)</code></pre>
<p>We can start with our imports.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">shuffle</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">util</span> <span class="k">import</span> <span class="n">get_normalized_data</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y2indicator</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now let's start our main method:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> notebook
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">print_period</span> <span class="o">=</span> <span class="mi">10</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_normalized_data</span><span class="p">()</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span>

    <span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">,]</span>
    <span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">]</span>
    <span class="n">Xtest</span>  <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:,]</span>
    <span class="n">Ytest</span>  <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:]</span>
    <span class="n">Ytrain_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytrain</span><span class="p">)</span>
    <span class="n">Ytest_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytest</span><span class="p">)</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">batch_sz</span>

    <span class="n">M</span> <span class="o">=</span> <span class="mi">300</span>
    <span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">W1_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
    <span class="n">b1_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">W2_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
    <span class="n">b2_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

    <span class="n">W1</span> <span class="o">=</span> <span class="n">W1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">b1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">W2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">b2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    
    <span class="sd">&quot;&quot;&quot; ----------------------------------- ADAM -----------------------------------&quot;&quot;&quot;</span>
    <span class="c1"># 1st moment initialization (aka m_t at t = 0, = 0). Note, we could initialize these to </span>
    <span class="c1"># an array of zeros, however it is not necessary considering numpy knows how to </span>
    <span class="c1"># automatically broadcast a scalar when you add it to an array </span>
    <span class="n">mW1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mb1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mW2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mb2</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># 2nd moment initialization</span>
    <span class="n">vW1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">vb1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">vW2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">vb2</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># define hyperparameters - will use the same parameters for ADAM and RMSProp</span>
    <span class="n">lr0</span> <span class="o">=</span> <span class="mf">0.001</span> 
    <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span>        <span class="c1"># we can think of this as momentum </span>
    <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span>      <span class="c1"># can think of this like RMSProp cache decay rate </span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-8</span>
    
    <span class="n">loss_adam</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">err_adam</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
            <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),</span> <span class="p">]</span>
            <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),</span> <span class="p">]</span>
            <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
            
            <span class="c1"># ---- Updates ----</span>
            <span class="c1"># 1st - calculate updated gradients for each array</span>
            <span class="n">gW2</span> <span class="o">=</span> <span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span>
            <span class="n">gb2</span> <span class="o">=</span> <span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span>
            <span class="n">gW1</span> <span class="o">=</span> <span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span>
            <span class="n">gb1</span> <span class="o">=</span> <span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span>
            
            <span class="c1"># Calculate updates for m, exponentially smoothed average of gradients, 1st moment</span>
            <span class="n">mW1</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">mW1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">gW1</span>
            <span class="n">mb1</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">mb1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">gb1</span>
            <span class="n">mW2</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">mW2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">gW2</span>
            <span class="n">mb2</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">mb2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">gb2</span>
            
            <span class="c1"># Calculate updates for v, exponentially smoothed average of the square of the </span>
            <span class="c1"># gradients, 2nd moment</span>
            <span class="n">vW1</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">vW1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">gW1</span><span class="o">*</span> <span class="n">gW1</span>
            <span class="n">vb1</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">vb1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">gb1</span><span class="o">*</span> <span class="n">gb1</span>
            <span class="n">vW2</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">vW2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">gW2</span><span class="o">*</span> <span class="n">gW2</span>
            <span class="n">vb2</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">vb2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">gb2</span><span class="o">*</span> <span class="n">gb2</span>
            
            <span class="c1"># Bias correction step - by convention, first time index is 1 </span>
            <span class="n">correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">t</span>
            <span class="n">hat_mW1</span> <span class="o">=</span> <span class="n">mW1</span> <span class="o">/</span> <span class="n">correction1</span>
            <span class="n">hat_mb1</span> <span class="o">=</span> <span class="n">mb1</span> <span class="o">/</span> <span class="n">correction1</span>
            <span class="n">hat_mW2</span> <span class="o">=</span> <span class="n">mW2</span> <span class="o">/</span> <span class="n">correction1</span>
            <span class="n">hat_mb2</span> <span class="o">=</span> <span class="n">mb2</span> <span class="o">/</span> <span class="n">correction1</span>
            
            <span class="n">correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">t</span>
            <span class="n">hat_vW1</span> <span class="o">=</span> <span class="n">vW1</span> <span class="o">/</span> <span class="n">correction2</span>
            <span class="n">hat_vb1</span> <span class="o">=</span> <span class="n">vb1</span> <span class="o">/</span> <span class="n">correction2</span>
            <span class="n">hat_vW2</span> <span class="o">=</span> <span class="n">vW2</span> <span class="o">/</span> <span class="n">correction2</span>
            <span class="n">hat_vb2</span> <span class="o">=</span> <span class="n">vb2</span> <span class="o">/</span> <span class="n">correction2</span>
            
            <span class="c1"># Update t - why?????</span>
            <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Finally, apply updates to the actual weights/params</span>
            <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">hat_mW1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hat_vW1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">hat_mb1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hat_vb1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">hat_mW2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hat_vW2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">hat_mb2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">hat_vb2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            
            <span class="c1"># Keep track of costs and errors so we can plot later</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
                <span class="n">l</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
                <span class="n">loss_adam</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>

                <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
                <span class="n">err_adam</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>

    <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------------- End of Adam---------------------------------&#39;</span><span class="p">)</span>
    
    <span class="sd">&quot;&quot;&quot; ---------------------------- RMSProp with Momentum ---------------------------&quot;&quot;&quot;</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">W1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>    <span class="c1"># Same initial weights that were used in Adam</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">b1_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">W2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">b2_0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">loss_rms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">err_rms</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="c1"># Comparable hyperparameters so we can have a fair comparison</span>
    <span class="n">lr0</span> <span class="o">=</span> <span class="mf">0.001</span>                <span class="c1"># Learning rate</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mf">0.9</span>                   <span class="c1"># Momentum term, corresponds to beta1</span>
    <span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.999</span>         <span class="c1"># Cache decay rate, corresponds to beta2</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-8</span>
    
    <span class="c1"># RMSProp Cache, initialized to 1 (same implementation as in TensorFlow)</span>
    <span class="c1"># This needs to be done because RMSProp does not have bias correction</span>
    <span class="n">cache_W2</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">cache_b2</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">cache_W1</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">cache_b1</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Initialize Momentum/Velocity terms to 0, note no bias correction</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
            <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
            <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Ytrain_ind</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),]</span>
            <span class="n">pYbatch</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    
            <span class="c1"># ---- Updates ----</span>
            <span class="c1"># General Form: Do everything we did for RMSProp, an add momentum to it at the </span>
            <span class="c1"># end. </span>
            <span class="c1"># 1) Calculate the Gradient</span>
            <span class="c1"># 2) Calculate the Cache Update</span>
            <span class="c1"># 3) Use variation on momentum, so instead of weighting just the previous velocity</span>
            <span class="c1"># we weight the change that we would have made by 1 - mu. So, instead of just </span>
            <span class="c1"># having mu * dW2, we have mu * dW2 and then we add another constant (1 - mu*dW2)</span>
            <span class="c1"># to the normal learning rate * gradient update that we normally would have done </span>
            
            <span class="n">gW2</span> <span class="o">=</span> <span class="n">derivative_w2</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W2</span>
            <span class="n">cache_W2</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_W2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gW2</span><span class="o">*</span><span class="n">gW2</span>
            <span class="n">dW2</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">dW2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gW2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">W2</span> <span class="o">-=</span> <span class="n">dW2</span>

            <span class="n">gb2</span> <span class="o">=</span> <span class="n">derivative_b2</span><span class="p">(</span><span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b2</span>
            <span class="n">cache_b2</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_b2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gb2</span><span class="o">*</span><span class="n">gb2</span>
            <span class="n">db2</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">db2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gb2</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_b2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">b2</span> <span class="o">-=</span> <span class="n">db2</span>

            <span class="n">gW1</span> <span class="o">=</span> <span class="n">derivative_w1</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">W1</span>
            <span class="n">cache_W1</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_W1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gW1</span><span class="o">*</span><span class="n">gW1</span>
            <span class="n">dW1</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">dW1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gW1</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">W1</span> <span class="o">-=</span> <span class="n">dW1</span>

            <span class="n">gb1</span> <span class="o">=</span> <span class="n">derivative_b1</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">,</span> <span class="n">pYbatch</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="o">*</span><span class="n">b1</span>
            <span class="n">cache_b1</span> <span class="o">=</span> <span class="n">decay_rate</span><span class="o">*</span><span class="n">cache_b1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span><span class="o">*</span><span class="n">gb1</span><span class="o">*</span><span class="n">gb1</span>
            <span class="n">db1</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">db1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr0</span> <span class="o">*</span> <span class="n">gb1</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache_b1</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="n">b1</span> <span class="o">-=</span> <span class="n">db1</span>
            
            <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="n">print_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
                <span class="n">l</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
                <span class="n">loss_rms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration i=</span><span class="si">%d</span><span class="s2">, j=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>

                <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
                <span class="n">err_rms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">err</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>

    <span class="n">pY</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_adam</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_rms</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">main</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Reading in and transforming data...
Cost at iteration i=0, j=0: 2147.070827
Error rate: 0.697
Cost at iteration i=0, j=10: 588.999795
Error rate: 0.187
Cost at iteration i=0, j=20: 399.359862
Error rate: 0.108
Cost at iteration i=0, j=30: 333.021415
Error rate: 0.092
Cost at iteration i=0, j=40: 292.684684
Error rate: 0.073
Cost at iteration i=0, j=50: 262.380853
Error rate: 0.065
Cost at iteration i=0, j=60: 230.907929
Error rate: 0.059
Cost at iteration i=0, j=70: 222.231061
Error rate: 0.052
Cost at iteration i=0, j=80: 209.672028
Error rate: 0.052
Cost at iteration i=1, j=0: 207.840002
Error rate: 0.052
Cost at iteration i=1, j=10: 194.216520
Error rate: 0.049
Cost at iteration i=1, j=20: 183.842938
Error rate: 0.044
Cost at iteration i=1, j=30: 176.089661
Error rate: 0.045
Cost at iteration i=1, j=40: 174.666713
Error rate: 0.047
Cost at iteration i=1, j=50: 171.138116
Error rate: 0.049
Cost at iteration i=1, j=60: 158.012662
Error rate: 0.048
Cost at iteration i=1, j=70: 155.995580
Error rate: 0.048
Cost at iteration i=1, j=80: 154.524563
Error rate: 0.046
Cost at iteration i=2, j=0: 154.578702
Error rate: 0.046
Cost at iteration i=2, j=10: 145.908771
Error rate: 0.043
Cost at iteration i=2, j=20: 141.455232
Error rate: 0.04
Cost at iteration i=2, j=30: 138.438420
Error rate: 0.042
Cost at iteration i=2, j=40: 140.651550
Error rate: 0.045
Cost at iteration i=2, j=50: 137.599638
Error rate: 0.043
Cost at iteration i=2, j=60: 127.967954
Error rate: 0.04
Cost at iteration i=2, j=70: 126.836930
Error rate: 0.039
Cost at iteration i=2, j=80: 129.237236
Error rate: 0.037
Cost at iteration i=3, j=0: 130.238918
Error rate: 0.041
Cost at iteration i=3, j=10: 125.752678
Error rate: 0.04
Cost at iteration i=3, j=20: 120.710198
Error rate: 0.036
Cost at iteration i=3, j=30: 119.677325
Error rate: 0.037
Cost at iteration i=3, j=40: 122.522703
Error rate: 0.041
Cost at iteration i=3, j=50: 122.184732
Error rate: 0.04
Cost at iteration i=3, j=60: 114.077212
Error rate: 0.038
Cost at iteration i=3, j=70: 112.966817
Error rate: 0.039
Cost at iteration i=3, j=80: 116.378527
Error rate: 0.036
Cost at iteration i=4, j=0: 117.617392
Error rate: 0.039
Cost at iteration i=4, j=10: 114.786611
Error rate: 0.037
Cost at iteration i=4, j=20: 109.170873
Error rate: 0.036
Cost at iteration i=4, j=30: 108.549830
Error rate: 0.033
Cost at iteration i=4, j=40: 110.954193
Error rate: 0.037
Cost at iteration i=4, j=50: 113.216539
Error rate: 0.035
Cost at iteration i=4, j=60: 106.765885
Error rate: 0.036
Cost at iteration i=4, j=70: 105.917620
Error rate: 0.033
Cost at iteration i=4, j=80: 109.337854
Error rate: 0.034
Cost at iteration i=5, j=0: 110.880603
Error rate: 0.036
Cost at iteration i=5, j=10: 111.500581
Error rate: 0.033
Cost at iteration i=5, j=20: 105.457665
Error rate: 0.035
Cost at iteration i=5, j=30: 102.853409
Error rate: 0.029
Cost at iteration i=5, j=40: 104.389982
Error rate: 0.032
Cost at iteration i=5, j=50: 108.948924
Error rate: 0.033
Cost at iteration i=5, j=60: 103.144376
Error rate: 0.035
Cost at iteration i=5, j=70: 102.601201
Error rate: 0.034
Cost at iteration i=5, j=80: 104.844103
Error rate: 0.032
Cost at iteration i=6, j=0: 106.223056
Error rate: 0.034
Cost at iteration i=6, j=10: 108.632585
Error rate: 0.034
Cost at iteration i=6, j=20: 103.732068
Error rate: 0.033
Cost at iteration i=6, j=30: 100.002771
Error rate: 0.029
Cost at iteration i=6, j=40: 100.388251
Error rate: 0.029
Cost at iteration i=6, j=50: 105.673950
Error rate: 0.031
Cost at iteration i=6, j=60: 101.209869
Error rate: 0.031
Cost at iteration i=6, j=70: 100.679370
Error rate: 0.034
Cost at iteration i=6, j=80: 102.738195
Error rate: 0.033
Cost at iteration i=7, j=0: 104.518793
Error rate: 0.035
Cost at iteration i=7, j=10: 109.478965
Error rate: 0.034
Cost at iteration i=7, j=20: 105.599002
Error rate: 0.033
Cost at iteration i=7, j=30: 100.982793
Error rate: 0.03
Cost at iteration i=7, j=40: 99.988630
Error rate: 0.03
Cost at iteration i=7, j=50: 104.526002
Error rate: 0.031
Cost at iteration i=7, j=60: 101.492697
Error rate: 0.029
Cost at iteration i=7, j=70: 100.245501
Error rate: 0.031
Cost at iteration i=7, j=80: 100.410748
Error rate: 0.03
Cost at iteration i=8, j=0: 101.721682
Error rate: 0.031
Cost at iteration i=8, j=10: 105.966997
Error rate: 0.032
Cost at iteration i=8, j=20: 104.209617
Error rate: 0.03
Cost at iteration i=8, j=30: 99.145083
Error rate: 0.029
Cost at iteration i=8, j=40: 97.678474
Error rate: 0.029
Cost at iteration i=8, j=50: 102.242486
Error rate: 0.028
Cost at iteration i=8, j=60: 100.668352
Error rate: 0.027
Cost at iteration i=8, j=70: 99.750481
Error rate: 0.029
Cost at iteration i=8, j=80: 98.138161
Error rate: 0.031
Cost at iteration i=9, j=0: 99.180736
Error rate: 0.029
Cost at iteration i=9, j=10: 105.264247
Error rate: 0.031
Cost at iteration i=9, j=20: 103.872365
Error rate: 0.028
Cost at iteration i=9, j=30: 98.075436
Error rate: 0.027
Cost at iteration i=9, j=40: 97.040009
Error rate: 0.027
Cost at iteration i=9, j=50: 102.067759
Error rate: 0.028
Cost at iteration i=9, j=60: 101.050897
Error rate: 0.027
Cost at iteration i=9, j=70: 100.037340
Error rate: 0.029
Cost at iteration i=9, j=80: 98.005822
Error rate: 0.03
Final error rate: 0.03
-------------------------------- End of Adam---------------------------------
Cost at iteration i=0, j=0: 2426.209838
Error rate: 0.892
Cost at iteration i=0, j=10: 532.227089
Error rate: 0.167
Cost at iteration i=0, j=20: 388.179538
Error rate: 0.115
Cost at iteration i=0, j=30: 344.841406
Error rate: 0.091
Cost at iteration i=0, j=40: 297.439443
Error rate: 0.079
Cost at iteration i=0, j=50: 253.608403
Error rate: 0.068
Cost at iteration i=0, j=60: 224.495871
Error rate: 0.063
Cost at iteration i=0, j=70: 214.427131
Error rate: 0.053
Cost at iteration i=0, j=80: 202.850310
Error rate: 0.055
Cost at iteration i=1, j=0: 201.529388
Error rate: 0.054
Cost at iteration i=1, j=10: 192.833639
Error rate: 0.051
Cost at iteration i=1, j=20: 183.596219
Error rate: 0.046
Cost at iteration i=1, j=30: 176.900687
Error rate: 0.047
Cost at iteration i=1, j=40: 175.641684
Error rate: 0.047
Cost at iteration i=1, j=50: 171.449806
Error rate: 0.049
Cost at iteration i=1, j=60: 159.085378
Error rate: 0.047
Cost at iteration i=1, j=70: 156.672770
Error rate: 0.045
Cost at iteration i=1, j=80: 156.383787
Error rate: 0.045
Cost at iteration i=2, j=0: 156.591487
Error rate: 0.046
Cost at iteration i=2, j=10: 152.651940
Error rate: 0.049
Cost at iteration i=2, j=20: 148.079450
Error rate: 0.046
Cost at iteration i=2, j=30: 143.289665
Error rate: 0.038
Cost at iteration i=2, j=40: 145.090865
Error rate: 0.044
Cost at iteration i=2, j=50: 143.937198
Error rate: 0.046
Cost at iteration i=2, j=60: 134.529339
Error rate: 0.042
Cost at iteration i=2, j=70: 133.222575
Error rate: 0.041
Cost at iteration i=2, j=80: 133.352681
Error rate: 0.04
Cost at iteration i=3, j=0: 134.286797
Error rate: 0.04
Cost at iteration i=3, j=10: 130.182083
Error rate: 0.041
Cost at iteration i=3, j=20: 127.691585
Error rate: 0.037
Cost at iteration i=3, j=30: 125.986581
Error rate: 0.038
Cost at iteration i=3, j=40: 127.866447
Error rate: 0.039
Cost at iteration i=3, j=50: 128.631623
Error rate: 0.039
Cost at iteration i=3, j=60: 121.548151
Error rate: 0.036
Cost at iteration i=3, j=70: 120.843228
Error rate: 0.038
Cost at iteration i=3, j=80: 122.094705
Error rate: 0.039
Cost at iteration i=4, j=0: 123.441084
Error rate: 0.039
Cost at iteration i=4, j=10: 121.557238
Error rate: 0.04
Cost at iteration i=4, j=20: 117.822181
Error rate: 0.039
Cost at iteration i=4, j=30: 115.251156
Error rate: 0.036
Cost at iteration i=4, j=40: 116.719986
Error rate: 0.04
Cost at iteration i=4, j=50: 119.368787
Error rate: 0.037
Cost at iteration i=4, j=60: 113.909990
Error rate: 0.037
Cost at iteration i=4, j=70: 113.216924
Error rate: 0.038
Cost at iteration i=4, j=80: 113.822508
Error rate: 0.037
Cost at iteration i=5, j=0: 114.959768
Error rate: 0.038
Cost at iteration i=5, j=10: 114.323753
Error rate: 0.038
Cost at iteration i=5, j=20: 111.504951
Error rate: 0.035
Cost at iteration i=5, j=30: 109.653831
Error rate: 0.038
Cost at iteration i=5, j=40: 110.738046
Error rate: 0.037
Cost at iteration i=5, j=50: 113.849866
Error rate: 0.037
Cost at iteration i=5, j=60: 109.768198
Error rate: 0.037
Cost at iteration i=5, j=70: 109.341777
Error rate: 0.037
Cost at iteration i=5, j=80: 109.913025
Error rate: 0.037
Cost at iteration i=6, j=0: 110.890013
Error rate: 0.037
Cost at iteration i=6, j=10: 110.705113
Error rate: 0.036
Cost at iteration i=6, j=20: 108.209940
Error rate: 0.036
Cost at iteration i=6, j=30: 106.165601
Error rate: 0.035
Cost at iteration i=6, j=40: 107.337522
Error rate: 0.035
Cost at iteration i=6, j=50: 110.463314
Error rate: 0.037
Cost at iteration i=6, j=60: 107.028071
Error rate: 0.036
Cost at iteration i=6, j=70: 106.543203
Error rate: 0.036
Cost at iteration i=6, j=80: 106.880316
Error rate: 0.037
Cost at iteration i=7, j=0: 107.673417
Error rate: 0.038
Cost at iteration i=7, j=10: 108.722810
Error rate: 0.036
Cost at iteration i=7, j=20: 106.857095
Error rate: 0.036
Cost at iteration i=7, j=30: 104.079833
Error rate: 0.034
Cost at iteration i=7, j=40: 105.069187
Error rate: 0.032
Cost at iteration i=7, j=50: 108.024067
Error rate: 0.034
Cost at iteration i=7, j=60: 105.826066
Error rate: 0.036
Cost at iteration i=7, j=70: 105.738961
Error rate: 0.034
Cost at iteration i=7, j=80: 105.782017
Error rate: 0.038
Cost at iteration i=8, j=0: 106.290092
Error rate: 0.037
Cost at iteration i=8, j=10: 107.120440
Error rate: 0.035
Cost at iteration i=8, j=20: 105.989624
Error rate: 0.035
Cost at iteration i=8, j=30: 103.321011
Error rate: 0.032
Cost at iteration i=8, j=40: 104.805380
Error rate: 0.034
Cost at iteration i=8, j=50: 106.961423
Error rate: 0.032
Cost at iteration i=8, j=60: 105.263030
Error rate: 0.032
Cost at iteration i=8, j=70: 104.940144
Error rate: 0.033
Cost at iteration i=8, j=80: 104.110568
Error rate: 0.035
Cost at iteration i=9, j=0: 104.356185
Error rate: 0.037
Cost at iteration i=9, j=10: 105.902251
Error rate: 0.035
Cost at iteration i=9, j=20: 105.567783
Error rate: 0.036
Cost at iteration i=9, j=30: 102.535401
Error rate: 0.031
Cost at iteration i=9, j=40: 103.786290
Error rate: 0.032
Cost at iteration i=9, j=50: 105.608610
Error rate: 0.031
Cost at iteration i=9, j=60: 104.998883
Error rate: 0.031
Cost at iteration i=9, j=70: 105.110111
Error rate: 0.031
Cost at iteration i=9, j=80: 103.699204
Error rate: 0.033
Final error rate: 0.033
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>





<div id="78cf68ab-87f3-491f-a888-df7ba634f599"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#78cf68ab-87f3-491f-a888-df7ba634f599');
/* Put everything inside the global mpl namespace */
window.mpl = {};


mpl.get_websocket_type = function() {
    if (typeof(WebSocket) !== 'undefined') {
        return WebSocket;
    } else if (typeof(MozWebSocket) !== 'undefined') {
        return MozWebSocket;
    } else {
        alert('Your browser does not have WebSocket support.' +
              'Please try Chrome, Safari or Firefox ≥ 6. ' +
              'Firefox 4 and 5 are also supported but you ' +
              'have to enable WebSockets in about:config.');
    };
}

mpl.figure = function(figure_id, websocket, ondownload, parent_element) {
    this.id = figure_id;

    this.ws = websocket;

    this.supports_binary = (this.ws.binaryType != undefined);

    if (!this.supports_binary) {
        var warnings = document.getElementById("mpl-warnings");
        if (warnings) {
            warnings.style.display = 'block';
            warnings.textContent = (
                "This browser does not support binary websocket messages. " +
                    "Performance may be slow.");
        }
    }

    this.imageObj = new Image();

    this.context = undefined;
    this.message = undefined;
    this.canvas = undefined;
    this.rubberband_canvas = undefined;
    this.rubberband_context = undefined;
    this.format_dropdown = undefined;

    this.image_mode = 'full';

    this.root = $('<div/>');
    this._root_extra_style(this.root)
    this.root.attr('style', 'display: inline-block');

    $(parent_element).append(this.root);

    this._init_header(this);
    this._init_canvas(this);
    this._init_toolbar(this);

    var fig = this;

    this.waiting = false;

    this.ws.onopen =  function () {
            fig.send_message("supports_binary", {value: fig.supports_binary});
            fig.send_message("send_image_mode", {});
            if (mpl.ratio != 1) {
                fig.send_message("set_dpi_ratio", {'dpi_ratio': mpl.ratio});
            }
            fig.send_message("refresh", {});
        }

    this.imageObj.onload = function() {
            if (fig.image_mode == 'full') {
                // Full images could contain transparency (where diff images
                // almost always do), so we need to clear the canvas so that
                // there is no ghosting.
                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);
            }
            fig.context.drawImage(fig.imageObj, 0, 0);
        };

    this.imageObj.onunload = function() {
        this.ws.close();
    }

    this.ws.onmessage = this._make_on_message_function(this);

    this.ondownload = ondownload;
}

mpl.figure.prototype._init_header = function() {
    var titlebar = $(
        '<div class="ui-dialog-titlebar ui-widget-header ui-corner-all ' +
        'ui-helper-clearfix"/>');
    var titletext = $(
        '<div class="ui-dialog-title" style="width: 100%; ' +
        'text-align: center; padding: 3px;"/>');
    titlebar.append(titletext)
    this.root.append(titlebar);
    this.header = titletext[0];
}



mpl.figure.prototype._canvas_extra_style = function(canvas_div) {

}


mpl.figure.prototype._root_extra_style = function(canvas_div) {

}

mpl.figure.prototype._init_canvas = function() {
    var fig = this;

    var canvas_div = $('<div/>');

    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');

    function canvas_keyboard_event(event) {
        return fig.key_event(event, event['data']);
    }

    canvas_div.keydown('key_press', canvas_keyboard_event);
    canvas_div.keyup('key_release', canvas_keyboard_event);
    this.canvas_div = canvas_div
    this._canvas_extra_style(canvas_div)
    this.root.append(canvas_div);

    var canvas = $('<canvas/>');
    canvas.addClass('mpl-canvas');
    canvas.attr('style', "left: 0; top: 0; z-index: 0; outline: 0")

    this.canvas = canvas[0];
    this.context = canvas[0].getContext("2d");

    var backingStore = this.context.backingStorePixelRatio ||
	this.context.webkitBackingStorePixelRatio ||
	this.context.mozBackingStorePixelRatio ||
	this.context.msBackingStorePixelRatio ||
	this.context.oBackingStorePixelRatio ||
	this.context.backingStorePixelRatio || 1;

    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;

    var rubberband = $('<canvas/>');
    rubberband.attr('style', "position: absolute; left: 0; top: 0; z-index: 1;")

    var pass_mouse_events = true;

    canvas_div.resizable({
        start: function(event, ui) {
            pass_mouse_events = false;
        },
        resize: function(event, ui) {
            fig.request_resize(ui.size.width, ui.size.height);
        },
        stop: function(event, ui) {
            pass_mouse_events = true;
            fig.request_resize(ui.size.width, ui.size.height);
        },
    });

    function mouse_event_fn(event) {
        if (pass_mouse_events)
            return fig.mouse_event(event, event['data']);
    }

    rubberband.mousedown('button_press', mouse_event_fn);
    rubberband.mouseup('button_release', mouse_event_fn);
    // Throttle sequential mouse events to 1 every 20ms.
    rubberband.mousemove('motion_notify', mouse_event_fn);

    rubberband.mouseenter('figure_enter', mouse_event_fn);
    rubberband.mouseleave('figure_leave', mouse_event_fn);

    canvas_div.on("wheel", function (event) {
        event = event.originalEvent;
        event['data'] = 'scroll'
        if (event.deltaY < 0) {
            event.step = 1;
        } else {
            event.step = -1;
        }
        mouse_event_fn(event);
    });

    canvas_div.append(canvas);
    canvas_div.append(rubberband);

    this.rubberband = rubberband;
    this.rubberband_canvas = rubberband[0];
    this.rubberband_context = rubberband[0].getContext("2d");
    this.rubberband_context.strokeStyle = "#000000";

    this._resize_canvas = function(width, height) {
        // Keep the size of the canvas, canvas container, and rubber band
        // canvas in synch.
        canvas_div.css('width', width)
        canvas_div.css('height', height)

        canvas.attr('width', width * mpl.ratio);
        canvas.attr('height', height * mpl.ratio);
        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');

        rubberband.attr('width', width);
        rubberband.attr('height', height);
    }

    // Set the figure to an initial 600x600px, this will subsequently be updated
    // upon first draw.
    this._resize_canvas(600, 600);

    // Disable right mouse context menu.
    $(this.rubberband_canvas).bind("contextmenu",function(e){
        return false;
    });

    function set_focus () {
        canvas.focus();
        canvas_div.focus();
    }

    window.setTimeout(set_focus, 100);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items) {
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) {
            // put a spacer in here.
            continue;
        }
        var button = $('<button/>');
        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +
                        'ui-button-icon-only');
        button.attr('role', 'button');
        button.attr('aria-disabled', 'false');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);

        var icon_img = $('<span/>');
        icon_img.addClass('ui-button-icon-primary ui-icon');
        icon_img.addClass(image);
        icon_img.addClass('ui-corner-all');

        var tooltip_span = $('<span/>');
        tooltip_span.addClass('ui-button-text');
        tooltip_span.html(tooltip);

        button.append(icon_img);
        button.append(tooltip_span);

        nav_element.append(button);
    }

    var fmt_picker_span = $('<span/>');

    var fmt_picker = $('<select/>');
    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');
    fmt_picker_span.append(fmt_picker);
    nav_element.append(fmt_picker_span);
    this.format_dropdown = fmt_picker[0];

    for (var ind in mpl.extensions) {
        var fmt = mpl.extensions[ind];
        var option = $(
            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);
        fmt_picker.append(option)
    }

    // Add hover states to the ui-buttons
    $( ".ui-button" ).hover(
        function() { $(this).addClass("ui-state-hover");},
        function() { $(this).removeClass("ui-state-hover");}
    );

    var status_bar = $('<span class="mpl-message"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];
}

mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {
    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,
    // which will in turn request a refresh of the image.
    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});
}

mpl.figure.prototype.send_message = function(type, properties) {
    properties['type'] = type;
    properties['figure_id'] = this.id;
    this.ws.send(JSON.stringify(properties));
}

mpl.figure.prototype.send_draw_message = function() {
    if (!this.waiting) {
        this.waiting = true;
        this.ws.send(JSON.stringify({type: "draw", figure_id: this.id}));
    }
}


mpl.figure.prototype.handle_save = function(fig, msg) {
    var format_dropdown = fig.format_dropdown;
    var format = format_dropdown.options[format_dropdown.selectedIndex].value;
    fig.ondownload(fig, format);
}


mpl.figure.prototype.handle_resize = function(fig, msg) {
    var size = msg['size'];
    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {
        fig._resize_canvas(size[0], size[1]);
        fig.send_message("refresh", {});
    };
}

mpl.figure.prototype.handle_rubberband = function(fig, msg) {
    var x0 = msg['x0'] / mpl.ratio;
    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;
    var x1 = msg['x1'] / mpl.ratio;
    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;
    x0 = Math.floor(x0) + 0.5;
    y0 = Math.floor(y0) + 0.5;
    x1 = Math.floor(x1) + 0.5;
    y1 = Math.floor(y1) + 0.5;
    var min_x = Math.min(x0, x1);
    var min_y = Math.min(y0, y1);
    var width = Math.abs(x1 - x0);
    var height = Math.abs(y1 - y0);

    fig.rubberband_context.clearRect(
        0, 0, fig.canvas.width, fig.canvas.height);

    fig.rubberband_context.strokeRect(min_x, min_y, width, height);
}

mpl.figure.prototype.handle_figure_label = function(fig, msg) {
    // Updates the figure title.
    fig.header.textContent = msg['label'];
}

mpl.figure.prototype.handle_cursor = function(fig, msg) {
    var cursor = msg['cursor'];
    switch(cursor)
    {
    case 0:
        cursor = 'pointer';
        break;
    case 1:
        cursor = 'default';
        break;
    case 2:
        cursor = 'crosshair';
        break;
    case 3:
        cursor = 'move';
        break;
    }
    fig.rubberband_canvas.style.cursor = cursor;
}

mpl.figure.prototype.handle_message = function(fig, msg) {
    fig.message.textContent = msg['message'];
}

mpl.figure.prototype.handle_draw = function(fig, msg) {
    // Request the server to send over a new figure.
    fig.send_draw_message();
}

mpl.figure.prototype.handle_image_mode = function(fig, msg) {
    fig.image_mode = msg['mode'];
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Called whenever the canvas gets updated.
    this.send_message("ack", {});
}

// A function to construct a web socket function for onmessage handling.
// Called in the figure constructor.
mpl.figure.prototype._make_on_message_function = function(fig) {
    return function socket_on_message(evt) {
        if (evt.data instanceof Blob) {
            /* FIXME: We get "Resource interpreted as Image but
             * transferred with MIME type text/plain:" errors on
             * Chrome.  But how to set the MIME type?  It doesn't seem
             * to be part of the websocket stream */
            evt.data.type = "image/png";

            /* Free the memory for the previous frames */
            if (fig.imageObj.src) {
                (window.URL || window.webkitURL).revokeObjectURL(
                    fig.imageObj.src);
            }

            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(
                evt.data);
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }
        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == "data:image/png;base64") {
            fig.imageObj.src = evt.data;
            fig.updated_canvas_event();
            fig.waiting = false;
            return;
        }

        var msg = JSON.parse(evt.data);
        var msg_type = msg['type'];

        // Call the  "handle_{type}" callback, which takes
        // the figure and JSON message as its only arguments.
        try {
            var callback = fig["handle_" + msg_type];
        } catch (e) {
            console.log("No handler for the '" + msg_type + "' message type: ", msg);
            return;
        }

        if (callback) {
            try {
                // console.log("Handling '" + msg_type + "' message: ", msg);
                callback(fig, msg);
            } catch (e) {
                console.log("Exception inside the 'handler_" + msg_type + "' callback:", e, e.stack, msg);
            }
        }
    };
}

// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas
mpl.findpos = function(e) {
    //this section is from http://www.quirksmode.org/js/events_properties.html
    var targ;
    if (!e)
        e = window.event;
    if (e.target)
        targ = e.target;
    else if (e.srcElement)
        targ = e.srcElement;
    if (targ.nodeType == 3) // defeat Safari bug
        targ = targ.parentNode;

    // jQuery normalizes the pageX and pageY
    // pageX,Y are the mouse positions relative to the document
    // offset() returns the position of the element relative to the document
    var x = e.pageX - $(targ).offset().left;
    var y = e.pageY - $(targ).offset().top;

    return {"x": x, "y": y};
};

/*
 * return a copy of an object with only non-object keys
 * we need this to avoid circular references
 * http://stackoverflow.com/a/24161582/3208463
 */
function simpleKeys (original) {
  return Object.keys(original).reduce(function (obj, key) {
    if (typeof original[key] !== 'object')
        obj[key] = original[key]
    return obj;
  }, {});
}

mpl.figure.prototype.mouse_event = function(event, name) {
    var canvas_pos = mpl.findpos(event)

    if (name === 'button_press')
    {
        this.canvas.focus();
        this.canvas_div.focus();
    }

    var x = canvas_pos.x * mpl.ratio;
    var y = canvas_pos.y * mpl.ratio;

    this.send_message(name, {x: x, y: y, button: event.button,
                             step: event.step,
                             guiEvent: simpleKeys(event)});

    /* This prevents the web browser from automatically changing to
     * the text insertion cursor when the button is pressed.  We want
     * to control all of the cursor setting manually through the
     * 'cursor' event from matplotlib */
    event.preventDefault();
    return false;
}

mpl.figure.prototype._key_event_extra = function(event, name) {
    // Handle any extra behaviour associated with a key event
}

mpl.figure.prototype.key_event = function(event, name) {

    // Prevent repeat events
    if (name == 'key_press')
    {
        if (event.which === this._key)
            return;
        else
            this._key = event.which;
    }
    if (name == 'key_release')
        this._key = null;

    var value = '';
    if (event.ctrlKey && event.which != 17)
        value += "ctrl+";
    if (event.altKey && event.which != 18)
        value += "alt+";
    if (event.shiftKey && event.which != 16)
        value += "shift+";

    value += 'k';
    value += event.which.toString();

    this._key_event_extra(event, name);

    this.send_message(name, {key: value,
                             guiEvent: simpleKeys(event)});
    return false;
}

mpl.figure.prototype.toolbar_button_onclick = function(name) {
    if (name == 'download') {
        this.handle_save(this, null);
    } else {
        this.send_message("toolbar_button", {name: name});
    }
};

mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {
    this.message.textContent = tooltip;
};
mpl.toolbar_items = [["Home", "Reset original view", "fa fa-home icon-home", "home"], ["Back", "Back to  previous view", "fa fa-arrow-left icon-arrow-left", "back"], ["Forward", "Forward to next view", "fa fa-arrow-right icon-arrow-right", "forward"], ["", "", "", ""], ["Pan", "Pan axes with left mouse, zoom with right", "fa fa-arrows icon-move", "pan"], ["Zoom", "Zoom to rectangle", "fa fa-square-o icon-check-empty", "zoom"], ["", "", "", ""], ["Download", "Download plot", "fa fa-floppy-o icon-save", "download"]];

mpl.extensions = ["eps", "jpeg", "pdf", "png", "ps", "raw", "svg", "tif"];

mpl.default_extension = "png";var comm_websocket_adapter = function(comm) {
    // Create a "websocket"-like object which calls the given IPython comm
    // object with the appropriate methods. Currently this is a non binary
    // socket, so there is still some room for performance tuning.
    var ws = {};

    ws.close = function() {
        comm.close()
    };
    ws.send = function(m) {
        //console.log('sending', m);
        comm.send(m);
    };
    // Register the callback with on_msg.
    comm.on_msg(function(msg) {
        //console.log('receiving', msg['content']['data'], msg);
        // Pass the mpl event to the overriden (by mpl) onmessage function.
        ws.onmessage(msg['content']['data'])
    });
    return ws;
}

mpl.mpl_figure_comm = function(comm, msg) {
    // This is the function which gets called when the mpl process
    // starts-up an IPython Comm through the "matplotlib" channel.

    var id = msg.content.data.id;
    // Get hold of the div created by the display call when the Comm
    // socket was opened in Python.
    var element = $("#" + id);
    var ws_proxy = comm_websocket_adapter(comm)

    function ondownload(figure, format) {
        window.open(figure.imageObj.src);
    }

    var fig = new mpl.figure(id, ws_proxy,
                           ondownload,
                           element.get(0));

    // Call onopen now - mpl needs it, as it is assuming we've passed it a real
    // web socket which is closed, not our websocket->open comm proxy.
    ws_proxy.onopen();

    fig.parent_element = element.get(0);
    fig.cell_info = mpl.find_output_cell("<div id='" + id + "'></div>");
    if (!fig.cell_info) {
        console.error("Failed to find cell for figure", id, fig);
        return;
    }

    var output_index = fig.cell_info[2]
    var cell = fig.cell_info[0];

};

mpl.figure.prototype.handle_close = function(fig, msg) {
    var width = fig.canvas.width/mpl.ratio
    fig.root.unbind('remove')

    // Update the output cell to use the data from the current canvas.
    fig.push_to_output();
    var dataURL = fig.canvas.toDataURL();
    // Re-enable the keyboard manager in IPython - without this line, in FF,
    // the notebook keyboard shortcuts fail.
    IPython.keyboard_manager.enable()
    $(fig.parent_element).html('<img src="' + dataURL + '" width="' + width + '">');
    fig.close_ws(fig, msg);
}

mpl.figure.prototype.close_ws = function(fig, msg){
    fig.send_message('closing', msg);
    // fig.ws.close()
}

mpl.figure.prototype.push_to_output = function(remove_interactive) {
    // Turn the data on the canvas into data in the output cell.
    var width = this.canvas.width/mpl.ratio
    var dataURL = this.canvas.toDataURL();
    this.cell_info[1]['text/html'] = '<img src="' + dataURL + '" width="' + width + '">';
}

mpl.figure.prototype.updated_canvas_event = function() {
    // Tell IPython that the notebook contents must change.
    IPython.notebook.set_dirty(true);
    this.send_message("ack", {});
    var fig = this;
    // Wait a second, then push the new image to the DOM so
    // that it is saved nicely (might be nice to debounce this).
    setTimeout(function () { fig.push_to_output() }, 1000);
}

mpl.figure.prototype._init_toolbar = function() {
    var fig = this;

    var nav_element = $('<div/>')
    nav_element.attr('style', 'width: 100%');
    this.root.append(nav_element);

    // Define a callback function for later on.
    function toolbar_event(event) {
        return fig.toolbar_button_onclick(event['data']);
    }
    function toolbar_mouse_event(event) {
        return fig.toolbar_button_onmouseover(event['data']);
    }

    for(var toolbar_ind in mpl.toolbar_items){
        var name = mpl.toolbar_items[toolbar_ind][0];
        var tooltip = mpl.toolbar_items[toolbar_ind][1];
        var image = mpl.toolbar_items[toolbar_ind][2];
        var method_name = mpl.toolbar_items[toolbar_ind][3];

        if (!name) { continue; };

        var button = $('<button class="btn btn-default" href="#" title="' + name + '"><i class="fa ' + image + ' fa-lg"></i></button>');
        button.click(method_name, toolbar_event);
        button.mouseover(tooltip, toolbar_mouse_event);
        nav_element.append(button);
    }

    // Add the status bar.
    var status_bar = $('<span class="mpl-message" style="text-align:right; float: right;"/>');
    nav_element.append(status_bar);
    this.message = status_bar[0];

    // Add the close button to the window.
    var buttongrp = $('<div class="btn-group inline pull-right"></div>');
    var button = $('<button class="btn btn-mini btn-primary" href="#" title="Stop Interaction"><i class="fa fa-power-off icon-remove icon-large"></i></button>');
    button.click(function (evt) { fig.handle_close(fig, {}); } );
    button.mouseover('Stop Interaction', toolbar_mouse_event);
    buttongrp.append(button);
    var titlebar = this.root.find($('.ui-dialog-titlebar'));
    titlebar.prepend(buttongrp);
}

mpl.figure.prototype._root_extra_style = function(el){
    var fig = this
    el.on("remove", function(){
	fig.close_ws(fig, {});
    });
}

mpl.figure.prototype._canvas_extra_style = function(el){
    // this is important to make the div 'focusable
    el.attr('tabindex', 0)
    // reach out to IPython and tell the keyboard manager to turn it's self
    // off when our div gets focus

    // location in version 3
    if (IPython.notebook.keyboard_manager) {
        IPython.notebook.keyboard_manager.register_events(el);
    }
    else {
        // location in version 2
        IPython.keyboard_manager.register_events(el);
    }

}

mpl.figure.prototype._key_event_extra = function(event, name) {
    var manager = IPython.notebook.keyboard_manager;
    if (!manager)
        manager = IPython.keyboard_manager;

    // Check for shift+enter
    if (event.shiftKey && event.which == 13) {
        this.canvas_div.blur();
        // select the cell after this one
        var index = IPython.notebook.find_cell_index(this.cell_info[0]);
        IPython.notebook.select(index + 1);
    }
}

mpl.figure.prototype.handle_save = function(fig, msg) {
    fig.ondownload(fig, null);
}


mpl.find_output_cell = function(html_output) {
    // Return the cell and output element which can be found *uniquely* in the notebook.
    // Note - this is a bit hacky, but it is done because the "notebook_saving.Notebook"
    // IPython event is triggered only after the cells have been serialised, which for
    // our purposes (turning an active figure into a static one), is too late.
    var cells = IPython.notebook.get_cells();
    var ncells = cells.length;
    for (var i=0; i<ncells; i++) {
        var cell = cells[i];
        if (cell.cell_type === 'code'){
            for (var j=0; j<cell.output_area.outputs.length; j++) {
                var data = cell.output_area.outputs[j];
                if (data.data) {
                    // IPython >= 3 moved mimebundle to data attribute of output
                    data = data.data;
                }
                if (data['text/html'] == html_output) {
                    return [cell, data, j];
                }
            }
        }
    }
}

// Register the function which deals with the matplotlib target/channel.
// The kernel may be null if the page has been refreshed.
if (IPython.notebook.kernel != null) {
    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);
}

</script>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<img src="data:image/png;base64,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" width="640">
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So we can see that RMSProp with momentum and Adam actually end up very close to one another. This makes sense because Adam is basically RMSProp with momentum added to it. The big difference is that Adam adds the bias correction step. We can see that this makes a big difference at the very beginning, but that this bias quickly goes away. You observe the same effect if you run just a low pass filter on a random signal. You can see that the output is initially biased towards zero, but that the bias goes away pretty quickly.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
