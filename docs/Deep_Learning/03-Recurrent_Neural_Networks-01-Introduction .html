
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../Projects.html">Projects</a>
</li>
        
<li>
  <a href="../Book_Reviews.html">Book Reviews</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Recurrent-Neural-Networks:-Introduction">1. Recurrent Neural Networks: Introduction<a class="anchor-link" href="#1.-Recurrent-Neural-Networks:-Introduction">&#182;</a></h1><p>As with the notebooks related to Hidden Markov Models, <em><strong>Recurrent Neural Networks</strong></em> are all about learning sequences. But, where Markov Models are limited by the Markov Assumption, Recurrent Neural Networks are <em>not</em>. As a result, they are more expressive and powerful than anything we have seen and haven't made progress on in decades.</p>
<h2 id="1.1-Outline">1.1 Outline<a class="anchor-link" href="#1.1-Outline">&#182;</a></h2><p>So, what will these notebooks contain, and how will they build on the previous notebooks surrounding Neural Networks and Hidden Markov Models?</p>
<blockquote><ul>
<li>In the first section, we are going to add <em>time</em> to our neural networks. This will introduce us to the <em>Simple Recurrent Unit</em>, also known as the <em>Elman Unit</em>.</li>
<li>We will then revisit the XOR problem, but will extend it so that it becomes the <em>parity</em> problem. We will demonstrate that regular feed forward neural networks will have trouble solving this problem, but recurrent networks will work because the key is to treat the input as a sequence. </li>
<li>Next, we will revisit one of the most popular applications of RNN's, <em>Language Modeling</em>. In the Markov Models notebooks we have generated poetry, and discriminate from two different poets just from the sequence of parts of speech tags that they used. We will extend our language model so that it <em>no longer</em> makes the markov assumption. </li>
<li>Another popular application for RNN's is word vectors or word embeddings. The most common technique for this is called <em>word-2-vec</em>, but we will go over how RNN's can also be used for creating word vectors. </li>
<li>We will then look at the very popular <em>LSTM</em>, <em><strong>Long-Short term memory unit</strong></em>, and the more modern and efficient <em>GRU</em>, <em><strong>Gated Recurrent Unit</strong></em>, which has been proven to yield comparable performance. </li>
<li>Finally we will apply these to more practical problems, such as learning a language model from wikipedia, and visualizing the word embeddings as a result.</li>
</ul>
</blockquote>
<h2 id="1.2-Tips">1.2 Tips<a class="anchor-link" href="#1.2-Tips">&#182;</a></h2><p>I will offer a tip that helped me in understanding RNN's: Understand the mechanics first, and worry about the "meaning" later. When we talk about LSTM's, we are going to talk about the ability to remember and forget things. Keep in mind, these are just convenient names that are utilized by way of analogy. We are not actually building something that is remembering or forgetting. They are just mathematical formulas. So, worry about the math and let the meaning come naturally to you.</p>
<p>What you most definitely do <em>not</em> want to do is the opposite; try to understand the meaning without understanding the mechanics. When you do that, the result is usually a sensationalist media article, or a pop science book. This set of notebooks is the opposite of that; we want to understand on a technical level what is happening. Explaining things in layman terms, of thinking of real life analogies is icing on the cake, only if you understand the technicalities.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="2.-Review-of-Import-Deep-Learning-Concepts">2. Review of Import Deep Learning Concepts<a class="anchor-link" href="#2.-Review-of-Import-Deep-Learning-Concepts">&#182;</a></h1><h2 id="2.1-Softmax-Function">2.1 Softmax Function<a class="anchor-link" href="#2.1-Softmax-Function">&#182;</a></h2><p>Let's begin by talking about the Softmax function. The softmax function is what we use to classify <em>two or more</em> classes. It is a bit more complicated to work with than the sigmoid-in particular it's derivative is harder to derive-but, we will let theano and tensorflow take care of that stuff for us. Remeber, all we do here is take an array of numbers, exponentiate them, divide by the sum, and that allows us to interpret the output of the softmax as a probability:</p>
<p>$$y_k=\frac{e^{a_k}}{\sum_je^{a_j}}$$</p>
<p>$$p\big(y=k \mid x \big) =\frac{e^{W_k^T x}}{\sum_je^{W_j^Tx}}$$</p>
<p>Where k represents the class $k$ in the output layer. In other words our $y$ output is going to be a  <strong>(kx1)</strong> vector for a single training example, and an <strong>(Nxk)</strong> matrix when computed for the entire training set.</p>
<p>In code, it will look like:</p>

<pre><code>def softmax(a):
    expA = np.exp(a)
    return expA / expA.sum(axis=1, keepdims=True)</code></pre>
<h2 id="2.2-Backpropagation">2.2 Backpropagation<a class="anchor-link" href="#2.2-Backpropagation">&#182;</a></h2><p>All machine learning models have two main functions, prediction and training. Going in the forward direction is how we do prediction, and at the output we get a probability that tells us what the most likely answer is. For training, we use <em><strong>gradient descent</strong></em>. That just means that we take the a cost function (squared error for regression, cross entropy for classification), calculate its derivative with respect to each parameter, and move the parameters slowly in that direction:</p>
<p>$$W \leftarrow W - \eta \nabla J$$</p>
<p>Eventually, we will hit a local minimum and the slope will be 0, so the weights won't change anymore.</p>
<h2 id="2.3-Unsupervised-Learning">2.3 Unsupervised Learning<a class="anchor-link" href="#2.3-Unsupervised-Learning">&#182;</a></h2><p>We have seen that deep networks can be used to find patterns in data that doesn't have labels (i.e. language). We can <em>learn a probability</em> distribution from the data!</p>
<h2 id="2.4-Markov-Models">2.4 Markov Models<a class="anchor-link" href="#2.4-Markov-Models">&#182;</a></h2><p>Suppose you have a bunch of states; these can represent words in a sentence, the weather, what page of a website you are on, etc. We can define them as:</p>
<p>$$states = \{ 1,2,...,M\}$$</p>
<p>A Markov Model is a model that makes the markov assumption:</p>
<blockquote><p><strong>The next state only depends on the previous state</strong>: <br>
<br>
$$p\big(s(t) \mid s(t-1), s(t-2),...,s(1)\big) = p \big(s(t) \mid s(t-1)\big)$$</p>
</blockquote>
<p>For example, if you want to know whether or not it is going to rain tomorrow, you assume that that only depends on today's weather. As another example, consider the following sentence:</p>
<blockquote><p>"I love dogs and cats."</p>
</blockquote>
<p>Let's say we are trying to predict the last word, which we know is cats. But lets say all we are given is the word "and", it will be impossible to predict "cats" based off of only that information. In this course, we will see that our models <em>no longer make the markov assumption</em>!</p>

</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
