
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../Projects.html">Projects</a>
</li>
        
<li>
  <a href="../Book_Reviews.html">Book Reviews</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="15.-Facial-Expression-Recognition---Theano">15. Facial Expression Recognition - Theano<a class="anchor-link" href="#15.-Facial-Expression-Recognition---Theano">&#182;</a></h1><p>We are now going to go through the facial expression recognition project that we have worked on in the past, but we will use <strong>Theano</strong> as our framework of choice this time! We will be creating a neural network that has 2000 units in the first hidden layer, and 1000 units in the second hidden layer. We can start with our imports.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">shuffle</span>

<span class="o">%</span> <span class="n">matplotlib</span> <span class="n">inline</span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now we can define the utilities that we are going to need.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;----------------------- Function to get data -----------------------------&quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">getData</span><span class="p">(</span><span class="n">balance_ones</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># images are 48x48 = 2304 size vectors</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">first</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../../../data/fer/fer2013.csv&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">first</span><span class="p">:</span>
            <span class="n">first</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">row</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
            <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()])</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">balance_ones</span><span class="p">:</span>
        <span class="c1"># balance the 1 class</span>
        <span class="n">X0</span><span class="p">,</span> <span class="n">Y0</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">!=</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">Y</span><span class="o">!=</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">X1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">])</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Y0</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
  
<span class="sd">&quot;&quot;&quot; --------- Creates indicator (N x K), from an input N x 1 y matrix --------&quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">y2indicator</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">ind</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">ind</span>
  
<span class="sd">&quot;&quot;&quot; ----------- Gives the error rate between targets and predictions ---------------- &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">error_rate</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">targets</span> <span class="o">!=</span> <span class="n">predictions</span><span class="p">)</span>
  
<span class="sd">&quot;&quot;&quot; Rectifier Linear Unit - an activation function that can be used in a neural network &quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
  
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Function to initialize a weight matrix and a bias. M1 is the input size, and M2 is the output size</span>
<span class="sd">W is a matrix of size M1 x M2, which is randomized initialy to a gaussian normal</span>
<span class="sd">We make the standard deviation of this the sqrt of size in + size out</span>
<span class="sd">The bias is initialized as zeros. Each is then turned into float 32s so that they can be used in </span>
<span class="sd">Theano and TensorFlow</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="k">def</span> <span class="nf">init_weight_and_bias</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">M2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">b</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">rmsprop</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="c1"># cache</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>
        <span class="n">new_c</span> <span class="o">=</span> <span class="n">decay</span><span class="o">*</span><span class="n">c</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span><span class="o">*</span><span class="n">g</span><span class="o">*</span><span class="n">g</span>

        <span class="c1"># momentum</span>
        <span class="n">zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">zeros</span><span class="p">)</span>
        <span class="n">new_m</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">m</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">g</span> <span class="o">/</span> <span class="n">T</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">new_c</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

        <span class="c1"># param update</span>
        <span class="n">new_p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="n">new_m</span>

        <span class="c1"># append the updates</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">new_c</span><span class="p">))</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">new_m</span><span class="p">))</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">new_p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">updates</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, we want to put our hidden layer into it's own class. We want to do this so we can add an arbitrary number of hidden layers more easily.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">HiddenLayer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">,</span> <span class="n">an_id</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">id</span> <span class="o">=</span> <span class="nb">id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">M1</span> <span class="o">=</span> <span class="n">M1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">=</span> <span class="n">M2</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">init_weight_and_bias</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">)</span>           <span class="c1"># Getting initial weights and bias&#39;s</span>
    
    <span class="sd">&quot;&quot;&quot;Recall, in theano a shared variable is an updatable variable&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="s1">&#39;W_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>   <span class="c1"># Unique name associated with id</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="s1">&#39;W_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>                <span class="c1"># Keep all params in 1 list to calc grad</span>
    
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">relu</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can define our <strong>ANN</strong> class. It will take in the hidden layer sizes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">rmsprop</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="c1"># cache</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>
        <span class="n">new_c</span> <span class="o">=</span> <span class="n">decay</span><span class="o">*</span><span class="n">c</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span><span class="o">*</span><span class="n">g</span><span class="o">*</span><span class="n">g</span>

        <span class="c1"># momentum</span>
        <span class="n">zeros</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_value</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">zeros</span><span class="p">)</span>
        <span class="n">new_m</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">m</span> <span class="o">-</span> <span class="n">lr</span><span class="o">*</span><span class="n">g</span> <span class="o">/</span> <span class="n">T</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">new_c</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

        <span class="c1"># param update</span>
        <span class="n">new_p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">+</span> <span class="n">new_m</span>

        <span class="c1"># append the updates</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">new_c</span><span class="p">))</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">new_m</span><span class="p">))</span>
        <span class="n">updates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">p</span><span class="p">,</span> <span class="n">new_p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">updates</span>


<span class="k">class</span> <span class="nc">HiddenLayer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">,</span> <span class="n">an_id</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">an_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M1</span> <span class="o">=</span> <span class="n">M1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M2</span> <span class="o">=</span> <span class="n">M2</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">init_weight_and_bias</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="s1">&#39;W_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="s1">&#39;b_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">relu</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ANN</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="n">hidden_layer_sizes</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_sz</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">show_fig</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">decay</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">decay</span><span class="p">)</span>
        <span class="n">reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">reg</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span>

        <span class="c1"># make a validation set</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">Xvalid</span><span class="p">,</span> <span class="n">Yvalid</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:],</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:]</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">]</span>

        <span class="c1"># initialize hidden layers</span>
        <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">M1</span> <span class="o">=</span> <span class="n">D</span>
        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">M2</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer_sizes</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">HiddenLayer</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span> <span class="n">M2</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
            <span class="n">M1</span> <span class="o">=</span> <span class="n">M2</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">init_weight_and_bias</span><span class="p">(</span><span class="n">M1</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="s1">&#39;W_logreg&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="s1">&#39;b_logreg&#39;</span><span class="p">)</span>

        <span class="c1"># collect params for later use</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">+=</span> <span class="n">h</span><span class="o">.</span><span class="n">params</span>

        <span class="c1"># set up theano functions and variables</span>
        <span class="n">thX</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">fmatrix</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
        <span class="n">thY</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
        <span class="n">pY</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">th_forward</span><span class="p">(</span><span class="n">thX</span><span class="p">)</span>

        <span class="n">rcost</span> <span class="o">=</span> <span class="n">reg</span><span class="o">*</span><span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">([(</span><span class="n">p</span><span class="o">*</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">])</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">T</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pY</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">thY</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">thY</span><span class="p">]))</span> <span class="o">+</span> <span class="n">rcost</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">th_predict</span><span class="p">(</span><span class="n">thX</span><span class="p">)</span>

        <span class="c1"># actual prediction function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_op</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">thX</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">prediction</span><span class="p">)</span>
        <span class="n">cost_predict_op</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">thX</span><span class="p">,</span> <span class="n">thY</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">cost</span><span class="p">,</span> <span class="n">prediction</span><span class="p">])</span>

        <span class="n">updates</span> <span class="o">=</span> <span class="n">rmsprop</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">train_op</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">thX</span><span class="p">,</span> <span class="n">thY</span><span class="p">],</span>
            <span class="n">updates</span><span class="o">=</span><span class="n">updates</span>
        <span class="p">)</span>

        <span class="n">n_batches</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">batch_sz</span>
        <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
                <span class="n">Xbatch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="o">+</span><span class="n">batch_sz</span><span class="p">)]</span>
                <span class="n">Ybatch</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="o">+</span><span class="n">batch_sz</span><span class="p">)]</span>

                <span class="n">train_op</span><span class="p">(</span><span class="n">Xbatch</span><span class="p">,</span> <span class="n">Ybatch</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">c</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">cost_predict_op</span><span class="p">(</span><span class="n">Xvalid</span><span class="p">,</span> <span class="n">Yvalid</span><span class="p">)</span>
                    <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
                    <span class="n">e</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">Yvalid</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;i:&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;j:&quot;</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="s2">&quot;nb:&quot;</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">,</span> <span class="s2">&quot;cost:&quot;</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="s2">&quot;error rate:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">show_fig</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">th_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layers</span><span class="p">:</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">th_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">pY</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">th_forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">T</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_op</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we finally have our main method. We are going to create a model that contains 2000 units in the first hidden layer, and 1000 units in the second hidden layer.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">getData</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ANN</span><span class="p">([</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">show_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>i: 0 j: 0 nb: 1308 cost: 1.9600968 error rate: 0.86
i: 0 j: 20 nb: 1308 cost: 1.9326383 error rate: 0.821
i: 0 j: 40 nb: 1308 cost: 1.9993237 error rate: 0.814
i: 0 j: 60 nb: 1308 cost: 1.974063 error rate: 0.792
i: 0 j: 80 nb: 1308 cost: 2.0640376 error rate: 0.859
i: 0 j: 100 nb: 1308 cost: 1.9693612 error rate: 0.781
i: 0 j: 120 nb: 1308 cost: 1.9572076 error rate: 0.844
i: 0 j: 140 nb: 1308 cost: 2.1412694 error rate: 0.833
i: 0 j: 160 nb: 1308 cost: 2.0697033 error rate: 0.853
i: 0 j: 180 nb: 1308 cost: 2.0310144 error rate: 0.789
i: 0 j: 200 nb: 1308 cost: 2.3919935 error rate: 0.863
i: 0 j: 220 nb: 1308 cost: 1.9690156 error rate: 0.825
i: 0 j: 240 nb: 1308 cost: 2.9617116 error rate: 0.859
i: 0 j: 260 nb: 1308 cost: 4.2984 error rate: 0.865
i: 0 j: 280 nb: 1308 cost: 1.9451818 error rate: 0.838
i: 0 j: 300 nb: 1308 cost: 1.9405148 error rate: 0.856
i: 0 j: 320 nb: 1308 cost: 2.0670571 error rate: 0.788
i: 0 j: 340 nb: 1308 cost: 1.9534435 error rate: 0.789
i: 0 j: 360 nb: 1308 cost: 1.9344307 error rate: 0.789
i: 0 j: 380 nb: 1308 cost: 1.9645922 error rate: 0.789
i: 0 j: 400 nb: 1308 cost: 5.3975463 error rate: 0.863
i: 0 j: 420 nb: 1308 cost: 1.9336445 error rate: 0.844
i: 0 j: 440 nb: 1308 cost: 1.969696 error rate: 0.788
i: 0 j: 460 nb: 1308 cost: 1.9291116 error rate: 0.789
i: 0 j: 480 nb: 1308 cost: 2.2121067 error rate: 0.863
i: 0 j: 500 nb: 1308 cost: 1.9315329 error rate: 0.789
i: 0 j: 520 nb: 1308 cost: 1.9290155 error rate: 0.789
i: 0 j: 540 nb: 1308 cost: 1.9861394 error rate: 0.789
i: 0 j: 560 nb: 1308 cost: 1.9405789 error rate: 0.789
i: 0 j: 580 nb: 1308 cost: 1.9331108 error rate: 0.789
i: 0 j: 600 nb: 1308 cost: 1.9262918 error rate: 0.789
i: 0 j: 620 nb: 1308 cost: 1.9273973 error rate: 0.789
i: 0 j: 640 nb: 1308 cost: 2.137092 error rate: 0.789
i: 0 j: 660 nb: 1308 cost: 1.9274015 error rate: 0.789
i: 0 j: 680 nb: 1308 cost: 2.0030372 error rate: 0.789
i: 0 j: 700 nb: 1308 cost: 1.9281007 error rate: 0.789
i: 0 j: 720 nb: 1308 cost: 1.9311236 error rate: 0.793
i: 0 j: 740 nb: 1308 cost: 1.930942 error rate: 0.789
i: 0 j: 760 nb: 1308 cost: 1.9280195 error rate: 0.789
i: 0 j: 780 nb: 1308 cost: 1.9353257 error rate: 0.79
i: 0 j: 800 nb: 1308 cost: 1.9263757 error rate: 0.789
i: 0 j: 820 nb: 1308 cost: 1.9285902 error rate: 0.789
i: 0 j: 840 nb: 1308 cost: 1.9283326 error rate: 0.789
i: 0 j: 860 nb: 1308 cost: 1.925663 error rate: 0.789
i: 0 j: 880 nb: 1308 cost: 1.928249 error rate: 0.789
i: 0 j: 900 nb: 1308 cost: 1.9854661 error rate: 0.789
i: 0 j: 920 nb: 1308 cost: 1.9272938 error rate: 0.789
i: 0 j: 940 nb: 1308 cost: 1.9293319 error rate: 0.789
i: 0 j: 960 nb: 1308 cost: 1.930688 error rate: 0.789
i: 0 j: 980 nb: 1308 cost: 3.546058 error rate: 0.79
i: 0 j: 1000 nb: 1308 cost: 1.9294019 error rate: 0.79
i: 0 j: 1020 nb: 1308 cost: 1.9305552 error rate: 0.789
i: 0 j: 1040 nb: 1308 cost: 1.9274943 error rate: 0.789
i: 0 j: 1060 nb: 1308 cost: 1.9272165 error rate: 0.789
i: 0 j: 1080 nb: 1308 cost: 1.9289182 error rate: 0.789
i: 0 j: 1100 nb: 1308 cost: 1.936025 error rate: 0.842
i: 0 j: 1120 nb: 1308 cost: 1.929261 error rate: 0.788
i: 0 j: 1140 nb: 1308 cost: 1.9257916 error rate: 0.789
i: 0 j: 1160 nb: 1308 cost: 1.9244026 error rate: 0.789
i: 0 j: 1180 nb: 1308 cost: 1.9233466 error rate: 0.789
i: 0 j: 1200 nb: 1308 cost: 1.925046 error rate: 0.789
i: 0 j: 1220 nb: 1308 cost: 1.9250493 error rate: 0.789
i: 0 j: 1240 nb: 1308 cost: 2.445864 error rate: 0.835
i: 0 j: 1260 nb: 1308 cost: 1.9296873 error rate: 0.789
i: 0 j: 1280 nb: 1308 cost: 1.9262257 error rate: 0.789
i: 0 j: 1300 nb: 1308 cost: 1.928734 error rate: 0.788
i: 1 j: 0 nb: 1308 cost: 1.9274153 error rate: 0.789
i: 1 j: 20 nb: 1308 cost: 1.9282556 error rate: 0.79
i: 1 j: 40 nb: 1308 cost: 2.1854393 error rate: 0.821
i: 1 j: 60 nb: 1308 cost: 1.9288077 error rate: 0.79
i: 1 j: 80 nb: 1308 cost: 1.9273546 error rate: 0.789
i: 1 j: 100 nb: 1308 cost: 1.925882 error rate: 0.789
i: 1 j: 120 nb: 1308 cost: 1.9271579 error rate: 0.789
i: 1 j: 140 nb: 1308 cost: 1.9276342 error rate: 0.789
i: 1 j: 160 nb: 1308 cost: 1.9278979 error rate: 0.788
i: 1 j: 180 nb: 1308 cost: 1.9272563 error rate: 0.788
i: 1 j: 200 nb: 1308 cost: 1.9246976 error rate: 0.789
i: 1 j: 220 nb: 1308 cost: 1.92681 error rate: 0.789
i: 1 j: 240 nb: 1308 cost: 1.9263301 error rate: 0.789
i: 1 j: 260 nb: 1308 cost: 1.9257641 error rate: 0.79
i: 1 j: 280 nb: 1308 cost: 1.925494 error rate: 0.789
i: 1 j: 300 nb: 1308 cost: 1.9264581 error rate: 0.789
i: 1 j: 320 nb: 1308 cost: 1.9297161 error rate: 0.789
i: 1 j: 340 nb: 1308 cost: 1.9280138 error rate: 0.789
i: 1 j: 360 nb: 1308 cost: 1.9271472 error rate: 0.789
i: 1 j: 380 nb: 1308 cost: 1.9296726 error rate: 0.789
i: 1 j: 400 nb: 1308 cost: 1.9273375 error rate: 0.789
i: 1 j: 420 nb: 1308 cost: 1.9255503 error rate: 0.789
i: 1 j: 440 nb: 1308 cost: 1.9257944 error rate: 0.789
i: 1 j: 460 nb: 1308 cost: 1.9263853 error rate: 0.789
i: 1 j: 480 nb: 1308 cost: 1.9282466 error rate: 0.789
i: 1 j: 500 nb: 1308 cost: 1.9313726 error rate: 0.789
i: 1 j: 520 nb: 1308 cost: 1.9340677 error rate: 0.789
i: 1 j: 540 nb: 1308 cost: 1.9307196 error rate: 0.789
i: 1 j: 560 nb: 1308 cost: 1.9295579 error rate: 0.789
i: 1 j: 580 nb: 1308 cost: 1.93036 error rate: 0.789
i: 1 j: 600 nb: 1308 cost: 1.929747 error rate: 0.789
i: 1 j: 620 nb: 1308 cost: 1.9296526 error rate: 0.789
i: 1 j: 640 nb: 1308 cost: 1.928709 error rate: 0.789
i: 1 j: 660 nb: 1308 cost: 1.9298744 error rate: 0.789
i: 1 j: 680 nb: 1308 cost: 1.937417 error rate: 0.789
i: 1 j: 700 nb: 1308 cost: 1.9295555 error rate: 0.789
i: 1 j: 720 nb: 1308 cost: 1.9283264 error rate: 0.789
i: 1 j: 740 nb: 1308 cost: 1.9269034 error rate: 0.789
i: 1 j: 760 nb: 1308 cost: 1.9257163 error rate: 0.789
i: 1 j: 780 nb: 1308 cost: 1.924526 error rate: 0.789
i: 1 j: 800 nb: 1308 cost: 1.9245574 error rate: 0.789
i: 1 j: 820 nb: 1308 cost: 1.924814 error rate: 0.789
i: 1 j: 840 nb: 1308 cost: 1.9254591 error rate: 0.789
i: 1 j: 860 nb: 1308 cost: 1.9272584 error rate: 0.789
i: 1 j: 880 nb: 1308 cost: 1.9297862 error rate: 0.789
i: 1 j: 900 nb: 1308 cost: 1.9298795 error rate: 0.789
i: 1 j: 920 nb: 1308 cost: 1.9285538 error rate: 0.789
i: 1 j: 940 nb: 1308 cost: 1.9279157 error rate: 0.789
i: 1 j: 960 nb: 1308 cost: 1.9269477 error rate: 0.789
i: 1 j: 980 nb: 1308 cost: 1.9269766 error rate: 0.789
i: 1 j: 1000 nb: 1308 cost: 1.927372 error rate: 0.789
i: 1 j: 1020 nb: 1308 cost: 1.9264969 error rate: 0.789
i: 1 j: 1040 nb: 1308 cost: 1.9264976 error rate: 0.789
i: 1 j: 1060 nb: 1308 cost: 1.926073 error rate: 0.789
i: 1 j: 1080 nb: 1308 cost: 1.9264979 error rate: 0.789
i: 1 j: 1100 nb: 1308 cost: 1.9316719 error rate: 0.798
i: 1 j: 1120 nb: 1308 cost: 1.9298369 error rate: 0.789
i: 1 j: 1140 nb: 1308 cost: 1.9281722 error rate: 0.789
i: 1 j: 1160 nb: 1308 cost: 1.9267855 error rate: 0.79
i: 1 j: 1180 nb: 1308 cost: 1.9246607 error rate: 0.788
i: 1 j: 1200 nb: 1308 cost: 1.9455155 error rate: 0.788
i: 1 j: 1220 nb: 1308 cost: 1.9263973 error rate: 0.789
i: 1 j: 1240 nb: 1308 cost: 1.9302982 error rate: 0.79
i: 1 j: 1260 nb: 1308 cost: 1.9299201 error rate: 0.789
i: 1 j: 1280 nb: 1308 cost: 1.928326 error rate: 0.789
i: 1 j: 1300 nb: 1308 cost: 1.9286051 error rate: 0.789
i: 2 j: 0 nb: 1308 cost: 1.9289194 error rate: 0.789
i: 2 j: 20 nb: 1308 cost: 1.9291619 error rate: 0.789
i: 2 j: 40 nb: 1308 cost: 1.926715 error rate: 0.789
i: 2 j: 60 nb: 1308 cost: 1.9267011 error rate: 0.789
i: 2 j: 80 nb: 1308 cost: 1.925863 error rate: 0.789
i: 2 j: 100 nb: 1308 cost: 1.9254769 error rate: 0.789
i: 2 j: 120 nb: 1308 cost: 1.9264581 error rate: 0.789
i: 2 j: 140 nb: 1308 cost: 1.9275635 error rate: 0.789
i: 2 j: 160 nb: 1308 cost: 1.9279593 error rate: 0.789
i: 2 j: 180 nb: 1308 cost: 1.9275194 error rate: 0.789
i: 2 j: 200 nb: 1308 cost: 1.9278306 error rate: 0.789
i: 2 j: 220 nb: 1308 cost: 1.9275943 error rate: 0.789
i: 2 j: 240 nb: 1308 cost: 1.9271764 error rate: 0.789
i: 2 j: 260 nb: 1308 cost: 1.9255534 error rate: 0.789
i: 2 j: 280 nb: 1308 cost: 1.9250325 error rate: 0.789
i: 2 j: 300 nb: 1308 cost: 1.9259413 error rate: 0.789
i: 2 j: 320 nb: 1308 cost: 1.9280107 error rate: 0.789
i: 2 j: 340 nb: 1308 cost: 1.9265481 error rate: 0.789
i: 2 j: 360 nb: 1308 cost: 1.9255563 error rate: 0.788
i: 2 j: 380 nb: 1308 cost: 1.9251419 error rate: 0.789
i: 2 j: 400 nb: 1308 cost: 1.9275148 error rate: 0.789
i: 2 j: 420 nb: 1308 cost: 1.9279933 error rate: 0.789
i: 2 j: 440 nb: 1308 cost: 1.9266092 error rate: 0.789
i: 2 j: 460 nb: 1308 cost: 1.9248382 error rate: 0.789
i: 2 j: 480 nb: 1308 cost: 1.9248874 error rate: 0.788
i: 2 j: 500 nb: 1308 cost: 1.9268639 error rate: 0.789
i: 2 j: 520 nb: 1308 cost: 1.9260527 error rate: 0.789
i: 2 j: 540 nb: 1308 cost: 1.9255645 error rate: 0.789
i: 2 j: 560 nb: 1308 cost: 1.9254756 error rate: 0.789
i: 2 j: 580 nb: 1308 cost: 1.9265329 error rate: 0.789
i: 2 j: 600 nb: 1308 cost: 1.9252056 error rate: 0.789
i: 2 j: 620 nb: 1308 cost: 1.925992 error rate: 0.789
i: 2 j: 640 nb: 1308 cost: 1.928254 error rate: 0.789
i: 2 j: 660 nb: 1308 cost: 1.9281934 error rate: 0.789
i: 2 j: 680 nb: 1308 cost: 1.9274412 error rate: 0.789
i: 2 j: 700 nb: 1308 cost: 1.9268596 error rate: 0.79
i: 2 j: 720 nb: 1308 cost: 1.9267217 error rate: 0.789
i: 2 j: 740 nb: 1308 cost: 1.9274473 error rate: 0.79
i: 2 j: 760 nb: 1308 cost: 1.9269769 error rate: 0.788
i: 2 j: 780 nb: 1308 cost: 1.9280994 error rate: 0.789
i: 2 j: 800 nb: 1308 cost: 1.9282677 error rate: 0.79
i: 2 j: 820 nb: 1308 cost: 1.9263415 error rate: 0.789
i: 2 j: 840 nb: 1308 cost: 1.9301753 error rate: 0.789
i: 2 j: 860 nb: 1308 cost: 1.9282435 error rate: 0.789
i: 2 j: 880 nb: 1308 cost: 1.9267455 error rate: 0.789
i: 2 j: 900 nb: 1308 cost: 1.9263834 error rate: 0.789
i: 2 j: 920 nb: 1308 cost: 1.9279094 error rate: 0.789
i: 2 j: 940 nb: 1308 cost: 1.928787 error rate: 0.789
i: 2 j: 960 nb: 1308 cost: 2.544893 error rate: 0.789
i: 2 j: 980 nb: 1308 cost: 1.9269984 error rate: 0.789
i: 2 j: 1000 nb: 1308 cost: 1.9258207 error rate: 0.789
i: 2 j: 1020 nb: 1308 cost: 1.9254777 error rate: 0.789
i: 2 j: 1040 nb: 1308 cost: 1.9268359 error rate: 0.789
i: 2 j: 1060 nb: 1308 cost: 1.9274381 error rate: 0.789
i: 2 j: 1080 nb: 1308 cost: 1.9291972 error rate: 0.789
i: 2 j: 1100 nb: 1308 cost: 1.9282752 error rate: 0.789
i: 2 j: 1120 nb: 1308 cost: 1.9305087 error rate: 0.789
i: 2 j: 1140 nb: 1308 cost: 1.932946 error rate: 0.789
i: 2 j: 1160 nb: 1308 cost: 1.9300828 error rate: 0.789
i: 2 j: 1180 nb: 1308 cost: 1.9282584 error rate: 0.789
i: 2 j: 1200 nb: 1308 cost: 1.9281523 error rate: 0.789
i: 2 j: 1220 nb: 1308 cost: 1.9286903 error rate: 0.789
i: 2 j: 1240 nb: 1308 cost: 1.9275593 error rate: 0.789
i: 2 j: 1260 nb: 1308 cost: 1.9278893 error rate: 0.789
i: 2 j: 1280 nb: 1308 cost: 1.9284737 error rate: 0.789
i: 2 j: 1300 nb: 1308 cost: 1.9294668 error rate: 0.789
i: 3 j: 0 nb: 1308 cost: 1.9296255 error rate: 0.789
i: 3 j: 20 nb: 1308 cost: 1.9296972 error rate: 0.789
i: 3 j: 40 nb: 1308 cost: 1.9297032 error rate: 0.789
i: 3 j: 60 nb: 1308 cost: 1.9290905 error rate: 0.789
i: 3 j: 80 nb: 1308 cost: 1.9293672 error rate: 0.789
i: 3 j: 100 nb: 1308 cost: 1.9292774 error rate: 0.789
i: 3 j: 120 nb: 1308 cost: 1.9283903 error rate: 0.789
i: 3 j: 140 nb: 1308 cost: 1.9283897 error rate: 0.789
i: 3 j: 160 nb: 1308 cost: 1.9276152 error rate: 0.789
i: 3 j: 180 nb: 1308 cost: 1.928108 error rate: 0.789
i: 3 j: 200 nb: 1308 cost: 1.9266742 error rate: 0.789
i: 3 j: 220 nb: 1308 cost: 1.9273871 error rate: 0.789
i: 3 j: 240 nb: 1308 cost: 1.9272538 error rate: 0.789
i: 3 j: 260 nb: 1308 cost: 1.9267428 error rate: 0.789
i: 3 j: 280 nb: 1308 cost: 1.9260736 error rate: 0.789
i: 3 j: 300 nb: 1308 cost: 1.9260852 error rate: 0.789
i: 3 j: 320 nb: 1308 cost: 1.927456 error rate: 0.789
i: 3 j: 340 nb: 1308 cost: 1.9280291 error rate: 0.789
i: 3 j: 360 nb: 1308 cost: 1.9282405 error rate: 0.789
i: 3 j: 380 nb: 1308 cost: 1.9323761 error rate: 0.789
i: 3 j: 400 nb: 1308 cost: 1.9244168 error rate: 0.789
i: 3 j: 420 nb: 1308 cost: 1.9253566 error rate: 0.789
i: 3 j: 440 nb: 1308 cost: 1.9273272 error rate: 0.789
i: 3 j: 460 nb: 1308 cost: 1.9253271 error rate: 0.789
i: 3 j: 480 nb: 1308 cost: 1.926416 error rate: 0.789
i: 3 j: 500 nb: 1308 cost: 1.9285085 error rate: 0.789
i: 3 j: 520 nb: 1308 cost: 1.9370718 error rate: 0.789
i: 3 j: 540 nb: 1308 cost: 1.9254866 error rate: 0.789
i: 3 j: 560 nb: 1308 cost: 1.9265872 error rate: 0.789
i: 3 j: 580 nb: 1308 cost: 1.9266706 error rate: 0.789
i: 3 j: 600 nb: 1308 cost: 1.9244821 error rate: 0.789
i: 3 j: 620 nb: 1308 cost: 1.9249223 error rate: 0.789
i: 3 j: 640 nb: 1308 cost: 1.9250251 error rate: 0.789
i: 3 j: 660 nb: 1308 cost: 1.9257115 error rate: 0.789
i: 3 j: 680 nb: 1308 cost: 1.924992 error rate: 0.789
i: 3 j: 700 nb: 1308 cost: 1.9246516 error rate: 0.789
i: 3 j: 720 nb: 1308 cost: 1.9249194 error rate: 0.789
i: 3 j: 740 nb: 1308 cost: 1.9268018 error rate: 0.789
i: 3 j: 760 nb: 1308 cost: 1.9264426 error rate: 0.789
i: 3 j: 780 nb: 1308 cost: 1.9258552 error rate: 0.789
i: 3 j: 800 nb: 1308 cost: 1.9275829 error rate: 0.789
i: 3 j: 820 nb: 1308 cost: 1.9272449 error rate: 0.789
i: 3 j: 840 nb: 1308 cost: 1.9266582 error rate: 0.789
i: 3 j: 860 nb: 1308 cost: 1.9257396 error rate: 0.789
i: 3 j: 880 nb: 1308 cost: 1.9265895 error rate: 0.789
i: 3 j: 900 nb: 1308 cost: 1.9250852 error rate: 0.789
i: 3 j: 920 nb: 1308 cost: 1.92567 error rate: 0.789
i: 3 j: 940 nb: 1308 cost: 1.9256543 error rate: 0.789
i: 3 j: 960 nb: 1308 cost: 1.9255366 error rate: 0.789
i: 3 j: 980 nb: 1308 cost: 1.9277928 error rate: 0.789
i: 3 j: 1000 nb: 1308 cost: 1.928687 error rate: 0.789
i: 3 j: 1020 nb: 1308 cost: 1.9286485 error rate: 0.789
i: 3 j: 1040 nb: 1308 cost: 1.9283109 error rate: 0.789
i: 3 j: 1060 nb: 1308 cost: 1.9272156 error rate: 0.789
i: 3 j: 1080 nb: 1308 cost: 1.9265348 error rate: 0.789
i: 3 j: 1100 nb: 1308 cost: 1.9279743 error rate: 0.789
i: 3 j: 1120 nb: 1308 cost: 1.9280534 error rate: 0.789
i: 3 j: 1140 nb: 1308 cost: 1.9278867 error rate: 0.789
i: 3 j: 1160 nb: 1308 cost: 1.9278477 error rate: 0.789
i: 3 j: 1180 nb: 1308 cost: 1.9280943 error rate: 0.789
i: 3 j: 1200 nb: 1308 cost: 1.9292291 error rate: 0.789
i: 3 j: 1220 nb: 1308 cost: 1.9271468 error rate: 0.789
i: 3 j: 1240 nb: 1308 cost: 1.9278908 error rate: 0.789
i: 3 j: 1260 nb: 1308 cost: 1.9276528 error rate: 0.789
i: 3 j: 1280 nb: 1308 cost: 1.9280906 error rate: 0.789
i: 3 j: 1300 nb: 1308 cost: 1.9259477 error rate: 0.789
i: 4 j: 0 nb: 1308 cost: 1.9254365 error rate: 0.789
i: 4 j: 20 nb: 1308 cost: 1.9259241 error rate: 0.789
i: 4 j: 40 nb: 1308 cost: 1.9269885 error rate: 0.789
i: 4 j: 60 nb: 1308 cost: 1.9280071 error rate: 0.788
i: 4 j: 80 nb: 1308 cost: 1.9278597 error rate: 0.789
i: 4 j: 100 nb: 1308 cost: 1.9273576 error rate: 0.789
i: 4 j: 120 nb: 1308 cost: 1.9282101 error rate: 0.789
i: 4 j: 140 nb: 1308 cost: 1.9273131 error rate: 0.789
i: 4 j: 160 nb: 1308 cost: 1.9259586 error rate: 0.789
i: 4 j: 180 nb: 1308 cost: 1.9264922 error rate: 0.789
i: 4 j: 200 nb: 1308 cost: 1.9273237 error rate: 0.789
i: 4 j: 220 nb: 1308 cost: 1.926893 error rate: 0.789
i: 4 j: 240 nb: 1308 cost: 1.9277275 error rate: 0.789
i: 4 j: 260 nb: 1308 cost: 1.9273682 error rate: 0.789
i: 4 j: 280 nb: 1308 cost: 1.9275875 error rate: 0.789
i: 4 j: 300 nb: 1308 cost: 1.926782 error rate: 0.789
i: 4 j: 320 nb: 1308 cost: 1.9256463 error rate: 0.789
i: 4 j: 340 nb: 1308 cost: 1.9263653 error rate: 0.789
i: 4 j: 360 nb: 1308 cost: 1.9257574 error rate: 0.789
i: 4 j: 380 nb: 1308 cost: 1.9270061 error rate: 0.789
i: 4 j: 400 nb: 1308 cost: 1.9287987 error rate: 0.789
i: 4 j: 420 nb: 1308 cost: 1.9294044 error rate: 0.789
i: 4 j: 440 nb: 1308 cost: 1.9275277 error rate: 0.789
i: 4 j: 460 nb: 1308 cost: 1.9270561 error rate: 0.789
i: 4 j: 480 nb: 1308 cost: 1.9267203 error rate: 0.789
i: 4 j: 500 nb: 1308 cost: 1.9276965 error rate: 0.789
i: 4 j: 520 nb: 1308 cost: 1.92721 error rate: 0.789
i: 4 j: 540 nb: 1308 cost: 1.9263842 error rate: 0.789
i: 4 j: 560 nb: 1308 cost: 1.9261937 error rate: 0.789
i: 4 j: 580 nb: 1308 cost: 1.9273423 error rate: 0.789
i: 4 j: 600 nb: 1308 cost: 1.9270396 error rate: 0.789
i: 4 j: 620 nb: 1308 cost: 1.9265834 error rate: 0.789
i: 4 j: 640 nb: 1308 cost: 1.9271472 error rate: 0.789
i: 4 j: 660 nb: 1308 cost: 1.9264166 error rate: 0.789
i: 4 j: 680 nb: 1308 cost: 1.9269034 error rate: 0.789
i: 4 j: 700 nb: 1308 cost: 1.927229 error rate: 0.789
i: 4 j: 720 nb: 1308 cost: 1.927414 error rate: 0.789
i: 4 j: 740 nb: 1308 cost: 1.9263638 error rate: 0.789
i: 4 j: 760 nb: 1308 cost: 1.9276469 error rate: 0.789
i: 4 j: 780 nb: 1308 cost: 1.9280436 error rate: 0.789
i: 4 j: 800 nb: 1308 cost: 1.9268202 error rate: 0.789
i: 4 j: 820 nb: 1308 cost: 1.9271905 error rate: 0.789
i: 4 j: 840 nb: 1308 cost: 1.9274355 error rate: 0.789
i: 4 j: 860 nb: 1308 cost: 1.9283943 error rate: 0.789
i: 4 j: 880 nb: 1308 cost: 1.9278791 error rate: 0.789
i: 4 j: 900 nb: 1308 cost: 1.9271013 error rate: 0.789
i: 4 j: 920 nb: 1308 cost: 1.9276682 error rate: 0.789
i: 4 j: 940 nb: 1308 cost: 1.9285917 error rate: 0.789
i: 4 j: 960 nb: 1308 cost: 1.9256511 error rate: 0.789
i: 4 j: 980 nb: 1308 cost: 1.9256104 error rate: 0.789
i: 4 j: 1000 nb: 1308 cost: 1.9260243 error rate: 0.789
i: 4 j: 1020 nb: 1308 cost: 1.9254693 error rate: 0.789
i: 4 j: 1040 nb: 1308 cost: 1.925481 error rate: 0.789
i: 4 j: 1060 nb: 1308 cost: 1.9270082 error rate: 0.789
i: 4 j: 1080 nb: 1308 cost: 1.9267545 error rate: 0.789
i: 4 j: 1100 nb: 1308 cost: 1.9253944 error rate: 0.789
i: 4 j: 1120 nb: 1308 cost: 1.926246 error rate: 0.789
i: 4 j: 1140 nb: 1308 cost: 1.9274472 error rate: 0.789
i: 4 j: 1160 nb: 1308 cost: 1.9255048 error rate: 0.789
i: 4 j: 1180 nb: 1308 cost: 1.9259347 error rate: 0.789
i: 4 j: 1200 nb: 1308 cost: 1.9255577 error rate: 0.789
i: 4 j: 1220 nb: 1308 cost: 1.943319 error rate: 0.789
i: 4 j: 1240 nb: 1308 cost: 1.926425 error rate: 0.789
i: 4 j: 1260 nb: 1308 cost: 1.928543 error rate: 0.789
i: 4 j: 1280 nb: 1308 cost: 1.9290622 error rate: 0.789
i: 4 j: 1300 nb: 1308 cost: 1.928374 error rate: 0.789
i: 5 j: 0 nb: 1308 cost: 1.9291532 error rate: 0.789
i: 5 j: 20 nb: 1308 cost: 1.9283859 error rate: 0.789
i: 5 j: 40 nb: 1308 cost: 1.95613 error rate: 0.789
i: 5 j: 60 nb: 1308 cost: 1.9285748 error rate: 0.789
i: 5 j: 80 nb: 1308 cost: 1.9285586 error rate: 0.789
i: 5 j: 100 nb: 1308 cost: 1.9267668 error rate: 0.789
i: 5 j: 120 nb: 1308 cost: 1.926367 error rate: 0.789
i: 5 j: 140 nb: 1308 cost: 1.9277219 error rate: 0.789
i: 5 j: 160 nb: 1308 cost: 1.9279777 error rate: 0.789
i: 5 j: 180 nb: 1308 cost: 1.9296522 error rate: 0.789
i: 5 j: 200 nb: 1308 cost: 1.9279768 error rate: 0.789
i: 5 j: 220 nb: 1308 cost: 1.9261986 error rate: 0.789
i: 5 j: 240 nb: 1308 cost: 1.9265149 error rate: 0.789
i: 5 j: 260 nb: 1308 cost: 1.9271082 error rate: 0.789
i: 5 j: 280 nb: 1308 cost: 1.9278667 error rate: 0.788
i: 5 j: 300 nb: 1308 cost: 1.9276773 error rate: 0.789
i: 5 j: 320 nb: 1308 cost: 1.9275625 error rate: 0.789
i: 5 j: 340 nb: 1308 cost: 1.9291185 error rate: 0.789
i: 5 j: 360 nb: 1308 cost: 1.929701 error rate: 0.789
i: 5 j: 380 nb: 1308 cost: 1.9304721 error rate: 0.789
i: 5 j: 400 nb: 1308 cost: 1.9268991 error rate: 0.789
i: 5 j: 420 nb: 1308 cost: 1.9262868 error rate: 0.789
i: 5 j: 440 nb: 1308 cost: 1.9272518 error rate: 0.789
i: 5 j: 460 nb: 1308 cost: 1.9268091 error rate: 0.789
i: 5 j: 480 nb: 1308 cost: 1.9267397 error rate: 0.789
i: 5 j: 500 nb: 1308 cost: 1.9252704 error rate: 0.789
i: 5 j: 520 nb: 1308 cost: 1.925359 error rate: 0.789
i: 5 j: 540 nb: 1308 cost: 1.9251602 error rate: 0.789
i: 5 j: 560 nb: 1308 cost: 1.9246112 error rate: 0.789
i: 5 j: 580 nb: 1308 cost: 1.9254616 error rate: 0.789
i: 5 j: 600 nb: 1308 cost: 1.9263132 error rate: 0.789
i: 5 j: 620 nb: 1308 cost: 1.9272838 error rate: 0.789
i: 5 j: 640 nb: 1308 cost: 1.9271624 error rate: 0.789
i: 5 j: 660 nb: 1308 cost: 1.927195 error rate: 0.789
i: 5 j: 680 nb: 1308 cost: 1.9267722 error rate: 0.789
i: 5 j: 700 nb: 1308 cost: 1.9251112 error rate: 0.789
i: 5 j: 720 nb: 1308 cost: 1.9254714 error rate: 0.789
i: 5 j: 740 nb: 1308 cost: 1.9254963 error rate: 0.788
i: 5 j: 760 nb: 1308 cost: 1.9261929 error rate: 0.789
i: 5 j: 780 nb: 1308 cost: 1.9258893 error rate: 0.789
i: 5 j: 800 nb: 1308 cost: 1.9255108 error rate: 0.789
i: 5 j: 820 nb: 1308 cost: 1.9272012 error rate: 0.789
i: 5 j: 840 nb: 1308 cost: 1.924837 error rate: 0.787
i: 5 j: 860 nb: 1308 cost: 1.9307642 error rate: 0.789
i: 5 j: 880 nb: 1308 cost: 1.92994 error rate: 0.789
i: 5 j: 900 nb: 1308 cost: 1.9282305 error rate: 0.789
i: 5 j: 920 nb: 1308 cost: 1.9285804 error rate: 0.789
i: 5 j: 940 nb: 1308 cost: 1.9263719 error rate: 0.788
i: 5 j: 960 nb: 1308 cost: 1.9276848 error rate: 0.788
i: 5 j: 980 nb: 1308 cost: 1.9320372 error rate: 0.789
i: 5 j: 1000 nb: 1308 cost: 1.9278187 error rate: 0.789
i: 5 j: 1020 nb: 1308 cost: 1.9269543 error rate: 0.789
i: 5 j: 1040 nb: 1308 cost: 1.9267397 error rate: 0.789
i: 5 j: 1060 nb: 1308 cost: 1.9288582 error rate: 0.789
i: 5 j: 1080 nb: 1308 cost: 1.928426 error rate: 0.789
i: 5 j: 1100 nb: 1308 cost: 1.9262706 error rate: 0.789
i: 5 j: 1120 nb: 1308 cost: 1.9267523 error rate: 0.789
i: 5 j: 1140 nb: 1308 cost: 1.9286425 error rate: 0.789
i: 5 j: 1160 nb: 1308 cost: 1.9288459 error rate: 0.789
i: 5 j: 1180 nb: 1308 cost: 1.9287055 error rate: 0.789
i: 5 j: 1200 nb: 1308 cost: 1.9259472 error rate: 0.789
i: 5 j: 1220 nb: 1308 cost: 1.9250637 error rate: 0.789
i: 5 j: 1240 nb: 1308 cost: 1.9252652 error rate: 0.789
i: 5 j: 1260 nb: 1308 cost: 1.9275125 error rate: 0.789
i: 5 j: 1280 nb: 1308 cost: 1.9300429 error rate: 0.789
i: 5 j: 1300 nb: 1308 cost: 1.9301184 error rate: 0.789
i: 6 j: 0 nb: 1308 cost: 1.9287871 error rate: 0.789
i: 6 j: 20 nb: 1308 cost: 1.9271711 error rate: 0.789
i: 6 j: 40 nb: 1308 cost: 1.9286389 error rate: 0.789
i: 6 j: 60 nb: 1308 cost: 1.9294896 error rate: 0.789
i: 6 j: 80 nb: 1308 cost: 1.92929 error rate: 0.789
i: 6 j: 100 nb: 1308 cost: 1.9276503 error rate: 0.789
i: 6 j: 120 nb: 1308 cost: 1.9281949 error rate: 0.789
i: 6 j: 140 nb: 1308 cost: 1.9270607 error rate: 0.789
i: 6 j: 160 nb: 1308 cost: 1.9260656 error rate: 0.789
i: 6 j: 180 nb: 1308 cost: 1.9254688 error rate: 0.789
i: 6 j: 200 nb: 1308 cost: 1.9250313 error rate: 0.789
i: 6 j: 220 nb: 1308 cost: 1.9261341 error rate: 0.789
i: 6 j: 240 nb: 1308 cost: 1.9276166 error rate: 0.789
i: 6 j: 260 nb: 1308 cost: 1.9271061 error rate: 0.789
i: 6 j: 280 nb: 1308 cost: 1.9259135 error rate: 0.789
i: 6 j: 300 nb: 1308 cost: 1.9260154 error rate: 0.789
i: 6 j: 320 nb: 1308 cost: 1.9272188 error rate: 0.789
i: 6 j: 340 nb: 1308 cost: 1.9272856 error rate: 0.789
i: 6 j: 360 nb: 1308 cost: 1.9294988 error rate: 0.789
i: 6 j: 380 nb: 1308 cost: 1.9286851 error rate: 0.789
i: 6 j: 400 nb: 1308 cost: 1.9264143 error rate: 0.789
i: 6 j: 420 nb: 1308 cost: 1.9260442 error rate: 0.789
i: 6 j: 440 nb: 1308 cost: 1.9262575 error rate: 0.789
i: 6 j: 460 nb: 1308 cost: 1.9262718 error rate: 0.789
i: 6 j: 480 nb: 1308 cost: 1.9295975 error rate: 0.789
i: 6 j: 500 nb: 1308 cost: 1.9308441 error rate: 0.789
i: 6 j: 520 nb: 1308 cost: 1.930458 error rate: 0.789
i: 6 j: 540 nb: 1308 cost: 1.9297601 error rate: 0.789
i: 6 j: 560 nb: 1308 cost: 1.9540528 error rate: 0.789
i: 6 j: 580 nb: 1308 cost: 1.9269836 error rate: 0.789
i: 6 j: 600 nb: 1308 cost: 1.927401 error rate: 0.789
i: 6 j: 620 nb: 1308 cost: 1.9270719 error rate: 0.789
i: 6 j: 640 nb: 1308 cost: 1.9280387 error rate: 0.789
i: 6 j: 660 nb: 1308 cost: 1.9278177 error rate: 0.789
i: 6 j: 680 nb: 1308 cost: 1.9266334 error rate: 0.789
i: 6 j: 700 nb: 1308 cost: 1.9262813 error rate: 0.789
i: 6 j: 720 nb: 1308 cost: 1.9272256 error rate: 0.786
i: 6 j: 740 nb: 1308 cost: 1.9274276 error rate: 0.789
i: 6 j: 760 nb: 1308 cost: 1.9289789 error rate: 0.789
i: 6 j: 780 nb: 1308 cost: 1.9299433 error rate: 0.789
i: 6 j: 800 nb: 1308 cost: 1.9297924 error rate: 0.789
i: 6 j: 820 nb: 1308 cost: 1.9291669 error rate: 0.789
i: 6 j: 840 nb: 1308 cost: 1.9281498 error rate: 0.789
i: 6 j: 860 nb: 1308 cost: 1.9286873 error rate: 0.789
i: 6 j: 880 nb: 1308 cost: 1.9274993 error rate: 0.789
i: 6 j: 900 nb: 1308 cost: 1.9266584 error rate: 0.789
i: 6 j: 920 nb: 1308 cost: 1.9276992 error rate: 0.789
i: 6 j: 940 nb: 1308 cost: 1.927995 error rate: 0.789
i: 6 j: 960 nb: 1308 cost: 1.9272643 error rate: 0.789
i: 6 j: 980 nb: 1308 cost: 1.9265876 error rate: 0.789
i: 6 j: 1000 nb: 1308 cost: 1.9263295 error rate: 0.789
i: 6 j: 1020 nb: 1308 cost: 1.925261 error rate: 0.789
i: 6 j: 1040 nb: 1308 cost: 1.9247782 error rate: 0.789
i: 6 j: 1060 nb: 1308 cost: 1.9239725 error rate: 0.788
i: 6 j: 1080 nb: 1308 cost: 1.9249032 error rate: 0.789
i: 6 j: 1100 nb: 1308 cost: 1.9258085 error rate: 0.789
i: 6 j: 1120 nb: 1308 cost: 1.9264895 error rate: 0.789
i: 6 j: 1140 nb: 1308 cost: 1.926167 error rate: 0.789
i: 6 j: 1160 nb: 1308 cost: 1.9262017 error rate: 0.789
i: 6 j: 1180 nb: 1308 cost: 2.0281615 error rate: 0.788
i: 6 j: 1200 nb: 1308 cost: 1.9262278 error rate: 0.789
i: 6 j: 1220 nb: 1308 cost: 1.9272046 error rate: 0.789
i: 6 j: 1240 nb: 1308 cost: 1.9286007 error rate: 0.789
i: 6 j: 1260 nb: 1308 cost: 1.9297556 error rate: 0.789
i: 6 j: 1280 nb: 1308 cost: 1.9295666 error rate: 0.789
i: 6 j: 1300 nb: 1308 cost: 1.9275141 error rate: 0.789
i: 7 j: 0 nb: 1308 cost: 1.927106 error rate: 0.789
i: 7 j: 20 nb: 1308 cost: 1.9267431 error rate: 0.789
i: 7 j: 40 nb: 1308 cost: 1.926187 error rate: 0.789
i: 7 j: 60 nb: 1308 cost: 1.9270061 error rate: 0.789
i: 7 j: 80 nb: 1308 cost: 1.9280136 error rate: 0.789
i: 7 j: 100 nb: 1308 cost: 1.9243602 error rate: 0.787
i: 7 j: 120 nb: 1308 cost: 1.9473099 error rate: 0.787
i: 7 j: 140 nb: 1308 cost: 1.9297203 error rate: 0.789
i: 7 j: 160 nb: 1308 cost: 1.9283054 error rate: 0.789
i: 7 j: 180 nb: 1308 cost: 1.928088 error rate: 0.789
i: 7 j: 200 nb: 1308 cost: 1.9287455 error rate: 0.789
i: 7 j: 220 nb: 1308 cost: 1.931258 error rate: 0.789
i: 7 j: 240 nb: 1308 cost: 1.9307598 error rate: 0.789
i: 7 j: 260 nb: 1308 cost: 1.9280415 error rate: 0.789
i: 7 j: 280 nb: 1308 cost: 1.9284546 error rate: 0.791
i: 7 j: 300 nb: 1308 cost: 1.9301025 error rate: 0.79
i: 7 j: 320 nb: 1308 cost: 1.9296002 error rate: 0.789
i: 7 j: 340 nb: 1308 cost: 1.9274582 error rate: 0.789
i: 7 j: 360 nb: 1308 cost: 1.9256883 error rate: 0.789
i: 7 j: 380 nb: 1308 cost: 1.9264567 error rate: 0.789
i: 7 j: 400 nb: 1308 cost: 1.9268074 error rate: 0.789
i: 7 j: 420 nb: 1308 cost: 1.9403182 error rate: 0.787
i: 7 j: 440 nb: 1308 cost: 1.92906 error rate: 0.788
i: 7 j: 460 nb: 1308 cost: 1.9272082 error rate: 0.789
i: 7 j: 480 nb: 1308 cost: 1.9267598 error rate: 0.789
i: 7 j: 500 nb: 1308 cost: 1.9263792 error rate: 0.789
i: 7 j: 520 nb: 1308 cost: 1.9275001 error rate: 0.789
i: 7 j: 540 nb: 1308 cost: 1.9291071 error rate: 0.789
i: 7 j: 560 nb: 1308 cost: 1.9305289 error rate: 0.789
i: 7 j: 580 nb: 1308 cost: 1.9279408 error rate: 0.789
i: 7 j: 600 nb: 1308 cost: 1.9272754 error rate: 0.789
i: 7 j: 620 nb: 1308 cost: 1.9272293 error rate: 0.789
i: 7 j: 640 nb: 1308 cost: 1.9267324 error rate: 0.789
i: 7 j: 660 nb: 1308 cost: 1.9270816 error rate: 0.789
i: 7 j: 680 nb: 1308 cost: 1.9269025 error rate: 0.789
i: 7 j: 700 nb: 1308 cost: 1.9266468 error rate: 0.789
i: 7 j: 720 nb: 1308 cost: 1.9300048 error rate: 0.788
i: 7 j: 740 nb: 1308 cost: 1.925838 error rate: 0.788
i: 7 j: 760 nb: 1308 cost: 1.9268119 error rate: 0.788
i: 7 j: 780 nb: 1308 cost: 1.9275143 error rate: 0.789
i: 7 j: 800 nb: 1308 cost: 1.9272768 error rate: 0.789
i: 7 j: 820 nb: 1308 cost: 1.9244446 error rate: 0.787
i: 7 j: 840 nb: 1308 cost: 1.9229839 error rate: 0.787
i: 7 j: 860 nb: 1308 cost: 1.9232488 error rate: 0.787
i: 7 j: 880 nb: 1308 cost: 1.9244642 error rate: 0.787
i: 7 j: 900 nb: 1308 cost: 1.926354 error rate: 0.788
i: 7 j: 920 nb: 1308 cost: 2.0220728 error rate: 0.781
i: 7 j: 940 nb: 1308 cost: 1.9267288 error rate: 0.789
i: 7 j: 960 nb: 1308 cost: 1.9274623 error rate: 0.789
i: 7 j: 980 nb: 1308 cost: 1.9277651 error rate: 0.789
i: 7 j: 1000 nb: 1308 cost: 1.9270552 error rate: 0.789
i: 7 j: 1020 nb: 1308 cost: 1.9268036 error rate: 0.789
i: 7 j: 1040 nb: 1308 cost: 1.9267982 error rate: 0.789
i: 7 j: 1060 nb: 1308 cost: 1.9266899 error rate: 0.789
i: 7 j: 1080 nb: 1308 cost: 1.9275892 error rate: 0.789
i: 7 j: 1100 nb: 1308 cost: 1.9271672 error rate: 0.789
i: 7 j: 1120 nb: 1308 cost: 1.9269547 error rate: 0.789
i: 7 j: 1140 nb: 1308 cost: 1.9268359 error rate: 0.789
i: 7 j: 1160 nb: 1308 cost: 1.9263027 error rate: 0.789
i: 7 j: 1180 nb: 1308 cost: 1.9260188 error rate: 0.789
i: 7 j: 1200 nb: 1308 cost: 1.9254451 error rate: 0.789
i: 7 j: 1220 nb: 1308 cost: 1.9253874 error rate: 0.789
i: 7 j: 1240 nb: 1308 cost: 1.9269611 error rate: 0.789
i: 7 j: 1260 nb: 1308 cost: 1.9274193 error rate: 0.789
i: 7 j: 1280 nb: 1308 cost: 1.9268352 error rate: 0.789
i: 7 j: 1300 nb: 1308 cost: 1.9262422 error rate: 0.789
i: 8 j: 0 nb: 1308 cost: 1.926283 error rate: 0.789
i: 8 j: 20 nb: 1308 cost: 1.9258375 error rate: 0.789
i: 8 j: 40 nb: 1308 cost: 1.9274607 error rate: 0.789
i: 8 j: 60 nb: 1308 cost: 1.9286085 error rate: 0.789
i: 8 j: 80 nb: 1308 cost: 1.9277244 error rate: 0.789
i: 8 j: 100 nb: 1308 cost: 1.9281131 error rate: 0.789
i: 8 j: 120 nb: 1308 cost: 1.931482 error rate: 0.789
i: 8 j: 140 nb: 1308 cost: 1.9293425 error rate: 0.789
i: 8 j: 160 nb: 1308 cost: 1.927576 error rate: 0.789
i: 8 j: 180 nb: 1308 cost: 1.9265491 error rate: 0.789
i: 8 j: 200 nb: 1308 cost: 1.9264163 error rate: 0.789
i: 8 j: 220 nb: 1308 cost: 1.9260199 error rate: 0.789
i: 8 j: 240 nb: 1308 cost: 1.926394 error rate: 0.789
i: 8 j: 260 nb: 1308 cost: 1.9274399 error rate: 0.789
i: 8 j: 280 nb: 1308 cost: 1.9265127 error rate: 0.789
i: 8 j: 300 nb: 1308 cost: 1.9245628 error rate: 0.789
i: 8 j: 320 nb: 1308 cost: 1.9244691 error rate: 0.789
i: 8 j: 340 nb: 1308 cost: 1.926514 error rate: 0.789
i: 8 j: 360 nb: 1308 cost: 1.9271004 error rate: 0.789
i: 8 j: 380 nb: 1308 cost: 1.9274329 error rate: 0.789
i: 8 j: 400 nb: 1308 cost: 1.9269518 error rate: 0.789
i: 8 j: 420 nb: 1308 cost: 1.9260254 error rate: 0.789
i: 8 j: 440 nb: 1308 cost: 1.9264946 error rate: 0.789
i: 8 j: 460 nb: 1308 cost: 1.9256228 error rate: 0.789
i: 8 j: 480 nb: 1308 cost: 1.9255605 error rate: 0.789
i: 8 j: 500 nb: 1308 cost: 1.9255868 error rate: 0.789
i: 8 j: 520 nb: 1308 cost: 1.925647 error rate: 0.789
i: 8 j: 540 nb: 1308 cost: 1.9262055 error rate: 0.789
i: 8 j: 560 nb: 1308 cost: 1.9268785 error rate: 0.789
i: 8 j: 580 nb: 1308 cost: 1.9243112 error rate: 0.788
i: 8 j: 600 nb: 1308 cost: 1.923573 error rate: 0.788
i: 8 j: 620 nb: 1308 cost: 1.9240916 error rate: 0.788
i: 8 j: 640 nb: 1308 cost: 1.9275495 error rate: 0.786
i: 8 j: 660 nb: 1308 cost: 1.9276748 error rate: 0.789
i: 8 j: 680 nb: 1308 cost: 1.927098 error rate: 0.789
i: 8 j: 700 nb: 1308 cost: 1.9277457 error rate: 0.789
i: 8 j: 720 nb: 1308 cost: 1.9268639 error rate: 0.789
i: 8 j: 740 nb: 1308 cost: 1.9262494 error rate: 0.789
i: 8 j: 760 nb: 1308 cost: 1.9264706 error rate: 0.789
i: 8 j: 780 nb: 1308 cost: 1.924718 error rate: 0.789
</pre>
</div>
</div>

</div>
</div>

</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
