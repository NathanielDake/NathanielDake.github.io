<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Natural-Language-Corpus-Data">2. Natural Language Corpus Data<a class="anchor-link" href="#2.-Natural-Language-Corpus-Data">&#182;</a></h1><p>This post was inspired and based off of chapter fourteen of the book <em>Beautiful Data</em>, the chapter being written by Peter Norvig. The chapter can be found <a href="http://norvig.com/ngrams/ch14.pdf">here</a>. The exercise will examining data consisting of the plainest of speech: 1 trillion words of English, taken from publically available webpages.</p>
<p>This data set was published by Thorsten Brants and Alex Franz of Google in 2006, and is made publically available through the Linguistic Data Consortium <a href="https://catalog.ldc.upenn.edu/LDC2006T13">here</a>.</p>
<p>This data set summarizes the original texts by <em>counting</em> the number of appearances of each words, and of each two-, three-, four-, and five-word sequence. For example, the word "the" appears 23 billion times (2.2% of the trillion words), making it the most common word.</p>
<hr>
<h1 id="Technical-Definitions">Technical Definitions<a class="anchor-link" href="#Technical-Definitions">&#182;</a></h1><p>Before we can dig into an analysis, we must learn a few pieces of technical terminology that will serve us well in the long run.</p>
<blockquote><p><strong>Corpus</strong>: A collection of text is called a <em>corpus</em>.</p>
<p><strong>Tokens</strong>: We treat the corpus as a sequence of <em>tokens</em>-meaning words and punctuation.</p>
<p><strong>Types</strong>: Each distinct token is called a <em>type</em>, so the text "Run, Lola Run" has 4 tokens (the comman counts as one) but only three types.</p>
<p><strong>Vocabulary</strong>: The set of all types is called the <em>vocabulary</em>.</p>
<p><strong>Unigram</strong>: A 1-token sequence is a <em>unigram</em>.</p>
<p><strong>Bigram</strong>: A 2-token sequence is a <em>bigram</em>.</p>
<p><strong>n-gram</strong>: An n-token sequence is an <em>n-gram</em>.</p>
<p><strong>Probability</strong>: We will refer to <em>P</em> as probability, as in P(<em>the</em>) = 0.022, which means that the probability of the toekn "the" is 0.022, or 2.2%. If <em>W</em> is a sequence of tokens, then <em>W3</em> is the third token, and <em>W1:3</em> is the sequence of the first through third tokens. $P(Wi = the|Wi-1=of)$ is the <em>conditional probability</em> of "the", given that "of" is the previous token.</p>
</blockquote>
<p>We are now ready to look at some tasks that can be accomplished using the data!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="1.-Word-Segmentation">1. Word Segmentation<a class="anchor-link" href="#1.-Word-Segmentation">&#182;</a></h1><p>In general, English readers do not need to perform the task of <em>word segmentation</em>, the process of deciding where word boundaries are. This is directly due to the use of spaces in the english language (as contrasted with, for example, Mandarin).</p>
<p>However, in some texts, such as URL's, spaces are not present. How could a search engine or computer program work out such a mistake?</p>
<p>As an example, lets look at the English text</p>
<blockquote><p>"<strong>choosespain.com</strong>"</p>
</blockquote>
<p>This website is hoping to convince you to choose Spain as a travel destination, but if you segment the name wrong, you get the less appealing name:</p>
<blockquote><p>"chooses pain"</p>
</blockquote>
<p>As a human reader, you are able to make the right choice by drawing upon years of experience; you may initially guess that it would be nearly an insurmountable task to encode that experience into a computer algorithm. However, there is a shortcut we can take that works surprisingly well! We can look up each phrase in the bigram table! We see that "choose Spain" has a count of 3,210, whereas "chooses pain" does not appear at all (which means that it occurs fewer than 40 times in the trillion word corpus). Thus "choose Spain" is at least 80 times more likely, and can safely be consdered the right segmentation.</p>
<p>Now supposed we are trying to interpret the phrase:</p>
<blockquote><p><strong>insufficientnumbers</strong></p>
</blockquote>
<p>If we add together capitalized and lowercase version of the words, the counts are:</p>

<pre><code>insufficient numbers -&gt;  20715
in sufficient numbers -&gt; 32378</code></pre>
<p>“In sufficient numbers” is 50% more frequent than “insufficient numbers” but that’s
hardly compelling evidence. We are left in a frustrating position: we can guess, but we
can’t be confident. In uncertain problems like this, we don’t have any way of calculating a definitive correct answer, we don’t have a complete model of what makes one answer
right, and in fact human experts don’t have a complete model, either, and can disagree on
the answer. Still, there is an established methodology for solving uncertain problems.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="1.1-Methodology">1.1 Methodology<a class="anchor-link" href="#1.1-Methodology">&#182;</a></h2><p><br></p>
<h3 id="1.1.1-Define-a-probabilistic-model">1.1.1 Define a probabilistic model<a class="anchor-link" href="#1.1.1-Define-a-probabilistic-model">&#182;</a></h3><p>While we can't define all the factors (semantic, syntactic, lexical, social) that make "choose Spain" a better candidate for a domain name, but we can define a simplified model that gives approximate probabilities. For short candidates like "choose Spain" we could just look up the <em>n</em>-gram in the corpus data and use that as the probability. For longer candidates we will need some way of composing an answer from smaller parts. For words we haven't seen before, we'll have to estimate the probability of an unknown word. The point is that we define a <em>language model</em>- a probability distribution over all the strings in the language-and learn the parameters of the model from our corpus data, then use the model to define the probability of each candidate.</p>
<h3 id="1.1.2-Enumerate-the-candidates">1.1.2 Enumerate the candidates<a class="anchor-link" href="#1.1.2-Enumerate-the-candidates">&#182;</a></h3><p>We may not be sure whether “insufficient numbers” or “in
sufficient numbers” is more likely to be the intended phrase, but we can agree that
they are both candidate segmentations, as is “in suffi cient numb ers,” but that “hello world” is not a valid candidate. In this step we withhold judgment and just enumerate
possibilities—all the possibilities if we can, or else a carefully selected sample.</p>
<h3 id="1.1.3-Choose-the-most-probable-candidate">1.1.3 Choose the most probable candidate<a class="anchor-link" href="#1.1.3-Choose-the-most-probable-candidate">&#182;</a></h3><p>Apply the language model to each candidate to get its probability, and choose the one with the highest probability.</p>
<p>From a mathematical perspective this can be written as:</p>
$$best = argmax_{c \in candidates} P(c)$$<p>And from a code perspective, it would be:</p>
<center>
```
best = max(candidates, key=P)
```
</center><p>Let's now apply this methodology to segmentation. We want to define a function, <code>segment</code>, which takes as input a string with no spaces and returns a list of words that is the best segmentation:</p>

<pre><code>&gt;&gt;&gt; segment('choosespain')
['choose','spain']</code></pre>
<p>Let's start with step 1, the probabilistic language model. The probability of a sequence of words is the product of the probabilities of each word, given the words context: all the preceeding words. It can be written mathematically as:</p>
$$P(W_{1:n}) = \prod_{k=1:n}P(W_k \;|\;W_{1:k-1} )$$<p>As an example, if we were analyzing the sentence:</p>
<blockquote><p>"The dog ran"</p>
</blockquote>
<p>It would be broken down into:</p>
$$P("the \; dog \; ran") = P(the) * P(dog \; | \; the) * P(ran \; | \; the dog)$$<p>Now, we don’t have the data to compute this exactly, so we can approximate the equation by
using a smaller context. Since we have data for sequences up to 5-grams, it would be
tempting to use the 5-grams, so that the probability of an n-word sequence would be the
product of each word given the four previous words (not all previous words).</p>
<p>There are three difficulties with the 5-gram model. First, the 5-gram data is about 30
gigabytes, so it can’t all fit in RAM. Second, many 5-gram counts will be 0, and we’d need
some strategy for backing off, using shorter sequences to estimate the 5-gram probabilities. Third, the search space of candidates will be large because dependencies extend up to four words away. All three of these difficulties can be managed, with some effort. But instead, let’s first consider a much simpler language model that solves all three difficulties at once: a <strong>unigram model</strong>, in which the probability of a sequence is just the product of the probability of each word by itself. In this model, the probability of each word is independent of the other words (this is a naive bayes approach of a bayesian network):</p>
$$P(W_{1:n}) \prod_{k=1:n}P(W_k)$$<p>Now, if we were to segment <code>wheninrome</code>, we would consider candidates such as <code>when</code> <code>in</code> <code>rome</code>, and compute:</p>
$$P(when) * P(in) * P(rome)$$<p>If the product is higher than any other candidates product, then that is the best answer!</p>
<p>An <em>n</em>-character string has $2^{n-1}$ different segmentations (there are <em>n</em>-1 positions between characters, each of which can either be or not be a word boundary. Thus the string:</p>
<p><code>wheninthecourseofhumaneventsitbecomesnecessary</code></p>
<p>has 35 trillion segmentations. But you most definitely were able to find the right segmentation in just a few seconds; clearly, you couldn't have enumerated all the candidates. You most likely scanned "w", "wh", and "whe", and rejected them as improbable words, but accepted "when" as probable. Then you moved on to the remainder and found its best segmentation. Once we make the simplifying assumption that each word is independent of the others, it means that we don’t have to consider all combinations of words.</p>
<p>That gives us a sketch of the <code>segment</code> function: consider every possible way to split the text into a first word and a remaining text (we can arbitrarily limit the longest possible word to, say, L=20 letters). For each possible split, find the best way to segment the remainder. Out of all the possible candidates, the one with the highest product of P(first)× P(remaining) is the best.</p>
<p>Here we show a table of choices for the first word, probability of the word, probability of the best segmentation of the remaining words, and probability of the whole (which is the product of the probabilities of the first and the remainder). We see that the segmentation starting with “when” is 50,000 times better than the second-best candidate.</p>
<p><img src="https://drive.google.com/uc?id=1xfDXID2XExkVrmLgzs1-Iaq4eDbIO7Ld"></p>
<p>In only a few lines of Python, we are able to implement <code>segment</code>. First, our imports that will be used throughout the rest of this notebook:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">re</span><span class="o">,</span> <span class="nn">string</span><span class="o">,</span> <span class="nn">random</span><span class="o">,</span> <span class="nn">glob</span><span class="o">,</span> <span class="nn">operator</span><span class="o">,</span> <span class="nn">heapq</span><span class="o">,</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">log10</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we will write two support functions that will be used in our <code>segment</code> functions. Note that <code>memo</code> uses <a href="https://en.wikipedia.org/wiki/Memoization">memoization</a>, which is defined as:</p>
<blockquote><p>An optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">memo</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
  <span class="s2">&quot;Memoize function f.&quot;</span>
  <span class="n">table</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="k">def</span> <span class="nf">fmemo</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">args</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">table</span><span class="p">:</span>
      <span class="n">table</span><span class="p">[</span><span class="n">args</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">table</span><span class="p">[</span><span class="n">args</span><span class="p">]</span>
  <span class="n">fmemo</span><span class="o">.</span><span class="n">memo</span> <span class="o">=</span> <span class="n">table</span>
  <span class="k">return</span> <span class="n">fmemo</span>  

<span class="k">def</span> <span class="nf">product</span><span class="p">(</span><span class="n">nums</span><span class="p">):</span>
    <span class="s2">&quot;Return the product of a sequence of numbers.&quot;</span>
    <span class="k">return</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">nums</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And now we can build out <code>segment</code> and it's related functions:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memo</span> 
<span class="k">def</span> <span class="nf">segment</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="s2">&quot;Return a list of words that is the best segmentation of text.&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span> <span class="k">return</span> <span class="p">[]</span>
  <span class="n">candidates</span> <span class="o">=</span> <span class="p">([</span><span class="n">first</span><span class="p">]</span> <span class="o">+</span> <span class="n">segment</span><span class="p">(</span><span class="n">rem</span><span class="p">)</span> <span class="k">for</span> <span class="n">first</span><span class="p">,</span> <span class="n">rem</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
  <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">Pwords</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">splits</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">L</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
  <span class="s2">&quot;Return a list of all possible (first, rem) pairs, len(first)&lt;=L.&quot;</span>
  <span class="k">return</span> <span class="p">[(</span><span class="n">text</span><span class="p">[:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">text</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:])</span>
         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">L</span><span class="p">))]</span>

<span class="k">def</span> <span class="nf">Pwords</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
  <span class="s2">&quot;The Naive Bayes probability of a sequence of words.&quot;</span>
  <span class="k">return</span> <span class="n">product</span><span class="p">(</span><span class="n">Pw</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>This is the entire program!</em>. We will use <code>product</code> as a utility function that multiplies together a list of numbers, <code>memo</code> is a decorator that caches the results of previous calls to a function so that they don't have to be recomputed, and <code>Pw</code> estimates the probability of a word by consulting the unigram count data.</p>
<p>Without <code>memo</code>, a call to <code>segment</code> for an <em>n</em>-character text makes $2^n$ recursive calls to <code>segment</code>; with <code>memo</code> it makes only <em>n</em> calles-<code>memo</code> makes this a fairly efficient dynamic programming algorithm. Each of the <em>n</em> calls considers O(L) splits, and evaluates each split by multiplying O(<em>n</em>) probabilities, so the whole algorithm is O($n^2L$).</p>
<p>As for <code>Pw</code>, we read in the unigram counts from a datafile. If a word appears in the corpus, its estimated probability is Count(word)/N, where N is the corpus size. Actually, instead of using the full 13-million-type unigram datafile, I created vocab_common, which (a) is case-insensitive, so that the counts for “the”, “The”, and “THE” are added together under a single entry for “the”; (b) only has entries for words made out of letters, not numbers or punctuation (so “+170.002” is out, as is “can’t”); and (c) lists only the most common 1/3 of a million words (which together cover 98% of the tokens).</p>
<p>The only tricky part of <code>Pw</code> is when a word has not been seen in the corpus. This happens
sometimes even with a trillion-word corpus, so it would be a mistake to return 0 for the
probability. But what should it be? The number of tokens in the corpus, <em>N</em>, is about a trillion,and the least common word in vocab_common has a count of 12,711. So a previously
unseen word should have a probability of somewhere between 0 and 12,710/N. Not all
unseen words are equally unlikely: a random sequence of 20 letters is less likely to be a
word than a random sequence of 6 letters. We will define a class for probability distributions, <code>Pdist</code>, which loads a datafile of (key, count) pairs. By default, the probability of an unknown word is 1/N, but each instance of a <code>Pdist</code> can supply a custom function to override the default. We want to avoid having too high a probability for very long words, so
we (rather arbitrarily) start at a probability of 10/<em>N</em>, and decrease by a factor of 10 forevery letter in the candidate word. We then define <code>Pw</code> as a <code>Pdist</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Pdist</span><span class="p">(</span><span class="nb">dict</span><span class="p">):</span>
  <span class="s2">&quot;A probability distribution estimated from counts in datafile.&quot;</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="p">[],</span> <span class="n">N</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">missingfn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">N</span> <span class="ow">or</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">itervalues</span><span class="p">()))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">missingfn</span> <span class="o">=</span> <span class="n">missingfn</span> <span class="ow">or</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span>
    <span class="k">else</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">missingfn</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">datafile</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">):</span>
  <span class="s2">&quot;Read key, value pairs from file.&quot;</span>
  <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/&#39;</span> <span class="o">+</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">sep</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">avoid_long_words</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
  <span class="s2">&quot;Estimate the probability of an unknown word.&quot;</span>
  <span class="k">return</span> <span class="mf">10.</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">**</span> <span class="nb">len</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">1024908267229</span> <span class="c1"># Number of tokens</span>

<span class="n">Pw</span> <span class="o">=</span> <span class="n">Pdist</span><span class="p">(</span><span class="n">datafile</span><span class="p">(</span><span class="s1">&#39;count_1w.txt&#39;</span><span class="p">),</span> <span class="n">N</span><span class="p">,</span> <span class="n">avoid_long_words</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;choosespain&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;choose&#39;, &#39;spain&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;thisisatest&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;test&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;wheninthecourseofhumaneventsitbecomesnecessary&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;when&#39;, &#39;in&#39;, &#39;the&#39;, &#39;course&#39;, &#39;of&#39;, &#39;human&#39;, &#39;events&#39;, &#39;it&#39;, &#39;becomes&#39;, &#39;necessary&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;speedofart&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;speed&#39;, &#39;of&#39;, &#39;art&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;expertsexchange&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;experts&#39;, &#39;exchange&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;itwasthebestoftimesitwastheworstoftimesitwastheageofwisdomitwastheageoffoolishness&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;it&#39;, &#39;was&#39;, &#39;the&#39;, &#39;best&#39;, &#39;of&#39;, &#39;times&#39;, &#39;it&#39;, &#39;was&#39;, &#39;the&#39;, &#39;worst&#39;, &#39;of&#39;, &#39;times&#39;, &#39;it&#39;, &#39;was&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;wisdom&#39;, &#39;it&#39;, &#39;was&#39;, &#39;the&#39;, &#39;age&#39;, &#39;of&#39;, &#39;foolishness&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;inaholeinthegroundtherelivedahobbitnotanastydirtywetholefilledwiththeendsofwormsandanoozysmellnoryetadrybaresandyholewithnothinginittositdownonortoeatitwasahobbitholeandthatmeanscomfort&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;in&#39;, &#39;a&#39;, &#39;hole&#39;, &#39;in&#39;, &#39;the&#39;, &#39;ground&#39;, &#39;there&#39;, &#39;lived&#39;, &#39;a&#39;, &#39;hobbit&#39;, &#39;not&#39;, &#39;a&#39;, &#39;nasty&#39;, &#39;dirty&#39;, &#39;wet&#39;, &#39;hole&#39;, &#39;filled&#39;, &#39;with&#39;, &#39;the&#39;, &#39;ends&#39;, &#39;of&#39;, &#39;worms&#39;, &#39;and&#39;, &#39;an&#39;, &#39;oozy&#39;, &#39;smell&#39;, &#39;nor&#39;, &#39;yet&#39;, &#39;a&#39;, &#39;dry&#39;, &#39;bare&#39;, &#39;sandy&#39;, &#39;hole&#39;, &#39;with&#39;, &#39;nothing&#39;, &#39;in&#39;, &#39;it&#39;, &#39;to&#39;, &#39;sitdown&#39;, &#39;on&#39;, &#39;or&#39;, &#39;to&#39;, &#39;eat&#39;, &#39;it&#39;, &#39;was&#39;, &#39;a&#39;, &#39;hobbit&#39;, &#39;hole&#39;, &#39;and&#39;, &#39;that&#39;, &#39;means&#39;, &#39;comfort&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="s1">&#39;faroutintheunchartedbackwatersoftheunfashionableendofthewesternspiralarmofthegalaxyliesasmallunregardedyellowsun&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;far&#39;, &#39;out&#39;, &#39;in&#39;, &#39;the&#39;, &#39;uncharted&#39;, &#39;backwaters&#39;, &#39;of&#39;, &#39;the&#39;, &#39;unfashionable&#39;, &#39;end&#39;, &#39;of&#39;, &#39;the&#39;, &#39;western&#39;, &#39;spiral&#39;, &#39;arm&#39;, &#39;of&#39;, &#39;the&#39;, &#39;galaxy&#39;, &#39;lies&#39;, &#39;a&#39;, &#39;small&#39;, &#39;un&#39;, &#39;regarded&#39;, &#39;yellow&#39;, &#39;sun&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Overall the results look good, but there are two errors: 'un','regarded' should be one word, and 'sitdown' should be two. Still, that’s a word precision rate of 157/159 = 98.7%; not too bad.</p>
<p>The first error is in part because “unregarded” does not appear in our 1/3-million-word
vocabulary. (It is in the full 13-million-word vocabulary at position 1,005,493, with count 7,557.) If we put it in the vocabulary, we see that the segmentation is correct:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The second error happens because, although “sit” and “down” are common words (with
probability .003% and .04%, respectively), the product of their two probabilities is just
slightly less than the probability of “sitdown” by itself. However, the probability of the
two-word sequence “sit down,” according to the bigram counts, is about 100 times
greater. We can try to fix this problem by modeling bigrams; that is, considering the probability of each word, given the previous word:</p>
$$P(W_{1:n}) = \prod_{k=1:n}P(W_k \; | \; W_{k-1})$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Of course the complete bigram table won’t fit into memory. If we keep only bigrams that
appear 100,000 or more times, that works out to a little over 250,000 entries, which does
fit. We can then estimate $P(down | sit)$ as Count(<em>sit down</em>)/Count(<em>sit</em>). If a bigram does not appear in the table, then we just fall back on the unigram value. We can define <code>cPw</code>, the conditional probability of a word given the previous word, as:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">cPw</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">prev</span><span class="p">):</span>
  <span class="s2">&quot;The conditional probability P(word | previous-word).&quot;</span>
  <span class="k">try</span><span class="p">:</span> 
    <span class="k">return</span> <span class="n">P2w</span><span class="p">[</span><span class="n">prev</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">word</span><span class="p">]</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">Pw</span><span class="p">[</span><span class="n">prev</span><span class="p">])</span>
  <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">Pw</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
  
<span class="n">P2w</span> <span class="o">=</span> <span class="n">Pdist</span><span class="p">(</span><span class="n">datafile</span><span class="p">(</span><span class="s1">&#39;count_2w.txt&#39;</span><span class="p">),</span> <span class="n">N</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(You may note cPw is not a probability distribution, because the sum over all words for a
given previous word can be greater than 1. This approach has the technical name stupid
backoff, but it works well in practice, so we won’t worry about it.) We can now compare
“sitdown” to “sit down” with a preceding “to”:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cPw</span><span class="p">(</span><span class="s1">&#39;sit&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">)</span> <span class="o">*</span> <span class="n">cPw</span><span class="p">(</span><span class="s1">&#39;down&#39;</span><span class="p">,</span> <span class="s1">&#39;sit&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cPw</span><span class="p">(</span><span class="s1">&#39;sitdown&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[14]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>1698.0002330199263</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bi-gram-Model">Bi-gram Model<a class="anchor-link" href="#Bi-gram-Model">&#182;</a></h2><p>We see that “sit down” is 1,698 times more likely than “sitdown”, because “sit down” is a
popular bigram, and because “to sit” is popular but “to sitdown” is not.</p>
<p>This looks promising! Let's implement a new version of segment using a bigram model. While we're at it, we'll fix two other issues.</p>
<ol>
<li><p>When <code>segment</code> added one new word to a sequence of <em>n</em> words segmented in the remainder, it called <code>Pwords</code> to multiply together at <em>n</em>+ probabilities. But segment had already multiplied all the probabilities in the remainder. It would be more efficient to remember the probability of the remainder and then just do one more multiplication.</p>
</li>
<li><p>There is a potential problem with arithmetic underflow. If we apply <code>Pwords</code> to a sequence consisting of the word “blah” repeated 61 times, we get 5.2•10$^{–321}$, but if we add one more “blah,” we get 0.0. The smallest positive floating-point number that can be represented is about 4.9•10$^{–324}$; anything smaller than that rounds to 0.0. To avoid underflow, the simplest solution is to add logarithms of numbers rather than multiplying the numbers themselves.</p>
</li>
</ol>
<p>We will define <code>segment2</code>, which differs from segment in three ways:</p>
<blockquote><p>1) <strong>Bigram Language Model</strong>: First, it uses a conditional bigram language model, cPw, rather than the unigram model Pw. <br>
2) <strong>Different Function Signature</strong>: Second, the function signature is different. Instead of being passed a single argument (the text), segment2 is also passed the previous word. At the start of the sentence, the previous word is the special beginning-of-sentence marker, <code>&lt;S&gt;</code>. The return value is not just a list of words, but rather a pair of values: the probability of the segmentation, followed by the list of words. We return the probability so that it can be stored (by memo) and need not be recomputed; this fixes problem (1), the inefficiency. The function <code>combine</code> takes four inputs—the first word and the remaining words, plus their probabilities—and combines them by appending the first word to the remaining words, and by multiplying the probabilities—except that in order to solve problem (2), we introduce the third difference...<br>
3) <strong>Add Logarithms</strong>: We add logarithms of probabilities instead of multiplying the raw probabilities. <br></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@memo</span>
<span class="k">def</span> <span class="nf">segment2</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prev</span><span class="o">=</span><span class="s1">&#39;&lt;S&gt;&#39;</span><span class="p">):</span>
  <span class="s2">&quot;Return (log P(words), words), where words is the best segmentation.&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span> <span class="k">return</span> <span class="mf">0.0</span><span class="p">,</span> <span class="p">[]</span>
  <span class="n">candidates</span> <span class="o">=</span> <span class="p">[</span><span class="n">combine</span><span class="p">(</span><span class="n">log10</span><span class="p">(</span><span class="n">cPw</span><span class="p">(</span><span class="n">first</span><span class="p">,</span> <span class="n">prev</span><span class="p">)),</span> <span class="n">first</span><span class="p">,</span> <span class="n">segment2</span><span class="p">(</span><span class="n">rem</span><span class="p">,</span> <span class="n">first</span><span class="p">))</span>
               <span class="k">for</span> <span class="n">first</span><span class="p">,</span> <span class="n">rem</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">(</span><span class="n">text</span><span class="p">)]</span>
  <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">combine</span><span class="p">(</span><span class="n">Pfirst</span><span class="p">,</span> <span class="n">first</span><span class="p">,</span> <span class="n">Prem_and_rem</span><span class="p">):</span>
  <span class="s2">&quot;Combine first and rem results into one (probability, words) pair.&quot;</span>
  <span class="n">Prem</span><span class="p">,</span> <span class="n">rem</span> <span class="o">=</span> <span class="n">Prem_and_rem</span>
  <span class="k">return</span> <span class="n">Pfirst</span> <span class="o">+</span> <span class="n">Prem</span><span class="p">,</span> <span class="p">[</span><span class="n">first</span><span class="p">]</span> <span class="o">+</span> <span class="n">rem</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>segment2</code> makes O(<em>nL</em>) recursive calls, and each one considers O(<em>L</em>) splits, so the wholealgorithm is O(<em>nL</em>$^2$). In effect this is the <strong>Viterbi</strong> algorithm, with <code>memo</code> implicitly creating the Viterbi tables.</p>
<p><code>segment2</code> correctly segments the “sit down” example, and gets right all the examples that
the first version got right. Neither version gets the “unregarded” example right.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">segment2</span><span class="p">(</span><span class="s1">&#39;inaholeinthegroundtherelivedahobbitnotanastydirtywetholefilledwiththeendsofwormsandanoozysmellnoryetadrybaresandyholewithnothinginittositdownonortoeatitwasahobbitholeandthatmeanscomfort&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>(-164.42574174168834, [&#39;in&#39;, &#39;a&#39;, &#39;hole&#39;, &#39;in&#39;, &#39;the&#39;, &#39;ground&#39;, &#39;there&#39;, &#39;lived&#39;, &#39;a&#39;, &#39;hobbit&#39;, &#39;not&#39;, &#39;a&#39;, &#39;nasty&#39;, &#39;dirty&#39;, &#39;wet&#39;, &#39;hole&#39;, &#39;filled&#39;, &#39;with&#39;, &#39;the&#39;, &#39;ends&#39;, &#39;of&#39;, &#39;worms&#39;, &#39;and&#39;, &#39;an&#39;, &#39;oozy&#39;, &#39;smell&#39;, &#39;nor&#39;, &#39;yet&#39;, &#39;a&#39;, &#39;dry&#39;, &#39;bare&#39;, &#39;sandy&#39;, &#39;hole&#39;, &#39;with&#39;, &#39;nothing&#39;, &#39;in&#39;, &#39;it&#39;, &#39;to&#39;, &#39;sit&#39;, &#39;down&#39;, &#39;on&#39;, &#39;or&#39;, &#39;to&#39;, &#39;eat&#39;, &#39;it&#39;, &#39;was&#39;, &#39;a&#39;, &#39;hobbit&#39;, &#39;hole&#39;, &#39;and&#39;, &#39;that&#39;, &#39;means&#39;, &#39;comfort&#39;])
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Could we improve on this performance? Probably. We could create a more accurate model
of unknown words. We could incorporate more data, and either keep more entries from
the unigram or bigram data, or perhaps add trigram data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
