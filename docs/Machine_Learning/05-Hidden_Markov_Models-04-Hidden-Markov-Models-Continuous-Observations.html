
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../Projects.html">Projects</a>
</li>
        
<li>
  <a href="../Book_Reviews.html">Book Reviews</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="7.-HMM's-with-Continuous-Observations">7. HMM's with Continuous Observations<a class="anchor-link" href="#7.-HMM's-with-Continuous-Observations">&#182;</a></h1><p>At this point we are ready to look at the use application of Hidden Markov Model's to discrete observations. All that is meant by continuous observations is that what you observe is a number on a scale, rather than a symbol such as heads or tails, or words. This is an interesting topic in and of itself, because it allows us to think about:</p>
<ul>
<li>What is continuous? </li>
<li>What is discrete?</li>
<li>What are we approximating as discrete that is really continuous?</li>
</ul>
<p>Think of an audio signal for a moment. Audio is simply sound, and sound is just air pressure that vibrates. By vibrating I just mean that it is oscillating in time. So, when audio is stored on your computer, obviously your computer cannot store an infinite number of values, and hence the air pressure has to be <strong>quantized</strong>; it has to be given a number that it can represent. Now, there are actually two things to quantize:</p>
<ol>
<li>The <strong>amplitude</strong> of the sound pressure $\rightarrow$ 16 bit int.</li>
<li>The <strong>time</strong> the amplitude occurred $\rightarrow$ 44100 Hz.</li>
</ol>
<p>As has been mentioned before in other posts, sound is generally sampled at 44.1 KHz. That is far too fast for your brain to realize it is not continuous. So, what happens when we use a hidden markov model, but we assume the observed variable is a mixture of gaussians? Well, in essence:</p>
<blockquote><p>The amplitude becomes continuous, and the time stays discrete.</p>
</blockquote>
<p>So, what changes when we use hidden markov models with <strong>Gaussian Mixture Models</strong>? First, the following is assuming you are familiar with GMM's (if not please review my posts on unsupervised learning). With that said, let's go through a very quick recap.</p>
<h2 id="1.-Gaussian-Mixture-Models-Review">1. Gaussian Mixture Models Review<a class="anchor-link" href="#1.-Gaussian-Mixture-Models-Review">&#182;</a></h2><p>Gaussian Mixture Models are a form of <strong>density estimation</strong>. They give us an approximation of the probability distribution of our data. We want to use gaussian mixture models when we notice that our data is multimodal (meaning there are multiple modes or bumps). From probability, we can recall that the <strong>mode</strong> is just the most common value.</p>
<p><img src="https://drive.google.com/uc?id=1d4ePx8RP9Cj1jxJYTjsZdsT4FU_a2qHz" width="400"></p>
<p>A Gaussian mixture is just the sum of weighted gaussians. To represent these weights we will introduce a new symbol called $\pi$. $\pi_k$ is the probability that x belongs to the $k$th Gaussian.</p>
<p>$$p(x) = \pi_1 N(\mu_1, \Sigma_1) + \pi_2 N(\mu_2, \Sigma_2) + \pi_3 N(\mu_3, \Sigma_3)$$</p>
<h3 id="1.1-$\pi$-is-a-distribution">1.1 $\pi$ is a distribution<a class="anchor-link" href="#1.1-$\pi$-is-a-distribution">&#182;</a></h3><p>Notice that there is a constraint here that all of the $\pi$'s have to sum to 1.</p>
<p>$$1 = \int p(x)dx = \int \pi_1 N(x | \mu_1, \Sigma_1)dx + \pi_2 N(x | \mu_2, \Sigma_2)dx$$
$$\pi_1 1 + \pi_2 1$$</p>
<h3 id="1.2-Latent-Variables">1.2 Latent Variables<a class="anchor-link" href="#1.2-Latent-Variables">&#182;</a></h3><p>Another way of thinking of this is that we introduced a new random variable called "Z". $Z$ represents which gaussian the data came from. So, we can say that:</p>
<p>$$\pi_k = P(Z = k)$$</p>
<p>This is like saying that there is some hidden cause called $Z$ that we can't measure. Each of these $Z$'s is causing a gaussian to be generated, and all we can see in our data is the combined effects of those individual $Z$'s. This will be important because it puts GMM's into the framework of <strong>expectation maximization</strong>.</p>
<h3 id="1.3-Training-a-GMM">1.3 Training a GMM<a class="anchor-link" href="#1.3-Training-a-GMM">&#182;</a></h3><p>Training a GMM is as follows:</p>
<ol>
<li><strong>Calculate Responsibilites</strong><br>
$R_k^{(n)}$ is the responsibility of the $k$th gaussian for generating the $n$th point. This is just the proportion of that gaussian, divided by all of the gaussians. If $\pi_k$ is large here, then it will overtake the other gaussians, and this will be approximately equal to 1. </li>
</ol>
<p>$$R_k^{(n)} = p(z^{(n)}|x) = \frac{\pi_k N (x^{(n)} | \mu_k, \Sigma_k) }{\sum_{j=1}^K \pi_j N (x^{(n)} | \mu_j, \Sigma_j)}$$</p>
<ol>
<li><strong>Calculate model parameters of the gaussians</strong>
We now need to recalculate the means, covariances, and $\pi$'s. The way that this is done is also similar to k-means, where we weight each samples influence on the parameter, by the responsibility. If that responsibility is small, then that $x$ matters less in the total calculation. </li>
</ol>
<p>$$\mu_k = \frac{1}{N_k}\sum_{n=1}^N R_k^{(n)} x^{(n)}$$</p>
<p>$$\Sigma_k = \frac{1}{N_k} \sum_{n=1}^N R_k^{(n)} (x^{(n)} - \mu_k)(x^{(n)} - \mu_k)^T$$</p>
<p>$$\pi_k = \frac{N_k}{N} \; with \; N_k = \sum_{n=1}^N R_k^{(n)}$$</p>
<h3 id="1.4-GMM's-and-HMM's">1.4 GMM's and HMM's<a class="anchor-link" href="#1.4-GMM's-and-HMM's">&#182;</a></h3><p>Now, the question is: How can we relate the concepts of GMM's back to what we already know about HMM's (specifically, to allow our HMM to deal with continuous data)? Remember that a Hidden Markov Model is defined by three things: $\pi$, $A$, and $B$. Because we are now working with continuous emissions, it seems that $B$ is what needs to change, since it used to be an $MxV$ matrix, and there are no longer just $V$ possible symbols.</p>
<p>Take a moment to think about what $\pi$ and $A$ really represent. They both are entirely related to what hidden state, $M$, we can expect to be in at a given time. Once we are in a specific hidden state, we know that $B$ determines the probability of observing a specific emission. This actually related to gaussian mixture models quite nicely! As we saw in the GMM recap, $\pi_k = P(Z = k)$. In other words, we have the concept of being in a specific hidden state, $Z$, and then the GMM has a process of determining the probability of observation; this will take the place of $B$.</p>
<p>How will this look in practice? Recall, that there are three things that we need for a Gaussian Mixture Model:</p>
<ul>
<li>The responsibilities, the probability of a specific gaussian: $R$</li>
<li>For each individual gaussian, the mean: $\mu$</li>
<li>For each individual gaussian, the covariance: $\Sigma$</li>
</ul>
<p>So, we are replacing $B$ by three new parameters: $R$, $\mu$ and $\Sigma$. We will use the letter $K$ to be the number of gaussians. We will store $R$ as an $MxK$ matrix, so there is one row for each hidden state, and then for each state there are $K$ different probabilities. Since this is a probability matrix, each row must sum to one.</p>

<pre><code>R = (M x K)</code></pre>
<p>Each individual $\mu$ is $D$ dimensional, and there are $K$ of them. We need that many for the $M$ states; hence, $\mu$ will be an $MxKxD$ matrix:</p>

<pre><code>mu = (M x K x D)</code></pre>
<p>$\Sigma$ will be $MxKxDxD$:</p>

<pre><code>sigma = (M x K x D x D)</code></pre>
<p>Once a state ($j$) has been chosen ($\pi$ and $A$ have done their job), we create a "$B$" observation probability matrix of size $MxT$:</p>
<p>$$B(j,t) = \sum_{k=1}^K \overbrace{R(j,k) N\Big( x(t), \mu(j,k), \Sigma(j,k)\Big)}^\text{Gaussian Mixture Model}$$</p>
<p>Even though we don't technically have a $B$ matrix anymore, we can still make one by calculating it for each sequence. And we will store the individual mixture components as well:</p>
<p>$$Comp(j, k, t) = R(j,k) N\Big( x(t), \mu(j,k), \Sigma(j,k)\Big)$$</p>
<h4 id="Expectation-Step">Expectation Step<a class="anchor-link" href="#Expectation-Step">&#182;</a></h4><p>We can then calculate a new $\gamma$, which is part of the expectation step:</p>
<p>$$\gamma(j,k,t) = \frac{\alpha(t, j) \beta(t, j)}{\sum_{j'=1}^M \alpha(t, j') \beta(t, j')} 
\frac{R(j,k) N\Big( x(t), \mu(j,k), \Sigma(j,k)\Big)}{\sum_{k'=1}^K R(j,k') N\Big( x(t), \mu(j,k'), \Sigma(j,k')\Big)} 
$$</p>
<p>$$\gamma(j,k,t) = \frac{\alpha(t, j) \beta(t, j)}{\sum_{j'=1}^M \alpha(t, j') \beta(t, j')} 
\frac{Comp(j, k, t)}{B(j,t)} 
$$</p>
<h4 id="Maximization-Step">Maximization Step<a class="anchor-link" href="#Maximization-Step">&#182;</a></h4><p>We can now define the updates for $R$, $\mu$ and $\Sigma$:</p>
<p>$$R(j, k) = \frac{\sum_{t=1}^T \gamma(j, k, t)}{\sum_{t=1}^T \sum_{k'=1}^K \gamma(j, k', t)}$$</p>
<p>$$\mu(j, k) = \frac{\sum_{t=1}^T \gamma(j,k,t)x(t)}{\sum_{t=1}^T \gamma(j,k,t)}$$</p>
<p>$$\Sigma(j,k) = \frac{\sum_{t=1}^T \gamma(j, k, t)\big(x(t) - \mu(j,k)\big) \big(x(t) - \mu(j, k) \big)^T}{\sum_{t=1}^T \gamma(j, k, t)}$$</p>

</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
