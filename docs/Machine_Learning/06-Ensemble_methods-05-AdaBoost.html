
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="5.-AdaBoost-Algorithm">5. AdaBoost Algorithm<a class="anchor-link" href="#5.-AdaBoost-Algorithm">&#182;</a></h1><p>We are now going to talk about <strong>boosting</strong>, and introduce a realization of the boosting idea, the algorithm <strong>AdaBoost</strong>. It is currently still one of the most powerful ensemble methods in existence, and like the random forest it is considered a good off the shelf/plug and play model.</p>
<p>The idea behind <strong>boosting</strong> is very different from <strong>bagging</strong> and <strong>random forest</strong>. Recall, in those instances we wanted a <strong>low bias, high variance</strong> model. In boosting, we actually want <strong>high bias</strong> models. In boosting nomenclature these are called <strong>weak learners</strong>. Weak learners generally have classification accuracies not much better than chance-in general about 50-60%. We will show later though, that even after ensembling the variance will remain low, and the test error will remain low, and the ensemble will not overfit-even when we keep adding more trees.</p>
<p><br></p>
<h2 id="1.1-Boosting">1.1 Boosting<a class="anchor-link" href="#1.1-Boosting">&#182;</a></h2><p>The hypothesis behind boosting is that if we <strong>combine many weak learners</strong>, as a whole they will be a strong learner. As in our stacking example earlier, what we are going to do is weight each learner so that each base model has a different amount of influence on the output.</p>
$$F(x) = \sum_{m=1}^M \alpha_mf_m(x)$$<p><br></p>
<h3 id="1.1.1-Weak-Learners">1.1.1 Weak Learners<a class="anchor-link" href="#1.1.1-Weak-Learners">&#182;</a></h3><p>So, how do we create or find weak learners? A typical way to do that with boosting is to use decision trees with a max depth of 1. These are also called <strong>decision stumps</strong>. A decision stump is just one split, in one dimension. In other words, you are only splitting the space in half for each tree. In other words, it is actually remarkable that a combination of these can actually yield a good classifier.</p>
<p>Another way is just to use a linear classifier like logistic regression. An added bonus of using such simple models is that they train extremely fast. This means that you can quickly train an ensemble of thousands of trees.</p>
<p><br></p>
<h3 id="1.1.2-Details">1.1.2 Details<a class="anchor-link" href="#1.1.2-Details">&#182;</a></h3><p>In Adaboost, which we will get to shortly, there are a couple of details which we have to make note of. First and foremost, Adaboost used <strong>{-1, +1}</strong> as targets, rather than <strong>{0, 1}</strong>. This will become clear why when we see the algorithm. So, for example if we were doing logistic regression we would rescale the output by multiplying by 2 and subtracting by 1.</p>
$$ModifiedLogisticOutput(x) = 2*LogisticOutput(x) - 1$$<p>The final output of Adaboost is:</p>
$$F_M(x) = sign(\sum_{m=1}^M \alpha_mf_m(x))$$<p>Where $f_m$ represents the individual base learners, $F_M$ is the ensemble model, with $M$ being the number of base learners. Since the targets are -1 and +1, the decision boundary is 0, so we just take the sign to determine the final prediction. We typicaly use $\alpha$ as the symbol to weight each classifier, since $w$ will be used for something else-in particular, to tell us how important each sample is.</p>
<p><br></p>
<h2 id="1.2-AdaBoost">1.2 AdaBoost<a class="anchor-link" href="#1.2-AdaBoost">&#182;</a></h2><p>Let's now talk about the <strong>AdaBoost</strong> algorithm itself. The idea is that we are going to add each base model one at a time, which is called <strong>additive modeling</strong>. We train the base model on all of the training data, which means there is no resampling or bootstrapping here. The difference between this and the other algorithms that we have studied, is that we are going to weight how important each sample is, using $w_i$ for $i = 1...N$. We will modify $w_i$ for each round. So intitially they will all be equal, but if we get the prediction for the pair $x_i, y_i$ wrong, then we will increase $w_i$ for that round. In this way, the base model knows which samples are more important. Else we will decrease $w_i$.</p>
<p>You can imagine that this may require us to modify the cost function. For example, for logistic regression, you would need to multiple each individual cross entropy by the weight for that sample.</p>
$$J_{weighted} = \sum_{i=1}^N w_i\Big[t_ilogy_i +(1-t_i)log(1-y_i) \Big]$$<p>For the decision tree, luckily the scikit learn API already allows us to pass in sample weights to our fit function!</p>
<p>Once we have trained the base model (on all data $X,Y$ with weights $w_i$, where $i=1..N$), we calculate its error weighted by $w_i$. We then compute $\alpha_m$, which represents how important this base model is to the final model as a function of the error. Note that if we had less error (our model was more accurate) then $\alpha_m$ should be bigger. Then we store $\alpha_m$ and we store the base model $f_m$. Once the loop is done, and we hit the specified number of base models, we exit the loop and we are done training.</p>
<p><br></p>
<h3 id="1.2.1-AdaBoost-Pseudocode">1.2.1 AdaBoost Pseudocode<a class="anchor-link" href="#1.2.1-AdaBoost-Pseudocode">&#182;</a></h3><hr>
<p><strong>Explained/with Equations</strong></p>
<ul>
<li>We want to start by giving $w_i$ a uniform distribution, so each sample has equal importantance. </li>
<li>Then in a loop, we create a base model $f_m$ and train it on all the data with the weights in $w$</li>
<li>We then calculate the error for this iteration, $\epsilon_m$, which is also weighted by the sample weights
$$\epsilon_m = \frac{\sum_{i=1}^N w_i I(y_i \neq f_m(x_i))}{\sum_{i=1}^N w_i}$$</li>
<li>We calculate $\alpha_m$ which is the log ratio of the weighted correct rate to the weighted error rate. 
$$\alpha_m = \frac{1}{2}log \Big[\frac{1-\epsilon_m}{\epsilon_m} \Big]$$</li>
<li>Essentially this means that if the model is more correct, it gets a higher weight $\alpha$</li>
<li>Note, the sum over $w$ when we calculate the error rate $\epsilon$ is not necessary if we normalize $w$ like we do in the second last step. It is left there for completion sake, and to remember that the error will be a number between 0 and 1. </li>
<li>Next, we update the $w_i$'s. We can see that if we are correct, the $y_i$ and $f_m(x_i)$ are the same sign, hence $w_i$ decreases. If we are wrong, then $y_i$ and $f_m(x_i)$ are of opposite signs, so $w_i$ increases. This is why we require the targets to be either -1 or +1.
$$w_i = w_i* exp\Big[-\alpha_my_if_m(x_i)\Big], i =1,...,N$$</li>
<li>We then normalize $w$ because we treat it like a probability distribution 
$$w_i = \frac{w_i}{\sum_{i=1}^N w_i}$$</li>
<li>The last step is to save $\alpha_m$ and $f_m(x)$</li>
</ul>
<p><strong>Pseudocode</strong></p>

<pre><code>Initialize w[i] = 1\N for i=1..N
for m=1..M:
  Fit f_m(x) with sample weights w[i]
  e_m = (w * (y != f_m(x) )) / w
  a_m = 0.5 * log((1 - e_m) / e_m)
  w = exp(-a_m * y * f_m(x))
  save a_m, f_m(x)</code></pre>
<hr>
<p>We can notice that the Adaboost algorithm is very specific to binary classification, requiring the two classes to be <strong>{-1, +1}</strong>. There are extensions in literature that discuss adaboost modifications for multiclass classification and regression. Our goal is just to get the main idea down. If you do desire to work with adaboost for multiclass classification or regression, scikit learn comes packaged with those already. As with random forest, the authors recommend using trees as the ideal base model, but linear classifiers are common as well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="2.-Additive-Modeling">2. Additive Modeling<a class="anchor-link" href="#2.-Additive-Modeling">&#182;</a></h1><p>We are now going to look at AdaBoost as an instance of additive modeling. Particularly, we are going to talk about a specific instance of additive modeling, called <strong>forward stagewise additive modeling</strong>. It is called this because at each stage we add a new base model, without modifying any of the existing base models we have already added.</p>
<h2 id="2.1-Forward-Stagewise-Additive-Modeling">2.1 Forward Stagewise Additive Modeling<a class="anchor-link" href="#2.1-Forward-Stagewise-Additive-Modeling">&#182;</a></h2><p>The general algorithm is seen below. Note that as conventions $L(y, f(x))$ is the loss/cost function given target $y$ and model $f(x)$. $F$ is the full model, and $f$ is the base model.</p>
<ol>
<li>We intialize our full model to 0</li>
<li>We begin by going into a loop from 1..M</li>
<li>We then create a base model, and find the best alpha and best model parameters, such that we minimize the total cost of the full model so far. 
$$(\alpha_m^*, \theta_m^*) = argmin_{\alpha_m, \theta_m} \sum_{i=1}^N L(y_i, F_{m-1}(x_i) + \alpha_mf_m(x_i;\theta_m))$$</li>
<li>The final step inside of the loop is to add this weighted model to the full model. Notice that this is recursive.
$$F_m(x) = F_{m-1}(x) + \alpha_m^* f_m(x;\theta_m^*)$$
Note what this model does not specify. It doesn't tell us that we have to weight the data points in any particular way. It doesn't tell us what cost function to use. Those are specific to any implementation of this, so AdaBoost is one such example. You can imagine that if we chose the wrong cost function, or wrong type of base model, or we did not weight the samples at all then we would get worse results. </li>
</ol>

<pre><code>Initialize F_0(x) = 0
for m = 1..M:
  Create base model
  Find the best alpha and best model parameters such that we minimize the full model so far
  Add the weighted base model to the full model</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="3.-AdaBoost-Loss-Function---Exponential-Loss">3. AdaBoost Loss Function - Exponential Loss<a class="anchor-link" href="#3.-AdaBoost-Loss-Function---Exponential-Loss">&#182;</a></h1><p>We have looked at many loss functions already:</p>
<ul>
<li>Binary cross-entropy (binary classification)</li>
<li>Multiclass cross-entropy (multiclass classification)</li>
<li>Squared Error (regression)</li>
<li>Absolute Error (regression)</li>
<li>Accuracy (perceptron)</li>
</ul>
<p>Now we are going to look at one more - the loss function that AdaBoost uses - Exponential Loss. Because our outputs are {-1, +1} it does not make sense to use cross entropy.</p>
<h2 id="3.1-Exponential-Loss">3.1 Exponential Loss<a class="anchor-link" href="#3.1-Exponential-Loss">&#182;</a></h2><p>The exponential loss function is defined as follows:</p>
$$L\Big(y, f(x)\Big) = exp\Big(-yf(x)\Big)$$<p>We can see that as always it takes in two arguments, the target $y$, and $f(x)$ which is the model. We can see that when $y$ and $f(x)$ are the same sign, the output approaches 0. When $y$ and $f(x)$ are opposite signs, the output approaches infinity. This means we still have the same asymptotic effect of the cross entropy function.</p>
<h2 id="3.2-Cross-Entropy-Comparison">3.2 Cross Entropy Comparison<a class="anchor-link" href="#3.2-Cross-Entropy-Comparison">&#182;</a></h2><p>Let's consider the positive class, $y=1$, for cross entropy. Recall, that with cross entropy and logistic regression, the input into the sigmoid, which we call the <strong>logit</strong>, has to equal infinity for the sigmoid to equal 1. Otherwise, we can get very close to 1, but not quite equal to one. Mathematically that looks like:</p>
$$Let \; f(x) = \sigma(w^Tx)$$$$CrossEntropy(y, f(x)) = ylog(f(x)) + (1-y)log(1 - f(x))$$<p>If our class is 1, i.e. y = 1, then if our prediction, $f(x)$ is 1, we have 0 loss. However, since $f(x)$ is the output of the sigmoid function, we would need the input to the sigmoid (the logit), $w^Tx$,  to equal infinity. Since $w^Tx$ can never reach infinity, we can keep increasing it forever.</p>
<h2 id="3.3-Relationship-to-Squared-Error">3.3 Relationship to Squared Error<a class="anchor-link" href="#3.3-Relationship-to-Squared-Error">&#182;</a></h2><p>This is why we don't want to use squared error for classification. Recall that squared error is defined as:
$$L = \Big[y - f(x)\Big]^2$$</p>
<p>Squared error does not care which side of $y$ $f(x)$ is on-anything that is far away from $y$ will increase the error. If $f(x)$ is on the right side of $y$, it doesn't matter how big it is, as long as it leads to the right prediction.</p>
<p><img src="https://drive.google.com/uc?id=1axW2Hc4F_AQSie-BwyqTAgF0bB2o8lL0"></p>
<h2 id="3.4-Additive-Modeling-Algorithm---AdaBoost-Derivation">3.4 Additive Modeling Algorithm - AdaBoost Derivation<a class="anchor-link" href="#3.4-Additive-Modeling-Algorithm---AdaBoost-Derivation">&#182;</a></h2><p>No we know that using exponential loss "makes sense" in this situation, but we can take it one step further. In particular, we are going to use the additive modeling definition, and solve for the best model weight, and in the process recover all of the equation that we saw in AdaBoost Algorithm. That means we will recover the equations for $\alpha$, the sample weights $w$, and the weighted errors.</p>
<p><br></p>
<h3 id="3.4.1-Replace-General-Cost-function-with-Exponential-Loss">3.4.1 Replace General Cost function with Exponential Loss<a class="anchor-link" href="#3.4.1-Replace-General-Cost-function-with-Exponential-Loss">&#182;</a></h3><p>So, we are starting at this point:</p>
$$(\alpha_m^*, \theta_m^*) = argmin_{\alpha_m, \theta_m} \sum_{i=1}^N L(y_i, F_{m-1}(x_i) + \alpha_mf_m(x_i;\theta_m))$$<p>The above equation is just saying that the best value for $\alpha$ and the best value for $\theta$ is that which minimize the total cost over $N$ training examples. So, our first step is to replace the general cost function with the exponential loss that we discussed earlier. 
$$(\alpha_m^*, f_m^*) = argmin_{\alpha_m, f_m} \sum_{i=1}^N exp\Big[-y_i*\Big(F_{m-1}(x_i) + \alpha_mf_m(x_i)\Big)\Big]$$</p>
<p>Recall that we are using the convention $F_M$ means the full ensemble model, and $f_m$ means the mth base model.</p>
<p><br></p>
<h3 id="3.4.2-Expand-Terms-in-Exponential">3.4.2 Expand Terms in Exponential<a class="anchor-link" href="#3.4.2-Expand-Terms-in-Exponential">&#182;</a></h3><p>We can now expand the inner terms of the exponential above. Meaning we start with:
$$J = \sum_{i=1}^N exp\Big[-y_i*\Big(F_{m-1}(x_i) + \alpha_mf_m(x_i)\Big)\Big]$$
And expand it to be:
$$J = \sum_{i=1}^N exp\Big[-y_iF_{m-1}(x_i) \Big] exp \Big[- y_i\alpha_mf_m(x_i)\Big]$$</p>
<p><br></p>
<h3 id="3.4.3-1st-Exponential-is-Proportional-to-$w_i$-at-iteration-$m$">3.4.3 1st Exponential is Proportional to $w_i$ at iteration $m$<a class="anchor-link" href="#3.4.3-1st-Exponential-is-Proportional-to-$w_i$-at-iteration-$m$">&#182;</a></h3><p>Let's look a little closer at the first exponential for a second:
$$exp\Big[-y_iF_{m-1}(x_i) \Big]$$
We are going to prove that this exponential is proportional to $w_i^{(m)}$, meaning we can write it like so:
$$J = \sum_{i=1}^N w_i^{(m)}exp \Big[- y_i\alpha_mf_m(x_i)\Big]$$
Let's show that proof now.</p>
<p><br></p>
<h3 id="3.4.3.1-Proof-of-$w_i$-proportionality">3.4.3.1 Proof of $w_i$ proportionality<a class="anchor-link" href="#3.4.3.1-Proof-of-$w_i$-proportionality">&#182;</a></h3><p>To show that the above idea is true, we must recall the update rule for $w_i$. It looked like:
$$w_i^{(m)} = w_i^{(m-1)} exp\Big(-y_i \alpha_{m-1}f_{m-1}(x_i)\Big)$$
We can keep defining $w_i$ in terms of earlier and earlier $w_i$s! For instance, we can plug in the value for $w_i^{(m-1)}$ and find:
$$w_i^{(m)} = w_i^{(m-2)} exp\Big(-y_i \alpha_{m-2}f_{m-2}(x_i)\Big)exp\Big(-y_i \alpha_{m-1}f_{m-1}(x_i)\Big)$$
This means we can define $w_i^{(m)}$ as:
$$w_i^{(m)} = w_i^{(0)} exp\Big( - y_i \sum_{m'=1}^{m-1} \alpha_{m'}f_{m'}(x_i) \Big)$$
And we know that $w_i^{(0)}$ is proportional to 1, because we made $w$ take on the uniform distribution:
$$w_i^{(0)} = \frac{1}{N}$$
So, we can replace $w_i^{(0)}$ with 1, and see that the sum is actually just equivalent to $F_{m-1}$! Meaning our equation for $w_i^{(m)}$ turns into:
$$w_i^{(m)} = 1 * exp \Big( - y_i F_{m-1} (x_i)\Big)$$</p>
<p>Awesome! We have now recovered our first update rule from the adaboost algorithm! We know how to update $w_i$ in terms of it's previous value. To recap, we started with:<br>
$$J = \sum_{i=1}^N exp\Big[-y_iF_{m-1}(x_i) \Big] exp \Big[- y_i\alpha_mf_m(x_i)\Big]$$
And we then proved that the first exponential could we defined as $w_m^{(m)}$. Once we sub that in, we arrive at our cost:
$$J = \sum_{i=1}^N w_i^{(m)}exp \Big[- y_i\alpha_mf_m(x_i)\Big]$$</p>
<p>Let's now return to the cost function.</p>
<p><br></p>
<h3 id="3.4.4-Realize-$y*f$-will-always-be-+1-or--1">3.4.4 Realize $y*f$ will always be +1 or -1<a class="anchor-link" href="#3.4.4-Realize-$y*f$-will-always-be-+1-or--1">&#182;</a></h3><p>We can then realize the $y$ times $f$ is always going to be either $+1$ or $-1$ depending on whether the prediction is right or wrong. So, we can split the sum into two terms, the correct terms and incorrect terms:</p>
$$J = e^{-\alpha_m} \sum _{y_i = f_m(x_i)} w_i^{(m)} + e^{\alpha_m} \sum _{y_i \neq f_m(x_i)} w_i^{(m)}$$<p><br></p>
<h3 id="3.4.5-Rename-and-Simplify">3.4.5 Rename and Simplify<a class="anchor-link" href="#3.4.5-Rename-and-Simplify">&#182;</a></h3><p>We can simplify this a little bit by renaming things. We will call the first sum, the number of weighted corrects $A$, and we will call the second sum the weighted of incorrects $B$. We will also just drop the subscript $m$ since it is just getting in our way.</p>
$$J = e^{-\alpha}A + e^\alpha B$$<p></p>
<p>This has taken us to our new cost function!</p>
<p><br></p>
<h3 id="3.4.5-Take-derivative,-set-to-0,-solve-for-alpha">3.4.5 Take derivative, set to 0, solve for alpha<a class="anchor-link" href="#3.4.5-Take-derivative,-set-to-0,-solve-for-alpha">&#182;</a></h3><p>We now want to take the derivative with repsect to $\alpha$, set to 0, and solve for $\alpha$. That will allow us to find the value of $\alpha$ that minimizes our cost, $J$.</p>
$$\frac{\partial J}{\partial \alpha} = -e ^{-\alpha}A + e^\alpha B = 0$$$$e ^{-\alpha}A = e^\alpha B$$$$-\alpha + lnA = \alpha + lnB$$$$2\alpha = lnA - lnB $$$$\alpha_m = \frac{1}{2} ln(\frac{A}{B}) = \frac{1}{2} ln(\frac{number \;weighted \; correct}{number \;weighted\;incorrect})$$<p>We can see that this is equal to our update equation for $\alpha$ in the adaboost algorithm! Note, the number correct, and the number incorrect, are represented as error rates in the orignal algorithm. But, since we would be dividing by the same number on both top and bottom, they cancel out.</p>
<hr>
<p><br></p>
<h2 id="3.5-Summary">3.5 Summary<a class="anchor-link" href="#3.5-Summary">&#182;</a></h2><p>Let's try and summarize what we just did, since that was rather long.</p>
<blockquote><ul>
<li>In the previous section, we looked at a general algorithm for <strong>forward stagewise additive modeling</strong>, and we claimed that AdaBoost was an instance of that algorithm. </li>
<li>The general additive algorithm does not specify the loss function or any other details. </li>
<li>We looked at the exponential loss function as a reasonable loss function for adaboost, where the targets are -1 and +1.  </li>
<li>We then plugged that into the additive modeling equation for finding the next $\alpha$</li>
<li>By manipulating the loss function, we recovered our update equation for $w_i$ - the sample weights for telling us how important each sample is for each base model we add</li>
<li>We saw that that recursive definition for $w_i$ exactly fits in this framework </li>
<li>We then optimized the loss function with respect to $\alpha$, to recover our update equation for $\alpha_m$ </li>
<li>We finally took the derivative of the cost $J$ with respect to $\alpha$, and set it to 0 in order to solve for the optimal $\alpha$. This is very intellectually satisfying, since it places adaboost in the same framework as many of the other models we have worked with-especially the perceptron, logistic regression, and neural networks. </li>
</ul>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="4.-AdaBoost-Implementation">4. AdaBoost Implementation<a class="anchor-link" href="#4.-AdaBoost-Implementation">&#182;</a></h1><p>We are now going to implement AdaBoost in code. Note that unlike random forest, AdaBoost doesn't require us to reimplement parts of the decision tree. This is due to the fact that the scikit learn api allows us to pass in the sample weights into the fit function.</p>
<p>We can start with our usual imports.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">rf_classification</span> <span class="k">import</span> <span class="n">get_data</span>

<span class="c1"># Seaborn Plot Styling</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;husl&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;poster&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;ticks&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now want to define our AdaBoost Class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">AdaBoost</span><span class="p">:</span> 
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>         
    <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>                      <span class="c1"># Number of Base Learners</span>
    
  <span class="sd">&quot;&quot;&quot;Fit Function&quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>                <span class="c1"># Instance variable to store decision stumps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="p">[]</span>                <span class="c1"># Instance variable to store the correspond weight alpha</span>
    
    <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>                  <span class="c1"># Get number of samples </span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>              <span class="c1"># Initialize W to be uniform distribution, 1/N</span>
    
    <span class="sd">&quot;&quot;&quot; ----------- Loop through total number of base learners ----------- &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">):</span>
      <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># Create new decision stump instance </span>
      <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">W</span><span class="p">)</span>           <span class="c1"># Fit tree to data, pass in sample weights </span>
      <span class="n">P</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>              <span class="c1"># Get Predictions, so we can calculate weighted errors </span>
      
      <span class="c1"># Note that we do not need to divide by w here, because we already normalized it </span>
      <span class="c1"># in the w update, thus w is always a probability distribution</span>
      <span class="c1"># Recall that in numpy we like to vectorize as many operations as possible. So, w is an</span>
      <span class="c1"># N length vector, and P != Y is also an N length vector. Therefore, the sum over the </span>
      <span class="c1"># element wise multiplication of those two things is also the dot product. </span>
      <span class="n">err</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">P</span> <span class="o">!=</span> <span class="n">Y</span><span class="p">)</span>
      
      <span class="sd">&quot;&quot;&quot; ----------- Calculate updates for alpha and w ----------- &quot;&quot;&quot;</span>
      <span class="c1"># Alpha calculation differs from that of pseudocode, but due to rules of logging it is </span>
      <span class="c1"># also correct. If we can turn multiplication and division into addition and subtraction</span>
      <span class="c1"># we generally do because it is faster and more numerically stable </span>
      <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">err</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">err</span><span class="p">))</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">*</span> <span class="n">P</span><span class="p">)</span>                  <span class="c1"># vectorized update</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">/</span> <span class="n">W</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>                                 <span class="c1"># Normalize so it sums to 1</span>
    
      <span class="sd">&quot;&quot;&quot; ----------- Add decision stump (model) and alpha to list ----------- &quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
      
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="c1"># Notice that this does not look like the scikit learn API, since we want to return</span>
    <span class="c1"># 2 things. The reason is that later we will want to compare plots of both exponential</span>
    <span class="c1"># loss and accuracy simultaneously. To calculate the accuracy, we need the prediction, </span>
    <span class="c1"># which is the sign of F(X). But, to caclulate the loss, we need F(X)  itself before</span>
    <span class="c1"># we take the sign, so we just return both. Also, note that there is no way to vectorize</span>
    <span class="c1"># the prediction, since we need to call predict for every stump object, meaning they </span>
    <span class="c1"># will be separate function calls.</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">FX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
      <span class="n">FX</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                <span class="c1"># update F(X) with each weighted prediction</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">FX</span><span class="p">),</span> <span class="n">FX</span>                         <span class="c1"># Return prediction and actual F(X)</span>
    
  <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="c1"># Similar to Scikit Learn API, but will also be returning loss in addition to accuracy</span>
    <span class="c1"># Notice the loss is normalized by the number of samples, in other words we take the mean</span>
    <span class="c1"># instead of the sum. This ensures that the loss and the accuracy will be on the same </span>
    <span class="c1"># scale, somewhere between 0 and 1</span>
    <span class="n">P</span><span class="p">,</span> <span class="n">FX</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>              <span class="c1"># Get predictions and actual F(X)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Y</span><span class="o">*</span><span class="n">FX</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>             <span class="c1"># Calculate final exponential loss (normalized)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">P</span> <span class="o">==</span> <span class="n">Y</span><span class="p">),</span> <span class="n">L</span>            
    
    
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">()</span>              
  <span class="n">Y</span><span class="p">[</span><span class="n">Y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>                         <span class="c1"># Make targets -1, +1, since adaboost requires that</span>
  <span class="n">Ntrain</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
  <span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">Ntrain</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:</span><span class="n">Ntrain</span><span class="p">]</span>
  <span class="n">Xtest</span><span class="p">,</span> <span class="n">Ytest</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">Ntrain</span><span class="p">:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">Ntrain</span><span class="p">:]</span>
  
  <span class="c1"># Main loop, we will test 1 up to 200 trees, which is more than enough to see convergence </span>
  <span class="n">T</span> <span class="o">=</span> <span class="mi">200</span>
  <span class="n">train_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
  <span class="n">test_losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
  <span class="n">test_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">num_trees</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">num_trees</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">train_errors</span><span class="p">[</span><span class="n">num_trees</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">test_errors</span><span class="p">[</span><span class="n">num_trees</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">test_losses</span><span class="p">[</span><span class="n">num_trees</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">continue</span>
    <span class="k">if</span> <span class="n">num_trees</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">num_trees</span><span class="p">)</span>
  
    <span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoost</span><span class="p">(</span><span class="n">num_trees</span><span class="p">)</span>           <span class="c1"># Create AdaBoost instance with num_trees instances</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span><span class="p">)</span>             <span class="c1"># Fit to train data</span>
    <span class="n">acc</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
    <span class="n">acc_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span><span class="p">)</span>
    <span class="n">train_errors</span><span class="p">[</span><span class="n">num_trees</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">acc_train</span>
    <span class="n">test_errors</span><span class="p">[</span><span class="n">num_trees</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">acc</span>
    <span class="n">test_losses</span><span class="p">[</span><span class="n">num_trees</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>
    
    <span class="k">if</span> <span class="n">num_trees</span> <span class="o">==</span> <span class="n">T</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;final train error:&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">acc_train</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;final test error:&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">acc</span><span class="p">)</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test errors&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test losses&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train errors&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test errors&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>dimensionality: 139
20
40
60
80
100
120
140
160
180
final train error: 0.0
final test error: 0.0
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Awesome! We can see that as we increase the number of models we do not overfit! The test error remains 0!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="5.-AdaBoost-vs.-Stacking">5. AdaBoost vs. Stacking<a class="anchor-link" href="#5.-AdaBoost-vs.-Stacking">&#182;</a></h1><p>We are now going to take a minute to compare to compare AdaBoost vs Stacking. We will see that AdaBoost is a big improvement on the old stacking idea. Remember, both of these methods use a weighted ensemble of base models:</p>
$$F(x) = \sum_{m=1}^M \alpha_m f_m(x)$$<p>The next step in our stacking setup was we setup a cost function to minimize. We used the squared error in our earlier example, but that need not be the case. We can choose any cost function for this, and the idea remains the same. It was only because we choose the squared error the first time that we could arrive at the quadratic programming solution. If we don't use squared error, we can still attempt to minimize the loss, we just need to use a different method, perhaps gradient descent. Now, we can just call the loss $L$ (being agnostic towards the loss function). So let's compare these two algorithms side by side:</p>
$$\textbf{AdaBoost} \rightarrow J = \frac{1}{N} \sum_{i=1}^N L \Big[y_i, F_M(x_i)\Big] = \frac{1}{N} \sum_{i=1}^N L \Big[y_i, \sum_{m=1}^M \alpha_m f_m(x_i|W^{(m)})\Big]$$$$\textbf{Stacking} \rightarrow J = \frac{1}{N} \sum_{i=1}^N L \Big[y_i, F_M(x_i)\Big] = \frac{1}{N} \sum_{i=1}^N L \Big[y_i, \sum_{m=1}^M \alpha_m f_m^{-i}(x_i)\Big]$$<p>We can see above that in AdaBoost we just have some base classifier $f_m$ that is trained given some sample weights $W^{(m)}$. In stacking, the base classifier is like a leave one out cross validated classifier. Recall, the $-i$ means that this $f_m$ is trained on every sample except the $ith$ sample.</p>
<p><br>
<strong>General Stacking Method</strong><br>
So, the cost functions were not extremely different, but they were a little different. Let's talk about how we find the alphas, which are the model weights next. We can think about a general method to train the stacking ensemble. The main thing is that we need to construct this cost function, and then optimize it with respect to alpha. To do that, we need to find all of the $f_m^{-i}$. In other words, we need to know how to construct the whole cost function before we even begin optimizing the alphas. Aka, we must know all of the $f$ values for i=1..N, and m = 1..M, before we can optimize $L$! Because $f_m^{-i}$ is indexed by $m$ and $i$, that means those values will be stored in a matrix of size <strong>(M x N)</strong>. Therefore, the number of models we need to train is <strong>O(NM)</strong>. That is <strong>quadratic complexity</strong>! And, since model training isn't fast, it is not a scalable solution.</p>
<p><br>
<strong>Complexity</strong><br>
You can see that we are stepping away from machine learning for a minute in order to think in terms of algorithms and complexity. In terms of algorithms, what does AdaBoost do? Well, it is actually a Greedy algorithm! It doesn't try to optimize all of the alphas at the same time. You would think that being a greedy algorithm, the solution would be suboptimal, but we have already seen that AdaBoost performs very well. AdaBoost is greedy because it only uses the current weights $W^{(m)}$ to find the weighted error, $err_m$, and to calculate $\alpha_m$. Then, on every subsequent iteration, $\alpha_m$ never changes. In other words, once we calculate $\alpha_1$ we never return to recalculate it. This is true for all earlier parameters. By building the classifier in an additive way, or a greedy way, we only need to train <strong>O(M)</strong> models. So, rather than having a problem that has quadratic complexity, we have a problem that has linear complexity!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><br></p>
<h1 id="6.-AdaBoost-Connection-to-Deep-Learning">6. AdaBoost Connection to Deep Learning<a class="anchor-link" href="#6.-AdaBoost-Connection-to-Deep-Learning">&#182;</a></h1><p>We are now going to make a connection between AdaBoost to Deep Learning! Recall that we do not need to use a tree as a base learner, we can also use logisitc regression (scaled to -1 and +1). If we write out the entire equation for the AdaBoost output, in terms of the individual base logistic regressions, we see a form that is very similar to that of a neural network!</p>
$$\textbf{AdaBoost} \rightarrow \hat{y} = sign \Big(\sum_{m=1}^M \alpha_m sign(w_m^Tx)\Big)$$$$\textbf{Neural  Net} \rightarrow \hat{y} = sign \Big(\sum_{m=1}^M \alpha_m tanh(w_m^Tx)\Big)$$<p>The main difference is that adaboost uses base classifiers with hard outputs; aka actual predictions equal to -1 and +1, rather than soft outputs where you get a number between -1 and +1. So the structure of these is the same, even though the training algorithms are different.</p>
<p>So an interesting conclusion that we can draw from this is that <strong>adaboost</strong> (with a logistic regression base learner) has the same structure as a 1-hidden-layer neural network, that can dynamically expand the size of its hidden layer. This is about as far as the analogy goes, since training AdaBoost is greedy, and you only get to set the current alpha given all the previous alphas and $w$'s. With neural networks, you update all of the weights at the same time, so it is more of a global optimization.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
