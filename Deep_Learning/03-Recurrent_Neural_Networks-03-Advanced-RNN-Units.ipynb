{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced RNN Units\n",
    "We are now going to move into the realm of advanced RNN units. Up until this point, we have been dealing with the simple recurrent unit exclusively. However, implementing **Rated** units, **Gated Recurrent Units**, and **Long Short-Term Memory** will allow our models to become far more powerful.\n",
    "\n",
    "## 1. Rated RNN Units\n",
    "A **rated recurrent unit** is simply a straightforward modification to the simple recurrent unit. Recall, a simple recurrent unit has the form:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1tGeRQ2PZsHbFq8XFsj32NC3gQ-46ilWZ\" width=\"320\">\n",
    "\n",
    "Which can be observed on a lower level as:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1F0wDzeyGBAa1l4_mDrIAiejherkAkb8p\" width=\"300\">\n",
    "\n",
    "And mathematically, we can define $h(t)$ as:\n",
    "\n",
    "$$h(t) = f\\big(W_x^T x(t) + W_h^T h(t-1)\\big)$$\n",
    "\n",
    "Where $f$ is our chosen activation function. The idea is that we we want to weight two things:\n",
    "\n",
    "1. $f\\big(x(t), h(t-1)\\big)$, which is the output that we would have gotten in a simple recurrent unit.\n",
    "2. $h(t-1)$, the previous value of the hidden state.\n",
    "\n",
    "In order to accomplish this weighting, we use a matrix known as the **rate matrix**, $z$, which is the same size as the hidden layer. We then perform an element by element multiplication on each dimension:\n",
    "\n",
    "$$h(t) = (1-z) \\odot h(t-1) + z \\odot f\\big( x(t), h(t-1)\\big)$$\n",
    "\n",
    "$z$ is known as the rate. You may notice that this looks very similar to the low pass filter that we created in the previous post. The idea here to is to learn $z$ in a way that it let's the some values from previous points in time carry greater weight, and others depend mainly on the most recent values. Visually, the rated recurrent unit looks like:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Tx7TAd3tUV-7-ezVPbJztx8dZyA3yC7m\" width=\"600\">\n",
    "\n",
    "The changes needed to implement this in code are relatively small and straightforward. We simply need to add a new param of size $M$, and include the above equation in the `recurrence` function.\n",
    "\n",
    "### 1.1.1 Rate Matrix Modifications\n",
    "There are more options for the rate matrix than simply what was discussed above. For example, we can make it dependent on the input and previous hidden state via a sigmoid and more weight matrices. \n",
    "\n",
    "We can also make it a matrix (size $M x M$) so that it multiplies against $h$ with matrix multiplication, instead of element wise multiplication. In this case, you can picture what is happening as the same type of everything connected to everything scenario that we had for the hidden to hidden state. \n",
    "\n",
    "### 1.1.2 Modifications to Poetry Generation\n",
    "The next thing that we are going to do is redo our poem generation example with the rated recurrent unit. It will be clear that this is just a small modification from the simple recurrent unit, so the code is almost exactly the same. Note, this new parameter will be trained in exactly the same way as all the other parameters-via gradient descent. In fact, when we look at more complex models the training will still remiain the same. \n",
    "\n",
    "To add a bit more complexity to the mix, we are going to try and solve the problem of the lines ending too quickly. Recall that this is because we have many training samples that result in the next word being the end token. Because the end token shows up in every line, it is over represented in the training data. To prevent this, we will only train for going to the end of the sequence 10% of the time. Otherwise we stop on the second to last word. \n",
    "\n",
    "Another change that we will make is that we will not model the initial distribution anymore. Recall, we use this to prevent every line from starting with the same word. This would happen because neural networks will give us the same prediction every time if we use the argmax. Instead, we will use the `START` token as the input, and the output will represent a probability distribution. This is valid because we already know that softmax gives us a valid probability distribution. We can then sample from this distribution to generate the next word. This way the sequences that we generate will be more stochastic, and it will treat the output probability as an actual probability, rather than a deterministic prediction:\n",
    "\n",
    "```\n",
    "p(w0) = softmax(f(START))\n",
    "w0 = randint(V, p=p(w0))\n",
    "```\n",
    "\n",
    "### 1.1.3 Rated RNN Unit in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:139: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and give of people a hornyhanded kindness as the arm ground snow snow cellar out \n",
      "and piano tell here him was stop in her me \n",
      "of then acquaintance out but a house them \n",
      "they be leafs a long it didnt held \n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from rnn_util import init_weight, get_robert_frost\n",
    "\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, D, M, V):\n",
    "        self.D = D # dimensionality of word embedding\n",
    "        self.M = M # hidden layer size\n",
    "        self.V = V # vocabulary size\n",
    "\n",
    "    def fit(self, X, learning_rate=10., mu=0.9, reg=0., activation=T.tanh, epochs=500, show_fig=False):\n",
    "        N = len(X)\n",
    "        D = self.D\n",
    "        M = self.M\n",
    "        V = self.V\n",
    "\n",
    "        # initial weights\n",
    "        We = init_weight(V, D)\n",
    "        Wx = init_weight(D, M)\n",
    "        Wh = init_weight(M, M)\n",
    "        bh = np.zeros(M)\n",
    "        h0 = np.zeros(M)\n",
    "        # z  = np.ones(M)\n",
    "        Wxz = init_weight(D, M)\n",
    "        Whz = init_weight(M, M)\n",
    "        bz  = np.zeros(M)\n",
    "        Wo = init_weight(M, V)\n",
    "        bo = np.zeros(V)\n",
    "\n",
    "        thX, thY, py_x, prediction = self.set(We, Wx, Wh, bh, h0, Wxz, Whz, bz, Wo, bo, activation)\n",
    "\n",
    "        lr = T.scalar('lr')\n",
    "\n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "\n",
    "        updates = []\n",
    "        for p, dp, g in zip(self.params, dparams, grads):\n",
    "            new_dp = mu*dp - lr*g\n",
    "            updates.append((dp, new_dp))\n",
    "\n",
    "            new_p = p + new_dp\n",
    "            updates.append((p, new_p))\n",
    "\n",
    "        self.predict_op = theano.function(inputs=[thX], outputs=prediction)\n",
    "        self.train_op = theano.function(\n",
    "            inputs=[thX, thY, lr],\n",
    "            outputs=[cost, prediction],\n",
    "            updates=updates\n",
    "        )\n",
    "\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            X = shuffle(X)\n",
    "            n_correct = 0\n",
    "            n_total = 0\n",
    "            cost = 0\n",
    "            for j in range(N):\n",
    "                if np.random.random() < 0.1:\n",
    "                    input_sequence = [0] + X[j]\n",
    "                    output_sequence = X[j] + [1]\n",
    "                else:\n",
    "                    input_sequence = [0] + X[j][:-1]\n",
    "                    output_sequence = X[j]\n",
    "                n_total += len(output_sequence)\n",
    "\n",
    "                # we set 0 to start and 1 to end\n",
    "                c, p = self.train_op(input_sequence, output_sequence, learning_rate)\n",
    "                # print \"p:\", p\n",
    "                cost += c\n",
    "                # print \"j:\", j, \"c:\", c/len(X[j]+1)\n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct += 1\n",
    "            if i % 50 == 0:\n",
    "                print(\"i:\", i, \"cost:\", cost, \"correct rate:\", (float(n_correct)/n_total))\n",
    "            if (i + 1) % 500 == 0:\n",
    "                learning_rate /= 2\n",
    "            costs.append(cost)\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, filename):\n",
    "        np.savez(filename, *[p.get_value() for p in self.params])\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename, activation):\n",
    "        # TODO: would prefer to save activation to file too\n",
    "        npz = np.load(filename)\n",
    "        We = npz['arr_0']\n",
    "        Wx = npz['arr_1']\n",
    "        Wh = npz['arr_2']\n",
    "        bh = npz['arr_3']\n",
    "        h0 = npz['arr_4']\n",
    "        Wxz = npz['arr_5']\n",
    "        Whz = npz['arr_6']\n",
    "        bz = npz['arr_7']\n",
    "        Wo = npz['arr_8']\n",
    "        bo = npz['arr_9']\n",
    "        V, D = We.shape\n",
    "        _, M = Wx.shape\n",
    "        rnn = SimpleRNN(D, M, V)\n",
    "        rnn.set(We, Wx, Wh, bh, h0, Wxz, Whz, bz, Wo, bo, activation)\n",
    "        return rnn\n",
    "\n",
    "    def set(self, We, Wx, Wh, bh, h0, Wxz, Whz, bz, Wo, bo, activation):\n",
    "        self.f = activation\n",
    "\n",
    "        # redundant - see how you can improve it\n",
    "        self.We = theano.shared(We)\n",
    "        self.Wx = theano.shared(Wx)\n",
    "        self.Wh = theano.shared(Wh)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        self.Wxz = theano.shared(Wxz)\n",
    "        self.Whz = theano.shared(Whz)\n",
    "        self.bz = theano.shared(bz)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.We, self.Wx, self.Wh, self.bh, self.h0, self.Wxz, self.Whz, self.bz, self.Wo, self.bo]\n",
    "\n",
    "        thX = T.ivector('X')\n",
    "        Ei = self.We[thX] # will be a TxD matrix\n",
    "        thY = T.ivector('Y')\n",
    "\n",
    "        def recurrence(x_t, h_t1):\n",
    "            # returns h(t), y(t)\n",
    "            hhat_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh)\n",
    "            z_t = T.nnet.sigmoid(x_t.dot(self.Wxz) + h_t1.dot(self.Whz) + self.bz)\n",
    "            h_t = (1 - z_t) * h_t1 + z_t * hhat_t\n",
    "            y_t = T.nnet.softmax(h_t.dot(self.Wo) + self.bo)\n",
    "            return h_t, y_t\n",
    "\n",
    "        [h, y], _ = theano.scan(\n",
    "            fn=recurrence,\n",
    "            outputs_info=[self.h0, None],\n",
    "            sequences=Ei,\n",
    "            n_steps=Ei.shape[0],\n",
    "        )\n",
    "\n",
    "        py_x = y[:, 0, :]\n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "        self.predict_op = theano.function(\n",
    "            inputs=[thX],\n",
    "            outputs=[py_x, prediction],\n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "        return thX, thY, py_x, prediction\n",
    "\n",
    "\n",
    "    def generate(self, word2idx):\n",
    "        # convert word2idx -> idx2word\n",
    "        idx2word = {v:k for k,v in word2idx.items()}\n",
    "        V = len(word2idx)\n",
    "        n_lines = 0\n",
    "        X = [ 0 ]\n",
    "        while n_lines < 4:\n",
    "            # print \"X:\", X\n",
    "            PY_X, _ = self.predict_op(X)\n",
    "            PY_X = PY_X[-1].flatten()\n",
    "            P = [ np.random.choice(V, p=PY_X)]\n",
    "            X = np.concatenate([X, P]) # append to the sequence\n",
    "            P = P[-1] # just grab the most recent prediction\n",
    "            if P > 1:\n",
    "                # it's a real word, not start/end token\n",
    "                word = idx2word[P]\n",
    "                print(word, end=\" \")\n",
    "            elif P == 1:\n",
    "                # end token\n",
    "                n_lines += 1\n",
    "                X = [0]\n",
    "                print('')\n",
    "\n",
    "\n",
    "def train_poetry():\n",
    "    sentences, word2idx = get_robert_frost()\n",
    "    rnn = SimpleRNN(50, 50, len(word2idx))\n",
    "    rnn.fit(sentences, learning_rate=1e-4, show_fig=True, activation=T.nnet.relu, epochs=2000)\n",
    "    rnn.save('RRNN_D50_M50_epochs2000_relu.npz')\n",
    "\n",
    "def generate_poetry():\n",
    "    sentences, word2idx = get_robert_frost()\n",
    "    rnn = SimpleRNN.load('RRNN_D50_M50_epochs2000_relu.npz', T.nnet.relu)\n",
    "    rnn.generate(word2idx)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     train_poetry()\n",
    "    generate_poetry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gated Recurrent Unit\n",
    "We are now going to introduce a unit that is more powerful than the ellman unit, the **Gated Recurrent Unit**, (GRU). GRU's were introduced in 2014, while LSTM's (which we will be going over next) were introduced in 1997. I have chosen to start with GRU's because they are slightly less complex, and thus a better place to begin. The GRU is very similar to LSTM, incorporating many of the same concepts, but having far fewer parameters, meaning it can train faster at a constant hidden layer size. \n",
    "\n",
    "### 2.1 GRU Architecture\n",
    "To start, let's go over the architecture of the GRU. Before we do that, however, I want to quickly recap the previous architectures that we have seen, particularly treating the hidden unit as a black box of sorts. This compartmental point of view will help us in extending the simple unit and rated unit to the GRU. \n",
    "\n",
    "**Feedforward Unit** <br>\n",
    "In the simplest feedforward network, this black box simply contains some nonlinear function like $tanh$ or $relu$:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=16Qe6uVFsX58tPqla11bUDtIuTyqlyyho\" width=\"300\">\n",
    "\n",
    "**Simple Recurrent Unit**<br>\n",
    "In a simple recurrent network, we just connect the output of the black box back to itself, with a time delay of one:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1lojDOZVNBmCE23wbEmNXgD7PfYWAo_eI\" width=\"300\">\n",
    "\n",
    "**Rated Recurrent Unit**<br>\n",
    "With the rated recurrent unit, we add a rating operation between what would have been the output of the simple recurrent network, and the previous output value. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1nLz2HGaYhP0JWnbm2FXIbpThFkb6toaM\" width=\"500\">\n",
    "\n",
    "We can think of of this new operation as a gate. Since it has to take on a value between 0 and 1, and the other gate has to take on the 1 minus that value, it is a gate that is choosing between two things: taking on the old value or taking on the new value. The result here is that we get a mixture of both.\n",
    "\n",
    "**Gated Recurrent Unit**<br>\n",
    "Finally, we arrive at the gated recurrent unit! The architecure simply requires that we add one more gate: \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1BqfOZ1LYDmOhXghEa4NJ7VIQbLbArzMY\" width=\"500\">\n",
    "\n",
    "The above diagram corresponds with the following mathematical equations:\n",
    "\n",
    "$$r_t = \\sigma \\big(x_t W_{xr} + h_{t-1}W_{hr} + b_r \\big)$$\n",
    "\n",
    "$$z_t = \\sigma \\big(x_t W_{xz} + h_{t-1}W_{hz} + b_z\\big)$$\n",
    "\n",
    "$$\\hat{h}_t = g \\big(x_t W_{xh} + (r_t \\odot h_{t-1})W_{hh} + b_h\\big)$$\n",
    "\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\hat{h}_t$$\n",
    "\n",
    "Note: $g$ represents an activation, and $\\odot$ is the symbol for element wise multiplication. With that said, we are simply seeing more of the same thing that we have been encountering thus far; weight matrices multiplied by inputs and passed through non linear functions and gates. \n",
    "\n",
    "Again, we see the update gate $z_t$ that we encountered with the rated unit. It still balances how much of the previous hidden value and how much of the new candidate hidden value combines to get the new hidden value. \n",
    "\n",
    "The new component is $r_t$, or the **reset gate**, which has the exact same functional form as the update gate- all of its weights are the same size. However, it is position in the black box is different. The reset gate is multiplied by the previous hidden state value. It controls how much of the previous hidden state we will consider, when we create the new candidate hidden value. In other words, it has the ability to reset the hidden value. \n",
    "\n",
    "For instance, consider the situation where $r_t$ is equal to 0, then we get:\n",
    "\n",
    "$$\\hat{h}_t = g \\big(x_t W_{xh} + b_h\\big)$$\n",
    "\n",
    "This would be as if $x_t$ were the beginning of a new sequence. Note that this is not the full picture, since $\\hat{h}$ is only a candidate for the new $h_t$, since $h_t$ will be a combination of $\\hat{h}$ and $h_{t-1}$ (which is controlled by the updated gate $z_t$). \n",
    "\n",
    "Now, if that all sounds a bit complex, there is a practical way to reason with it. At the most general level, we are simply adding more parameters to our model, and making it more expressive. Adding more parameters allows us to fit more complex patterns. \n",
    "\n",
    "### 2.2 GRU in Code\n",
    "When it comes to implement a GRU in code, it should be relatively simple if you already understand the simple recurrent unit and the rated recurrent unit. We simply need to add more weights, and modify the recurrence. The next step to making our code better is to modularize it. We have mentioned that the GRU can be looked at as a black box. To do this, we can simply make the GRU into a class so that it can be abstracted away. By doing this, we can stack GRU's, meaning anywhere that a hidden layer could go, a GRU can go as well. This allows us to just think of it as a thing that takes an input and produces an output. The fact that it contains a memory of previous inputs is just an internal detail of the black box. \n",
    "\n",
    "Some pseudocode is show below:\n",
    "\n",
    "```\n",
    "class GRU:\n",
    "    def __init__(Mi, Mo):\n",
    "        Wxr = random(Mi, Mo)\n",
    "        # ...\n",
    "    \n",
    "    def recurrence(x_t, h_t1):\n",
    "        r = sigmoid(x_t.dot(Wxr) + h_t1.dot(Whr) + br)\n",
    "        # ...\n",
    "        return (1 - z)*h_t1 + z*hhat\n",
    "    \n",
    "    def output(x):\n",
    "        return scan(recurrence, x)     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
