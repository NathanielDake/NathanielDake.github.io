{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. The Simple Recurrent Unit\n",
    "We are now going to talk about the _**simple recurrent unit**_, also known as the _Elman Unit_. But before we do that, I want to quickly touch on _sequences_. This is going to look slightly different from the data that we are used to.\n",
    "\n",
    "Recall that our input data $X$ is usually represented with an $NxD$ matrix ($N$ samples and $D$ features); there are no sequences here. Well, let's suppose that we did have a sequence of length $T$. How many dimensions would that require? \n",
    "\n",
    "Well, if the observation was a $D$ dimensional vector, and we have $T$ of them, then one sequence of observation will be a $TxD$ matrix. If we have $N$ training samples, then we will end up with an $NxTxD$ matrix, which is a 3 dimensional object. \n",
    "\n",
    "Sometimes our sequences are not of equal length, such as the cases with sentences, music, sound, or even someones credit history. How can we handle this? We have encountered this problem in the Hidden Markov Notebooks. The solution is to store each observation in a python list. So, instead of a 3 dimensional matrix, we will have a length $N$ list where each element is a 2-d observation sequence as a numpy array. Because a python list can contain any object as an element, this is okay. \n",
    "\n",
    "## 1.1 Simple Recurrent Unit \n",
    "Okay, now we can dig into our simple recurrent unit. Take a simple feedforward neural network with one hidden layer: \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ZTVGTPWR_s98SAt4b7IS97wMrn0DTiUV\" width=\"300\">\n",
    "\n",
    "The input layer and output layer will stay exactly the same, they are actually not part of the recurrent unit itself. However, they are included here for context. What we want to do is create a feedback connection from the hidden layer to itself:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1tGeRQ2PZsHbFq8XFsj32NC3gQ-46ilWZ\" width=\"300\">\n",
    "\n",
    "We can include the weights as well, first a regular feedforward net:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1fpOVWmY2Wos-JqWM7dmOx8SXRlMJhWQj\" width=\"300\">\n",
    "\n",
    "And a recurrent net:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=19HBt3NRTF9js20EJeYvekBI6QITmNGoO\" width=\"300\">\n",
    "\n",
    "Notice that the feedback loop implies that there is a delay of one time unit. So, one of the input units into $h(t)$ is $h(t-1)$. \n",
    "\n",
    "A question that you may have is: How big is $W_h$? Just like the other layers, we connect \"everything-to-everything\". So, if there are $M$ hidden units, the first hidden unit connects back to all $M$ units, the second hidden unit connects back to all $M$ units, and so on. In total there will be $M^2$ hidden to hidden weights. Hence, $W_h$ is an $MxM$ matrix. \n",
    "\n",
    "### 1.1.1 Simple Recurrent Mathematical Output\n",
    "Here is how we would represent the output of a recurrent net in math:\n",
    "\n",
    "$$h(t) = f \\big(W_h^T h(t-1) + W_x^T x(t) + b_h\\big)$$\n",
    "\n",
    "$$y(t) = softmax\\big(W_o^T h(t) + b_o\\big)$$\n",
    "\n",
    "Note that the feedback connection represents a time delay of 1, so the hidden layer takes in both $x$ and its last hidden value. Also, note that $f$ can be any of the usual nonlinearities, such as the sigmoid, tanh, or ReLu. \n",
    "\n",
    "## 1.2 Not The Markov Assumption\n",
    "One thing that is worth noting is that this is not the Markov Assumption. Why is that? Well, even though $h(t)$ is defined in terms of its previous value, it's previous value can be defined in terms of the value before that, and so on:\n",
    "\n",
    "$$h(t) = f \\big(W_h^T h(t-1) + W_x^T x(t) + b_h\\big)$$\n",
    "\n",
    "$$h(t) = f \\Big(W_h^T f \\big( W_h^T h(t-2) + W_x^T x(t-1) + b_h \\big) + W_x^T x(t) + b_h\\Big)$$\n",
    "\n",
    "This also means that $h(t)$ has to have an initial state, $h(0)$. Sometimes researchers will set this to 0, and other times it will be a hyperparameter that we can use back propagation on. Since theano automatically differentiates things for us, we will treat it as an updatable parameter. \n",
    "\n",
    "## 1.3 More Layers \n",
    "One question you may have is can we add more than one recurrent unit to the network:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1gUgdG48Pz3H1oEQIXz_huhugWMt858-W\" width=\"400\">\n",
    "\n",
    "The answer is yes! The number of recurrent layers is a hyperparameter, just like how the number of hidden layers is a parameter for a regular feed forward net. The question of how many depends on your type of specific problem. \n",
    "\n",
    "And that is all there is to it! Just by adding that one hidden layer, we have created a recurrent neural network! We will see in the coding networks how this can already do some amazing things, such as exponentially decrease the number of hidden units we would have needed in a feed forward neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 2. Prediction and Relationship to Markov Models\n",
    "We are now going to look more closely at what a recurrent neural network can predict, and talk about how under certain circumstances, we can relate it back to what we know about Markov Models. \n",
    "\n",
    "Adding a time component gives us a few more options in terms of the objective, or in other words _what we are trying to predict_. \n",
    "\n",
    "## 2.1 Softmax\n",
    "Let's being by talking about something we know already: Softmax and Multiclass classification. So, suppose we take an entire sequence, and take the argmax of the softmax, and use that as the class:\n",
    "\n",
    "$$Prediction = argmax_k \\big( p(y=k \\mid x)\\big) = argmax_k \\big( softmax(W_o^T h(t)) + b_o\\big)$$\n",
    "\n",
    "What assumption have we made? Well, first and foremost we forgot about the time component, but we will get to that shortly. The thing that we are assuming is that the entire sequence corresponds to one class. This makes perfect sense for a lot of problems. For example, if we wanted to classify between male and female voices, you would have sound samples from males and females. If you were looking at one sound file of a male saying \"Hello world\", the class for that sample is male; one sequence, one label. \n",
    "\n",
    "$$Prediction(t) = argmax_k \\big( p(y(t)=k \\mid x)\\big) = argmax_k \\big( softmax(W_o^T h(t)) + b_o\\big)$$\n",
    "\n",
    "But, we can't forget that for every $h(t)$ there is a $y(t)$. $y(t)$ is just the final layer calculated from $h(t)$. So, technically we could have a label for every time step.\n",
    "\n",
    "What are some situations where this may be useful? Well, let's think about brain computer interfaces. These are systems that are constantly reading electrical signals from your brain. Suppose the purpose of this brain computer interface is for controlling a wheelchair. Considering the wheelchair is how you will get around, you will need fine grain controll on the device. So, it moves forward, stops, turns left, etc, when you want it to. These would be our labels:\n",
    "\n",
    "$$Labels = \\{ forward, stop, left, right, back\\}$$ \n",
    "\n",
    "We want the action to be taken right as we think of it. In this case we would need a $y(t)$ for every $x(t)$. So this is precisely a situation where we would want to have not just one label for one sequence, but one label for every moment in time. \n",
    "\n",
    "Let's now consider a third situation. Suppose we are looking at a sequence of words, or in other words, sentences. As is typical, we want to predict the next word given all of the previous words (our target is the previous word). What is interesting about this is that it is essentially unsupervised learning, since there is essentially no target label-the target is just the input. In other words, we are trying to model the probability:\n",
    "\n",
    "$$p\\big( x(t) \\mid x(t-1), x(t-2),...,x(1)\\big)$$\n",
    "\n",
    "Remember that the current hidden state depends on all of the previous hidden states, so this is _not_ the markov assumption. Consider what happens when we try to make the whole input sequence the target sequence; we are trying to model the probability: \n",
    "\n",
    "$$p\\big(x(1)\\big)p\\big(x(2) \\mid x(1)\\big)p\\big(x(3) \\mid x(2), x(1)\\big)...$$\n",
    "\n",
    "Now, what happens if we join the first two terms together? Using bayes rule we get the joint probability of $p\\big( x(2), x(1)\\big)$:\n",
    "\n",
    "$$p\\big( x(2), x(1)\\big))p\\big(x(3) \\mid x(2), x(1)\\big)...$$\n",
    "\n",
    "Now, if we join the first two terms of that together, we get:\n",
    "\n",
    "$$p\\big( x(3), x(2), x(1)\\big)$$\n",
    "\n",
    "This is known as the chain rule of probability. Eventually what we end up with is just the joint probability of the entire sequence.\n",
    "\n",
    "## 2.2 Joint Probability\n",
    "That is very interesting, because it is exactly what we are trying to optimize with Hidden Markov Models! So, we can see that with Recurrent Neural Networks we can optimize the same thing as HMM's, but without making the markov assumption. This could lead us to the intuition that these RNN's may be more powerful. \n",
    "\n",
    "Another advantage is that, if you recall, HMM's suffer from the fact that longer sequences have probabilities that go down to zero, because you are continually multiplying values together that are less than 1. Luckily, we don't have that problem with RNN's, and the fact that we have the joint probability is implicit in the model itself, so there is no need to calculate it. \n",
    "\n",
    "## 2.3 Summary\n",
    "To conclude, we have identified 3 different ways of using RNN's for prediction. \n",
    "> 1. Predict one label over entire sequence (e.g. differentiate between male and female voices.\n",
    "2. Predict a label for every step of input sequence (e.g. control device using BCI)\n",
    "3. Predict the next value in a sequence (e.g. next word in sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Unfolding a Recurrent Neural Network\n",
    "Because time is a somewhat invisible notion in the recurrent neural network architecture, this section is about visualizing the RNN in terms of a feedforward neural network. One key idea that returns here is the idea of shared weights. \n",
    "\n",
    "Imagine we have a sequence of length 5, if we were to unfold the RNN in time, such that it has no recurrent connections at all, we would get this feed forward neural network with 5 hidden layers. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1aPEj0Q7T42nbs0Fqy4wtoJKdx5ec31sh\" width=\"700\">\n",
    "\n",
    "It is as if $h(0)$ is the input, and each $x(t)$ is just some additional control signal at each step. We can see that the hidden to hidden weight, $w_h$, is just repeated at every layer. So, it is like a deep network with the same shared weight between each layer. Similarly, $w_x$ is shared between each of the five $x$'s going into the hidden layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Backpropagation Through Time \n",
    "We are now going to return through backpropagation as a mental exercise. Note, in code we are going to use Theano and Tensorflow to calculate the gradients for us. This would be useful if you were to write a Recurrent Neural Network from scratch. \n",
    "\n",
    "If you recall the notebooks on vanilla deep learning, you may remember that **backpropagation** is just a fancy name for gradient descent. It has some interesting properties, but the entire idea lies behind _**calculating the gradient**_ and _**moving in that direction**_. \n",
    "\n",
    "Similary, _**Backpropagation through time**_, often shortened to **BPTT**, is just a fancy name for backpropagation, which itself is just gradient descent. So what does that mean for us? That means that in the code, updating all of the weights is going to exactly the same, which makes things very easy for us:\n",
    "\n",
    "```\n",
    "W = W - \\alpha*T.grad(cost, W)\n",
    "```\n",
    "\n",
    "## 4.1 Recurisive Nature \n",
    "However, we are still interested in taking the gradient. First, let's consider what our neural network outputs above would be calculated as (use the diagram for reference):\n",
    "\n",
    "**Solve $y(1)$**\n",
    "$$y(1) = softmax\\big(W_o^Th(1)\\big)$$\n",
    "\n",
    "**Solve $y(2)$**\n",
    "$$y(2) = softmax\\big(W_o^Th(2)\\big)$$\n",
    "\n",
    "$$h(2) = f\\big(W_x^T x(2) + W_h^T h(1)\\big)$$\n",
    "\n",
    "$$y(2) = softmax\\big(W_o^T f\\big(W_x^T x(2) + W_h^T h(1)\\big) \\big)$$\n",
    "\n",
    "**Solve $y(3)$**\n",
    "$$y(3) = softmax\\big(W_o^Th(3)\\big)$$\n",
    "\n",
    "$$h(3) = f\\big(W_x^T x(3) + W_h^T h(2)\\big)$$\n",
    "\n",
    "$$h(3) = f\\Big(W_x^T x(3) + W_h^T f\\big(W_x^T x(2) + W_h^T h(1)\\big)\\Big)$$\n",
    "\n",
    "$$y(3) = softmax\\Big\\{W_o^T f\\Big(W_x^T x(3) + W_h^T f\\big(W_x^T x(2) + W_h^T h(1)\\big)\\Big)\\Big\\}$$\n",
    "\n",
    "**General Pattern**<br>\n",
    "We could continue this for $y(4)$ and $y(5)$, but instead let us write out the general pattern:\n",
    "\n",
    "$$y(t) = softmax \\big(W_o^T h(t) \\big)$$\n",
    "\n",
    "$$y(t) = softmax \\Big(W_o^T f\\big(W_h^T h(t-1) + W_x^T x(t) \\big) \\Big)$$\n",
    "\n",
    "$$y(t) = softmax\\Big\\{W_o^T f\\Big(W_x^T x(t) + W_h^T f\\big(W_x^T x(t-1) + W_h^T h(t-2)\\big)\\Big)\\Big\\}$$\n",
    "\n",
    "## 4.2 How Does Error Change With Respect to our Weights?\n",
    "The pattern of our output can clearly be seen above. \n",
    "\n",
    "#### 4.2.1 Derivative of Error With Respect to $W_o$\n",
    "Note, our output weight, $W_o$, occurs _after_ the recurrence, so we do not need to consider time in that case. Hence, when trying to determine how our error, $J$, changes with respect to the output weight, we can calculate the derivative as normal (for more detail please see my post on Neural Network Training):\n",
    "\n",
    "$$\\frac{dJ}{dW_o} = \\frac{\\partial J}{\\partial y}\\frac{\\partial y}{\\partial a}\\frac{\\partial a}{\\partial W_o}$$\n",
    "\n",
    "#### 4.2.1 Derivative of Error With Respect to $W_h$\n",
    "However, we have to be more careful when determining how the error changes with respect to $W_h$. If we look at the derivative of $J$ with respect to $W_h$:\n",
    "\n",
    "$$\\frac{dJ}{dW_h} = \\frac{\\partial J}{\\partial a}\\frac{\\partial a}{\\partial h}\\frac{\\partial h}{\\partial W_h}$$\n",
    "\n",
    "In order to best demonstrate this and make the idea more concrete, let's look specifically node 3 and how the error, $J_3$, changes as we change $W_h$:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1y-pmfwOHpW2iHiF_v1VbmCU-cpstMTLT\" width=\"650\">\n",
    "\n",
    "$$\\frac{dJ_3}{dW_h} = \\frac{\\partial J_3}{\\partial a_3}\\frac{\\partial a_3}{\\partial h_3}\\frac{\\partial h_3}{\\partial W_h}$$\n",
    "\n",
    "The first term of that derivative is no problem, it is simply:\n",
    "\n",
    "$$\\frac{\\partial J_3}{\\partial a_3} = \\frac{\\partial J_3}{\\partial y_3}\\frac{\\partial y_3}{\\partial a_3}$$\n",
    "\n",
    "And the second is also taken care of easily:\n",
    "\n",
    "$$a_3 = W_o^Th_3$$\n",
    "\n",
    "$$\\frac{\\partial a_3}{\\partial h_3} = W_o^T$$\n",
    "\n",
    "The third term in that derivative, $\\frac{\\partial h_3}{\\partial W_h}$, is where we run into problems. Recall, that $h_3$ was determined to be:\n",
    "\n",
    "$$h(3) = f\\big(W_x^T x(3) + W_h^T h(2)\\big)$$\n",
    "\n",
    "And hence we can rewrite our derivative as: \n",
    "\n",
    "$$\\frac{\\partial h_3}{\\partial W_h} = \\frac{\\partial f\\big(W_x^T x(3) + W_h^T h(2)\\big)}{\\partial W_h}$$\n",
    "\n",
    "To make our example more concrete, we will let our activation function, $f$, be the $tanh$ function:\n",
    "\n",
    "$$\\frac{\\partial h_3}{\\partial W_h} = \\frac{\\partial \\Big \\{ tanh\\big(W_x^T x(3) + W_h^T h(2)\\big) \\Big \\}}{\\partial W_h}$$\n",
    "\n",
    "Now, we can perform the chain rule and up with the following:\n",
    "\n",
    "$$\\frac{\\partial h_3}{\\partial W_h} = \\overbrace{sech^2\\big(W_x^T x(3) + W_h^T h(2)\\big)}^\\text{Outer } * \\overbrace{\\frac{ \\partial \\big(W_x^T x(3) + W_h^T h(2) \\big)}{\\partial W_h}}^\\text{Inner}$$\n",
    "\n",
    "The inner derivative is where we start to run into issues. We can drop $W_x^T x(3)$, considering it does not depend on $W_h$. However, we are still left with a product of _two_ terms which are a function of $W_h$, $W_h^T*h(2)$. This means that we need to use the _product rule_ in order to solve the derivative. If we let $a$ and $b$ equal (dropping the transpose on our matrices to increase readability):\n",
    "\n",
    "$$a = h(2)$$\n",
    "\n",
    "$$b = W_h$$\n",
    "\n",
    "Then, be the product rule, the derivative of $(a*b)'$ is:\n",
    "\n",
    "$$(a*b)' = a'*b + b'*a$$\n",
    "\n",
    "Recall, $h(2)$ was defined as:\n",
    "\n",
    "$$h(2) = tanh\\big(W_x x(2) + W_h h(1)\\big)$$\n",
    "\n",
    "So, $a$ can be written as:\n",
    "\n",
    "$$a = tanh\\big(W_x x(2) + W_h h(1)\\big)$$\n",
    "\n",
    "$$(a*b)' = \\overbrace{ \\frac{\\partial \\Big \\{ tanh \\big(W_x x(2) + W_h h(1)\\big) \\Big \\} }{\\partial W_h}}^\\text{a'} * \\overbrace{W_h}^\\text{b} + \\overbrace{1}^\\text{b'} *\\overbrace { tanh\\big(W_x x(2) + W_h h(1)\\big)}^\\text{a}$$\n",
    "\n",
    "But wait! We once again run into an issue where in order to find $a'$ we need to use the chain rule:\n",
    "\n",
    "$$a' = \\frac{\\partial \\Big \\{ tanh \\big(W_x x(2) + W_h h(1)\\big) \\Big \\} }{\\partial W_h}$$\n",
    "\n",
    "So, we can go ahead and work that out:\n",
    "\n",
    "$$a' = \\frac{\\partial h_2}{\\partial W_h} = \\overbrace{sech^2\\big(W_x x(2) + W_h h(1)\\big)}^\\text{Outer } * \\overbrace{\\frac{\\partial\\big( W_x x(2) + W_h h(1) \\big)}{\\partial W_h}}^\\text{Inner}$$\n",
    "\n",
    "And again, we run into problems with the inner derivative! We can drop the first term since it does not depend on $W_h$, but that still leaves us with:\n",
    "\n",
    "$$\\frac{\\partial\\big( W_h h(1) \\big)}{\\partial W_h}$$\n",
    "\n",
    "Clearly, we need to use the product rule since we have two terms being multiplied which both depend on the variable we are taking the derivative with respect to ($W_h$). Let us again use $a$ and $b$ to represent our terms:\n",
    "\n",
    "$$a = h(1)$$\n",
    "\n",
    "$$a = tanh\\big(W_x x(1) + W_h h(0)\\big)$$\n",
    "\n",
    "$$b = W_h$$\n",
    "\n",
    "And utilize the product rule:\n",
    "\n",
    "$$(a*b)' = \\overbrace{ \\frac{\\partial \\Big \\{ tanh \\big(W_x x(1) + W_h h(0)\\big) \\Big \\} }{\\partial W_h}}^\\text{a'} * \\overbrace{W_h}^\\text{b} + \\overbrace{1}^\\text{b'} *\\overbrace { tanh\\big(W_x x(1) + W_h h(0)\\big)}^\\text{a}$$\n",
    "\n",
    "The trend should be very clear at this point: \n",
    "\n",
    "> We are continually going to be alternating back and forth, recursively, between the chain rule and product rule until we hit our base case/starting point. \n",
    "\n",
    "We can see that $a'$ will require the chain rule:\n",
    "\n",
    "$$a' = \\frac{\\partial h_1}{\\partial W_h} = \\overbrace{sech^2\\big(W_x x(1) + W_h h(0)\\big)}^\\text{Outer } * \\overbrace{\\frac{\\partial\\big( W_x x(1) + W_h h(0) \\big)}{\\partial W_h}}^\\text{Inner}$$\n",
    "\n",
    "However, we have finally reached a base case and will _not_ require another product rule! The inner derivative, which is:\n",
    "\n",
    "$$\\frac{\\partial\\big( W_x x(1) + W_h h(0) \\big)}{\\partial W_h}$$\n",
    "\n",
    "Can have the first term dropped, and we can substitute in $h(0)$:\n",
    "\n",
    "$$\\frac{\\partial\\Big( W_h * tanh\\big(W_x x(0)\\big)\\Big)}{\\partial W_h}$$\n",
    "\n",
    "We see that only the first term in this product depends on $W_h$, and hence our derivative is just:\n",
    "\n",
    "$$\\frac{\\partial\\Big( W_h * tanh\\big(W_x x(0)\\big)\\Big)}{\\partial W_h} = 1 * tanh\\big(W_x x(0)\\big) $$\n",
    "\n",
    "The main takeaway from this process should be the intuition that because of the recurrent net is defined, there will be a recursive process of the chain rule leading the product rule which leads back to the chain rule, and so on, down to our base case. This same process will occur when dealing with $\\frac{\\partial J}{\\partial W_x}$, however, we need not go through all of the steps for sake of redundancy. Keep this intuition in mind as we discuss the _parity problem_ and the _vanishing gradient_ in the next sections.\n",
    "\n",
    "One final thing to keep in mind, recall from earlier posts regarding deep learning that a weight gets updated via an _error signal_ from whatever nodes it influences when it goes in the forward direction. Below, we can see the influence of $w_h$ in red:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=15Tokk-VRfYQ9JzCPZFYI43x7GlyQs7rb\" width=\"700\">\n",
    "\n",
    "The upward arrows going to the output $y$ only matter if you are considering that as a part of your output. This will be different depending on whether you have a target for every time step, or a target just at the end of a sequence. The arrows going from left to right must always be backpropagated, because these are the hidden to hidden weights. \n",
    "\n",
    "## 4.2 Vanishing Gradient / Exploding Gradient\n",
    "One thing to realize here is that the same things are going to be multiplied again and again due to the chain rule of calculus. This will happen for both the hidden to hidden weights, as well as the input to hidden weights. The result is that you will either have something that goes down to 0, or gets large very quickly. This is referred to as the _vanish gradient_ and _exploding gradient_ problem respectively. \n",
    "\n",
    "One solution to this is **gradient clipping**. It is a pretty simple algorithm, as can be seen from the pseudocode below:\n",
    "\n",
    "```\n",
    "g = derivative_error_wrt_weights\n",
    "if ||g|| >= threshold:\n",
    "    g = g * (threshold / ||g||)\n",
    "```\n",
    "\n",
    "## 4.3 Truncated BPTT\n",
    "Another modification of back propagation through time is **truncated backpropagation through time**. Because the derivatives of $W_h$ and $W_x$ depend on every single time step of the sequence so far, the calculation will take a very long time for very long sequences. One approximation is just to stop after a certain number of time steps. One disadvantage of this is that it won't incorporate the error at longer periods of time. However, if you don't care about dependencies at greater than 3 time steps, then you can just truncate at 3 time steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 5. The Parity Problem \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
