{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network Training\n",
    "We previously went over the architecture of a neural network, mainly talking about how to pass data from input to output, and end up with a probabilistic prediction. However, the problem was that all of our weights were random and thus, our predictions were not accurate!\n",
    "\n",
    "<img src=\"images/main-nn-equation.png\">\n",
    "\n",
    "In this section we are going to focus on how to change our weights so that they are accurate. Recall, our training data will consist of input and target pairs, with the goal being to make the predictions, $y$, as close to the targets $t$ as possible. To do this we will create a **cost function** such that:\n",
    "* The **cost is large** when the prediction is not close to the target\n",
    "* The **cost is small** when the prediction is close to the target\n",
    "In the end we are trying to make our cost as small as possible! Remember that in the past the method that we used to achieve this is called gradient descent. This is a bit harder with neural networks, because the equations are a bit more complex, but no new skills are required.\n",
    "\n",
    "## Gradient Descent in NNs: Backpropagation\n",
    "Gradient descent in neural networks has a special name: **backpropagation**. Backpropagation is recursive in nature and allows us to find the gradient in a systematic way. This recursiveness allows us to do backpropagation for neural networks that are arbitrarily deep, without much added complexity. For instance, a 1-hidden layer ANN is no harder than a 100 layer ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What do all of these symbols and letter mean?\n",
    "## Training Data\n",
    "* Training inputs: X\n",
    "* Training targets: Y\n",
    "* Generally speaking, these are both matrices\n",
    "* X is of shape N x D\n",
    "    * N = number of samples\n",
    "    * D = number of input features\n",
    "* Y is of shape N x 1 \n",
    "    * aka a column vector\n",
    "    * a 2-d object \n",
    "* Alternatively, Y can just be a vector of only 1-D of length N\n",
    "    * this is how we will represent it in Numpy\n",
    "    \n",
    "## Training Data and Predictions\n",
    "* Inputs: X, Targets: Y, Predictions: p(Y|X)\n",
    "* p(Y|X) represents a full probability distribution over all individual values in the matrix Y, given the matrix X\n",
    "* p(Y|X) is therefore also a matrix, same size as Y\n",
    "* p(y = k | x) is a probability value - a single number \n",
    "    * Representing the probability that y is of class k, given the input vector x \n",
    "* Note: \n",
    "    * Capital letters usually represent matrices\n",
    "    * lowercase letters usually represent vectors\n",
    "\n",
    "## p(Y | X) is inconvenient\n",
    "* p(Y | X) is inconvenient to write \n",
    "* many characters\n",
    "* variable names cannot contain spaces or parentheses\n",
    "* so we resort to things like:\n",
    "    * p_y_given_x\n",
    "    * py_x\n",
    "    * Py\n",
    "* none are really ideal\n",
    "* Old school alternative for predictions is to write:\n",
    "## $$\\hat{y}$$\n",
    "* notice that this still cannot represent a variable in code...\n",
    "\n",
    "## Another convention\n",
    "* This is were things start to get confusing\n",
    "* an alternative way to represent inputs, targets, and predictions is:\n",
    "    * Inputs: X\n",
    "    * Targets: T\n",
    "    * Predictions: Y\n",
    "* this is beneficial since we no longer need to write out P of Y given X anywhere\n",
    "* however this Y now conflicts with our other Y- before Y meant the targets, now it means the predictions\n",
    "\n",
    "## Using Context\n",
    "* we will need to use context to determine what Y really is \n",
    "* if we see Y and T at the same time, it should be clear that Y is a prediction\n",
    "* if you see Y being assigned the output a neural network, its a prediction\n",
    "* but if you see Y and Yhat, Y and p_y_given_x at the same time, Y is a target \n",
    "\n",
    "## Weights\n",
    "\n",
    "<img src=\"images/weights-diagram.png\">\n",
    "\n",
    "* we have some conventions for the sizes of things\n",
    "* N is the number of samples we have collected in our experiment\n",
    "* D is then number of features, which is the size of the input layer in the neural network\n",
    "* M represents the size of the hidden layer\n",
    "* K represents the size of the output layer \n",
    "* K is the number of output classes, and can be anything from 2 and larger \n",
    "* when we have a 1 hidden layer neural network, one way to name the weights is as follows:\n",
    "    * W is the weight matrix from the input to hidden layer - (D x M)\n",
    "    * b is the bias term at hidden layer - (M x 1)\n",
    "    * V is the weight matrix from the hidden to output layer - (M x K)\n",
    "    * c is the bias term at the output layer - (K x 1)\n",
    "* you can imagine that if we start adding more hidden layers, we are going to run out of letters! So using V and c isn't really a great option.\n",
    "* How about just numbering our W's and b's?\n",
    "    * W1, b1, W2, b2, ... and so on\n",
    "\n",
    "## Indices\n",
    "* we may or may not put indices in different places if they represent different things\n",
    "* Example: if we are looking for the target T for the nth sample and kth class\n",
    "* In Numpy: T[n,k]\n",
    "* T(n,k)\n",
    "* $T^n_k$\n",
    "* $T_{nk}$\n",
    "* $t^n_k$\n",
    "* $t_{nk}$\n",
    "\n",
    "## Indexing\n",
    "* i, j, and k are common letters we use for indices\n",
    "* Ex. i=1...D (input layer)\n",
    "* Ex. j=1...M (hidden layer)\n",
    "* Ex. k=1...K (output layer)\n",
    "* the problem with i, j, and k is what happens if we have more than 1 hidden layer, and eventually run out of letters\n",
    "* we then pick and index outside of these 3 current letters:\n",
    "    * q = 1...Q\n",
    "    * r = 1...R\n",
    "    * s = 1...S\n",
    "\n",
    "## Learning Rate\n",
    "* greek letters: alpha or eta\n",
    "\n",
    "<img src=\"images/learning-rate.png\">\n",
    "\n",
    "## Cost/Objective/Error Function\n",
    "* Typical letters: E or J\n",
    "* Cost or error: usually means something we want to minimize\n",
    "* Objective: can be something to minimize or maximize \n",
    "* Probabilistic interpretation of cost: negative log-likelihood\n",
    "* we are trying to maximize the log likelihood, or minimizing the negative log likelihood\n",
    "* minimizing E is the same as maximizing -E\n",
    "* So if you are minimizing the negative log-likelihood (gradient **descent**) is the same as maximizing the log-likelihood (and likelihood) (gradient **ascent**)\n",
    "\n",
    "## Likelihood\n",
    "* Typically we use the uppercase L for likelihood, lowercase l for log-likelihood, if they are both presented together\n",
    "* If discussing log-likelihood or negative log-likelihood by itself, we might just use L since L is easy to see, and l can be confused for I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What does it mean to train a Neural Network? \n",
    "* this is going to be very similar to logistic regression\n",
    "\n",
    "## The Main Concepts\n",
    "* we very intuitively define something called the \"cost\"\n",
    "* we want to minimize the cost!  \n",
    "* But how do we minimize the cost? This falls into the domain of calculus! Calculus provides the tools to find the min/max of a function!\n",
    "* we specifically use a method called **gradient descent**\n",
    "\n",
    "## How do we define cost? \n",
    "* recall that for binary classification, this is exactly how we would calculate the likelihood of a sequence of coin tosses\n",
    "* So for example, say we flip 2 heads and 3 tails\n",
    "* Because these are independent trials, the total likelihood is then:\n",
    "\n",
    "$$Likelihood = p(H)p(H)p(T)p(T)p(T)$$\n",
    "\n",
    "* again, the reason we can multiply these probabilities is because each coin toss is independent of the others\n",
    "* another way to write this is to call:\n",
    "\n",
    "$$p = p(H)$$\n",
    "* and hence we can rewrite likelihood as:\n",
    "\n",
    "$$Likelihood = p^{number \\; of\\; heads}(1-p)^{number \\;of \\; tails}$$\n",
    "\n",
    "## Minimize or Maximize?\n",
    "* the likelihood, or in other words the probability of our model, aka the probability of our data, given our model/parameters, is something we want to maximize\n",
    "* but recall that we are looking for a cost, in other words, something to minimize\n",
    "* In order to get something that we can call the cost, we take the negative log of the likelihood and call it the \"cost\"\n",
    "* Negative log likelihood = -{#H logp + #T log(1-p)}\n",
    "* recall from logistic regression that this is called the cross entropy cost function\n",
    "\n",
    "## Cross-Entropy\n",
    "* we can phrase it in terms of the output probability of the logistic regression model, and the targets\n",
    "* $y_n$ = output of logistic regression or neural network\n",
    "* $t_n$ = actual target (0 or 1) in the binary case \n",
    "\n",
    "<img src=\"images/cross-entropy-cost.png\">\n",
    "\n",
    "* notice that if we had a neural network doing binary classification, we would use this exact same cost function\n",
    "* recall that in order to find the best weights to minimize this cost, we can use gradient descent \n",
    "* we can also maximize the negative of this, gradient ascent\n",
    "\n",
    "## Cross Entropy for Multi-class Classification\n",
    "* in this section we want to be able to handle any number of outputs \n",
    "* lets consider a die roll (6 faces, but lets call it K)\n",
    "* the probability of rolling k = $y_k$\n",
    "* $t_k$ = 1 if we roll k, 0 if we do not roll k\n",
    "* we have N total die rolls, so $t_{n,k} =1$ if we rolled k on the nth die roll \n",
    "* therefore, only one of the $t_{n,k}$ can equal 1 for any particular n\n",
    "    * $t_{n,k}$ is thus an indicator matrix or one hot encoded matrix of 1s and 0s\n",
    "    \n",
    "<img src=\"images/multi-class-likelihood.png\">\n",
    "\n",
    "* notice that y and k now have two indexes each:\n",
    "    * n corresponds to which sample we are looking at \n",
    "    * and k corresponds to which class we are looking at \n",
    "* notice that for any particular n, only 1 of the k targets can be 1, and the rest must be 0\n",
    "* that is because if we roll a die and get a 6, then that same die roll can't be any other number \n",
    "\n",
    "## Cross Entropy for Multi-class Classification\n",
    "\n",
    "<img src=\"images/cross-entropy-for-multiclass-classification.png\">\n",
    "\n",
    "* What is cross entropy: https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/\n",
    "* we again want to transform this into a cost, so we will take the negative log of the likelihood\n",
    "* this is called the cross entropy cost function, but for multiclass classification\n",
    "* next we will see how to perform gradient descent on the new cost function and how to write it in code! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Backpropagation Intro \n",
    "* Lets start by recalling how we trained the logistic regression model\n",
    "\n",
    "## Logistic Regression Recap\n",
    "* we start with an objective function, and for binary classification we use cross entropy \n",
    "\n",
    "<img src=\"images/cross-entropy-cost.png\">\n",
    "\n",
    "* we know that this objective function has a global minimum, so it looks something like a parabola \n",
    "* now we usually will randomize our weights initially, and then we slowly move towards the minimum in small steps\n",
    "* we find the direction for the minimum using the gradient, $\\frac{dJ}{dw}$\n",
    "\n",
    "<img src=\"images/cost-minimization.png\">\n",
    "\n",
    "* we end up with an update rule:\n",
    "\n",
    "$$w += w - \\alpha\\frac{dJ}{dw}$$\n",
    "\n",
    "* where $\\alpha$ is the learning rate \n",
    "* this process is called gradient descent\n",
    "* You can also do gradient ascent, where the goal is to find a global maximum\n",
    "\n",
    "$$w += w + \\alpha\\frac{dJ}{dw}$$\n",
    "\n",
    "## Neural Networks Gradient Descent\n",
    "* we are going to do the exact same process with neural networks! However, because they are nonlinear we are going to find a local minima, not global minimum\n",
    "* additionally, because the weight updates are dependent on the error at multiple outputs, we are going to need the concepts of total derivatives\n",
    "\n",
    "### Total Derivatives\n",
    "* so if you have a function of x and y, $f(x,y)$, where x is a function of t, $x(t)$, and y is a function of t, $y(t)$, hence it is a parameterized function\n",
    "* The goal is to take the **total derivative**\n",
    "\n",
    "$$\\frac{df}{dt}$$\n",
    "\n",
    "* To do this we use the **chain rule**\n",
    "\n",
    "$$\\frac{df}{dt}= \\frac{df}{dx}\\frac{dx}{dt} +\\frac{df}{dy}\\frac{dy}{dt}$$\n",
    "\n",
    "* now if you had a vector x, which has k components, and they are all parameterized by t, you can imagine that you would do the same thing, using a summation\n",
    "\n",
    "$$\\frac{df}{dt} = \\sum_k\\frac{df}{dx_k}\\frac{dx_k}{dt}$$\n",
    "\n",
    "## Objective function with Softmax\n",
    "* basically this is the same as the likelihood of rolling a die\n",
    "* so if you were to roll a die, you would get your likelihood to be, with n independent and identically distributed tosses:\n",
    "\n",
    "$$Likelihood = \\prod_{n=1}^N\\prod_{k=1}^6 (\\theta_k^n)^{t_k^n}$$\n",
    "\n",
    "* so with neural networks this is exactly the same!\n",
    "\n",
    "$$P(targets\\;|\\;inputs, weights) = P(targets\\;|\\;X,W,V) = \\prod_{n=1}^N\\prod_{k=1}^K (\\theta_k^n)^{t_k^n}$$\n",
    "\n",
    "* so we are going to work with the log likelihood, not the negative log liklelihood, and do gradient ascent instead of descent\n",
    "* so lets take the log likelihood\n",
    "\n",
    "$$\\sum_n\\sum_kt_k^nlogy_k^n$$\n",
    "\n",
    "* now that we have our objective function, what do we do with it?\n",
    "* it is the same idea as with logistic regression! We want to find the derivative with respect to certain weights. \n",
    "* since we have 2 sets of weights for a 1 hidden layer NN (W and V), the dimensions of each node are D, M, and K, and they are indexed by d, m, k\n",
    "\n",
    "<img src=\"images/1-hidden-layer-diagram.png\">\n",
    "\n",
    "* so we want to find these derivatives: \n",
    "\n",
    "$$\\frac{dJ}{dV_{mk}}$$\n",
    "$$\\frac{dJ}{dW_{dm}}$$\n",
    "\n",
    "* Note that in these derivatives, J can be thought of as our **error**\n",
    "* we are trying to find how the error (cost, J) changes as we change our weights!\n",
    "* because we are doing backpropagation, we are going to find $\\frac{dJ}{dV_{mk}}$ first, because it is on the right, followed by backpropagating the error, and then we will find $\\frac{dJ}{dW_{dm}}$\n",
    "* this can be done using the chain rule \n",
    "\n",
    "$$\\frac{dJ}{dV_{mk}}= \\sum_n\\sum_{k'}t_{k'}^n\\frac{1}{y_{k'}^n}\\frac{dy_{k'}^n}{dV_{mk}}$$\n",
    "\n",
    "* now the question is, how do we find: $\\frac{dy_{k'}^n}{dV_{mk}}$?\n",
    "* in other words...\n",
    "\n",
    "### How do we find the derivative of softmax?\n",
    "$$y_k=\\frac{e^{a_k}}{\\sum_je^{a_j}}$$\n",
    "* where the activation, $a_k$ is just the dot product of the input times the weights\n",
    "\n",
    "$$a_k= V_k^TZ$$\n",
    "\n",
    "* so we want to find just the derivative of the softmax first\n",
    "    * if k == k'\n",
    "\n",
    "$$\\frac{dy_{k'}}{da_k} = y_{k'}(1-y_k)$$  \n",
    "\n",
    "    * if k != k'\n",
    "$$\\frac{dy_{k'}}{da_k} = -y_{k'}y_k$$  \n",
    "\n",
    "* these can be combined using the kronecker delta\n",
    "    * if i == j\n",
    "    \n",
    "$$\\delta_{ij} = 1$$    \n",
    "\n",
    "    * if i != j\n",
    "\n",
    "$$\\delta_{ij} = 0$$        \n",
    "\n",
    "* so the derivative is...\n",
    "\n",
    "$$\\frac{dy_{k'}}{da_k} = y_{k'}(\\delta_{kk'}-y_k)$$  \n",
    "\n",
    "* we also know from the dot product that the derivative of the activation is just zm\n",
    "\n",
    "$$\\frac{da_k}{dV_{mk}}=z_m$$\n",
    "\n",
    "## Combine\n",
    "\n",
    "$$\\frac{dJ}{dV_{mk}}= \\sum_n(t_k^n-y_k^n)z_m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
