{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Embeddings\n",
    "This section is intended to serve as an introduction to many of the things that we will touch on in my NLP section pertaining to Deep learning applied to natural language processing.\n",
    "\n",
    "You will notice that a lot of the RNN examples that we go over, as well as elsewhere on the web will use word sequences as examples. Why is that?\n",
    "\n",
    "1. Language is an easy topic to comprehend! We speak, read, and write every single day, which makes it rather intuitive to deal with. If you are reading this post, then you are unavoidably using those abilities at this very moment.\n",
    "2. RNN's allow for us to no longer treat sentences as **bag-of-words**.\n",
    "\n",
    "Let's focus on number 2 above for a moment. As an example, consider the sentence:\n",
    "\n",
    "> \"Dogs love cats and I\"\n",
    "\n",
    "It _almost_ has the correct gramatical structure, but its meaning is most certainly different from the original sentence:\n",
    "\n",
    "> \"I love dogs and cats\"\n",
    "\n",
    "So, there is a lot of information (in the quantitative sense) that is thrown away when you use bag-of-words. At this point I am assuming that you have gone through my posts concerning Logistic Regression and intro to NLP, which both go over sentiment analysis and utilize bag-of-words. But in case you have not, let me define bag of words quickly.\n",
    "\n",
    "## 1. Bag-of-Words\n",
    "Consider a the task of sentiment analysis, where we are trying to determine whether a sentence is positive or negative. A positive sentence may be:\n",
    "\n",
    "> \"Wow, today is a great day!\"\n",
    "\n",
    "While a negative sentence may be:\n",
    "\n",
    "> \"Ugh, this movie is absolutely terrible.\"\n",
    "\n",
    "In order to turn each sentence into an input for the classifier, we first start with a vector of 0's of size $V$ (our vocabularly size), so there is an entry for every individual word:\n",
    "\n",
    "```\n",
    "X=[0,0,0,...,0]\n",
    "len(X) = V\n",
    "```\n",
    "\n",
    "We keep track of which word goes with which index using a dictionary, `word2idx`. Now, for every word in the sentence, we will set the corresponding index in the vector to `1`, or perhaps some other frequency measure:\n",
    "\n",
    "```\n",
    "X[idx_of_word] = 1\n",
    "```\n",
    "\n",
    "So, there is a nonzero value for every word that appears in the sentence, and everywhere else zero:\n",
    "\n",
    "```\n",
    "X = [0,1,0,0,...,1]\n",
    "```\n",
    "\n",
    "You can see how given this vector, it wouldn't be easy to determine the correct order of words in the sentence. It isn't completely impossible, if for instance the words are such that their is only one possible ordering, but generally some information is lost. \n",
    "\n",
    "Now, what happens when you have the two similar sentences:\n",
    "\n",
    "> \"Today is a good day.\"\n",
    "\n",
    "And:\n",
    "\n",
    "> \"Today is _not_ a good day.\"\n",
    "\n",
    "Well, these lead to nearly the exact same input vector, except `X[not] = 1`. This is actually a known drawback of bag-of-words; they are notoriously bad at being able to handle negation. Now, given what we know about RNN's, you can imagine that they may be good at this because they keep state! For instance, if the RNN saw the word _not_, it may negate everything that comes after it. \n",
    "\n",
    "## 2. Word Embeddings\n",
    "This brings us to a paramount question: How _do_ we treat words in deep learning? The popular method at the moment, which has been able to produce very impressive results, is the use of word embeddings or word vectors. That means that given a vocabulary size $V$, we choose a dimensionality that is much smaller than that, $D$, where $D << V$, and then map each word vector to somewhere in the $D$ dimensional space. By training a model to do certain things like trying to predict the next word, or try to predict surrounding words, we get vectors (word embeddings) that can be manipulated via arithmetic to produce analogies such as:\n",
    "\n",
    "\n",
    "> king - man $\\approx$ queen - woman\n",
    "\n",
    "The question now is how do we use word embeddings with Recurrent Neural Networks? To accomplish this, we simply create an embedding layer in the RNN. So, the input simply arrives as a one hot encoded word, and in the next layer it becomes a $D$ dimensional vector.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Q2eh1IRL0qxB05p-xb4TJvYSwEHZP7HO\" width=\"500\">\n",
    "\n",
    "This requires the word embedding transformation matrix to be a $VxD$ matrix, where the $i$th row is the word vector for the $i$th word. For reference, all of the matrix dimensions are below:\n",
    "\n",
    "$$W_e = VxD$$\n",
    "\n",
    "$$W_x = DxM$$\n",
    "\n",
    "$$W_h = MxM$$\n",
    "\n",
    "$$W_o = MxK$$\n",
    "\n",
    "Two questions will naturally arise at this point. The first being:\n",
    "\n",
    "1. How do we traing this model?\n",
    "\n",
    "The answer to this is our old friend, gradient descent. We will also see later that when we do Word2Vec that there are some variations on the cross entropy error function that will help us speed up training. The second question is:\n",
    "\n",
    "2. What are the targets? \n",
    "\n",
    "This is a good question because language models don't necessarily have targets. You can attempt to learn word embeddings on a sentiment analysis task, so your targets could be movie ratings or some kind of movie score. Your targets could also be next word prediction as we discussed before. Again, if we use Word2Vec, the targets will also change based on the particular Word2Vec method we use. \n",
    "\n",
    "## 3. Word Analogies with Word Embeddings\n",
    "We are now going to go over how you actually can perform calculations that show that:\n",
    "\n",
    "> king - man $\\approx$ queen - woman\n",
    "\n",
    "It is quite simple, but worth going through so that intuitions can start forming about this entire process.\n",
    "\n",
    "We can start be rewriting the above as:\n",
    "\n",
    "> king - man + woman = ?\n",
    "\n",
    "Then there are two main steps:\n",
    "\n",
    "1. Convert 3 words on the left to their word embeddings. For example: \n",
    "\n",
    "```\n",
    "vec(king) = Word_embedding[word2idx[\"king\"]]\n",
    "v0 = vec(king) - vec(man) + vec(woman)\n",
    "```\n",
    "\n",
    "And `v0` is just a vector in space with an infinte number of values!\n",
    "\n",
    "2. We want to then find the \"closest\" actual word in our vocabulary to the `v0`, and return that word.\n",
    "\n",
    "Why do we need to do that? Well, the result of `vec(king) - vec(man) + vec(woman)` just gives us a vector. There is no way to map from vectors to words, since a vector space is continuous, and that would require and infinite number of words. So, the idea is that we just find closest word. \n",
    "\n",
    "### 3.1 Distance\n",
    "There are various ways of defining distance in the context above. Sometimes, we will simply use _Euclidean Distance_:\n",
    "\n",
    "$$\\text{Euclidean Distance: } ||a - b||^2$$\n",
    "\n",
    "It is also common to use the _cosine distance_:\n",
    "\n",
    "$$\\text{Cosine Distance: } cosine\\_distance(a, b) = \\frac{1 - a^Tb}{||a|| \\; ||b||}$$\n",
    "\n",
    "In this later form, since only the angle matters, because:\n",
    "\n",
    "$$a^Tb = ||a|| \\; ||b|| cos(a,b)$$ \n",
    "\n",
    "During training we normalize all of the word vectors so that their length is 1:\n",
    "\n",
    "$$cos(0) = 1, \\; cos(90) = 0, \\; cos(180) = -1$$\n",
    "\n",
    "When two vectors are closer, $cos(\\theta)$ is bigger. So, we want our distance to be:\n",
    "\n",
    "$$\\text{Distance} = 1 - cos(\\theta)$$\n",
    "\n",
    "At this point we can say that all of the word embeddings lie on the unit sphere. \n",
    "\n",
    "### 3.2 Find the best word\n",
    "Once we have our distance function, how do we actually find the closest word? The simplest word is to just look at every word in the vocabulary, and get the distance between each vector and your expression vector. Keep track of the smallest distance and then return that word. \n",
    "\n",
    "```\n",
    "min_dist = Infinity; best_word = ''\n",
    "for word, idx in word2idx.items():\n",
    "    v1 = Word_embedding[idx]\n",
    "    if dist(v0, v1) < min_dist:\n",
    "        min_dist = dist(v0, v1)\n",
    "        best_word = word\n",
    "\n",
    "print(\"The best word is: \", best_word)\n",
    "```\n",
    "\n",
    "We may want to leave out the words from the left side of the equation, in this case _king, man_, and _woman_. Note that we will not be using this on our upcoming poetry data, since it doesn't have the kind of vocabulary we are looking for. We are more interested in things like nouns when we do word analogies. We want to compare kings and queens, men and women, occupations, etc. We will look more at word analogies later on. \n",
    "\n",
    "## 4. Representing a Sequence of Words as a Sequence of Word Embeddings\n",
    "Let's quickly go over one small detail from the upcoming code, that may be slightly confusing. We have a word embedding matrix, $W_e$, which is of size $V x D$ (V = vocabulary size, D = word vector dimensionality), and we have an input sequence of word indexes of length $T$. We would like to get a sequence of word vectors that represent a sentence, which is a $TxD$ vector. However, we will need to update the word embeddings via backpropagation, so the $TxD$ matrix we get after grabbing the word vectors cannot be the input into the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
