{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Facial Expression Recognition - Theano\n",
    "We are now going to go through the facial expression recognition project that we have worked on in the past, but we will use **Theano** as our framework of choice this time! We will be creating a neural network that has 2000 units in the first hidden layer, and 1000 units in the second hidden layer. We can start with our imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "% matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can define the utilities that we are going to need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"----------------------- Function to get data -----------------------------\"\"\"\n",
    "def getData(balance_ones=True):\n",
    "    # images are 48x48 = 2304 size vectors\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True\n",
    "    for line in open('../../../data/fer/fer2013.csv'):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            Y.append(int(row[0]))\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y)\n",
    "\n",
    "    if balance_ones:\n",
    "        # balance the 1 class\n",
    "        X0, Y0 = X[Y!=1, :], Y[Y!=1]\n",
    "        X1 = X[Y==1, :]\n",
    "        X1 = np.repeat(X1, 9, axis=0)\n",
    "        X = np.vstack([X0, X1])\n",
    "        Y = np.concatenate((Y0, [1]*len(X1)))\n",
    "\n",
    "    return X, Y\n",
    "  \n",
    "\"\"\" --------- Creates indicator (N x K), from an input N x 1 y matrix --------\"\"\"\n",
    "def y2indicator(y):\n",
    "    N = len(y)\n",
    "    K = len(set(y))\n",
    "    ind = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "  \n",
    "\"\"\" ----------- Gives the error rate between targets and predictions ---------------- \"\"\"\n",
    "def error_rate(targets, predictions):\n",
    "    return np.mean(targets != predictions)\n",
    "  \n",
    "\"\"\" Rectifier Linear Unit - an activation function that can be used in a neural network \"\"\"\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "  \n",
    "\"\"\"\n",
    "Function to initialize a weight matrix and a bias. M1 is the input size, and M2 is the output size\n",
    "W is a matrix of size M1 x M2, which is randomized initialy to a gaussian normal\n",
    "We make the standard deviation of this the sqrt of size in + size out\n",
    "The bias is initialized as zeros. Each is then turned into float 32s so that they can be used in \n",
    "Theano and TensorFlow\n",
    "\"\"\"\n",
    "def init_weight_and_bias(M1, M2):\n",
    "    W = np.random.randn(M1, M2) / np.sqrt(M1)\n",
    "    b = np.zeros(M2)\n",
    "    return W.astype(np.float32), b.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsprop(cost, params, lr, mu, decay, eps):\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        # cache\n",
    "        ones = np.ones_like(p.get_value(), dtype=np.float32)\n",
    "        c = theano.shared(ones)\n",
    "        new_c = decay*c + (np.float32(1.0) - decay)*g*g\n",
    "\n",
    "        # momentum\n",
    "        zeros = np.zeros_like(p.get_value(), dtype=np.float32)\n",
    "        m = theano.shared(zeros)\n",
    "        new_m = mu*m - lr*g / T.sqrt(new_c + eps)\n",
    "\n",
    "        # param update\n",
    "        new_p = p + new_m\n",
    "\n",
    "        # append the updates\n",
    "        updates.append((c, new_c))\n",
    "        updates.append((m, new_m))\n",
    "        updates.append((p, new_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to put our hidden layer into it's own class. We want to do this so we can add an arbitrary number of hidden layers more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "  def __init__(self, M1, M2, an_id):\n",
    "    self.id = id\n",
    "    self.M1 = M1\n",
    "    self.M2 = M2\n",
    "    W, b = init_weight_and_bias(M1, M2)           # Getting initial weights and bias's\n",
    "    \n",
    "    \"\"\"Recall, in theano a shared variable is an updatable variable\"\"\"\n",
    "    self.W = theano.shared(W, 'W_%s' % self.id)   # Unique name associated with id\n",
    "    self.b = theano.shared(b, 'W_%s' % self.id)\n",
    "    self.params = [self.W, self.b]                # Keep all params in 1 list to calc grad\n",
    "    \n",
    "  def forward(self, X):\n",
    "    return relu(X.dot(self.W) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our **ANN** class. It will take in the hidden layer sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsprop(cost, params, lr, mu, decay, eps):\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        # cache\n",
    "        ones = np.ones_like(p.get_value(), dtype=np.float32)\n",
    "        c = theano.shared(ones)\n",
    "        new_c = decay*c + (np.float32(1.0) - decay)*g*g\n",
    "\n",
    "        # momentum\n",
    "        zeros = np.zeros_like(p.get_value(), dtype=np.float32)\n",
    "        m = theano.shared(zeros)\n",
    "        new_m = mu*m - lr*g / T.sqrt(new_c + eps)\n",
    "\n",
    "        # param update\n",
    "        new_p = p + new_m\n",
    "\n",
    "        # append the updates\n",
    "        updates.append((c, new_c))\n",
    "        updates.append((m, new_m))\n",
    "        updates.append((p, new_p))\n",
    "    return updates\n",
    "\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, an_id):\n",
    "        self.id = an_id\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        W, b = init_weight_and_bias(M1, M2)\n",
    "        self.W = theano.shared(W, 'W_%s' % self.id)\n",
    "        self.b = theano.shared(b, 'b_%s' % self.id)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        return relu(X.dot(self.W) + self.b)\n",
    "\n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=1e-3, mu=0.9, decay=0.9, reg=0, eps=1e-10, epochs=100, batch_sz=30, show_fig=False):\n",
    "        learning_rate = np.float32(learning_rate)\n",
    "        mu = np.float32(mu)\n",
    "        decay = np.float32(decay)\n",
    "        reg = np.float32(reg)\n",
    "        eps = np.float32(eps)\n",
    "\n",
    "        # make a validation set\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.int32)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        # initialize hidden layers\n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        self.hidden_layers = []\n",
    "        M1 = D\n",
    "        count = 0\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, count)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "        W, b = init_weight_and_bias(M1, K)\n",
    "        self.W = theano.shared(W, 'W_logreg')\n",
    "        self.b = theano.shared(b, 'b_logreg')\n",
    "\n",
    "        # collect params for later use\n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # set up theano functions and variables\n",
    "        thX = T.fmatrix('X')\n",
    "        thY = T.ivector('Y')\n",
    "        pY = self.th_forward(thX)\n",
    "\n",
    "        rcost = reg*T.sum([(p*p).sum() for p in self.params])\n",
    "        cost = -T.mean(T.log(pY[T.arange(thY.shape[0]), thY])) + rcost\n",
    "        prediction = self.th_predict(thX)\n",
    "\n",
    "        # actual prediction function\n",
    "        self.predict_op = theano.function(inputs=[thX], outputs=prediction)\n",
    "        cost_predict_op = theano.function(inputs=[thX, thY], outputs=[cost, prediction])\n",
    "\n",
    "        updates = rmsprop(cost, self.params, learning_rate, mu, decay, eps)\n",
    "        train_op = theano.function(\n",
    "            inputs=[thX, thY],\n",
    "            updates=updates\n",
    "        )\n",
    "\n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                train_op(Xbatch, Ybatch)\n",
    "\n",
    "                if j % 20 == 0:\n",
    "                    c, p = cost_predict_op(Xvalid, Yvalid)\n",
    "                    costs.append(c)\n",
    "                    e = error_rate(Yvalid, p)\n",
    "                    print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def th_forward(self, X):\n",
    "        Z = X\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return T.nnet.softmax(Z.dot(self.W) + self.b)\n",
    "\n",
    "    def th_predict(self, X):\n",
    "        pY = self.th_forward(X)\n",
    "        return T.argmax(pY, axis=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_op(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally have our main method. We are going to create a model that contains 2000 units in the first hidden layer, and 1000 units in the second hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 j: 0 nb: 1308 cost: 1.9600968 error rate: 0.86\n",
      "i: 0 j: 20 nb: 1308 cost: 1.9326383 error rate: 0.821\n",
      "i: 0 j: 40 nb: 1308 cost: 1.9993237 error rate: 0.814\n",
      "i: 0 j: 60 nb: 1308 cost: 1.974063 error rate: 0.792\n",
      "i: 0 j: 80 nb: 1308 cost: 2.0640376 error rate: 0.859\n",
      "i: 0 j: 100 nb: 1308 cost: 1.9693612 error rate: 0.781\n",
      "i: 0 j: 120 nb: 1308 cost: 1.9572076 error rate: 0.844\n",
      "i: 0 j: 140 nb: 1308 cost: 2.1412694 error rate: 0.833\n",
      "i: 0 j: 160 nb: 1308 cost: 2.0697033 error rate: 0.853\n",
      "i: 0 j: 180 nb: 1308 cost: 2.0310144 error rate: 0.789\n",
      "i: 0 j: 200 nb: 1308 cost: 2.3919935 error rate: 0.863\n",
      "i: 0 j: 220 nb: 1308 cost: 1.9690156 error rate: 0.825\n",
      "i: 0 j: 240 nb: 1308 cost: 2.9617116 error rate: 0.859\n",
      "i: 0 j: 260 nb: 1308 cost: 4.2984 error rate: 0.865\n",
      "i: 0 j: 280 nb: 1308 cost: 1.9451818 error rate: 0.838\n",
      "i: 0 j: 300 nb: 1308 cost: 1.9405148 error rate: 0.856\n",
      "i: 0 j: 320 nb: 1308 cost: 2.0670571 error rate: 0.788\n",
      "i: 0 j: 340 nb: 1308 cost: 1.9534435 error rate: 0.789\n",
      "i: 0 j: 360 nb: 1308 cost: 1.9344307 error rate: 0.789\n",
      "i: 0 j: 380 nb: 1308 cost: 1.9645922 error rate: 0.789\n",
      "i: 0 j: 400 nb: 1308 cost: 5.3975463 error rate: 0.863\n",
      "i: 0 j: 420 nb: 1308 cost: 1.9336445 error rate: 0.844\n",
      "i: 0 j: 440 nb: 1308 cost: 1.969696 error rate: 0.788\n",
      "i: 0 j: 460 nb: 1308 cost: 1.9291116 error rate: 0.789\n",
      "i: 0 j: 480 nb: 1308 cost: 2.2121067 error rate: 0.863\n",
      "i: 0 j: 500 nb: 1308 cost: 1.9315329 error rate: 0.789\n",
      "i: 0 j: 520 nb: 1308 cost: 1.9290155 error rate: 0.789\n",
      "i: 0 j: 540 nb: 1308 cost: 1.9861394 error rate: 0.789\n",
      "i: 0 j: 560 nb: 1308 cost: 1.9405789 error rate: 0.789\n",
      "i: 0 j: 580 nb: 1308 cost: 1.9331108 error rate: 0.789\n",
      "i: 0 j: 600 nb: 1308 cost: 1.9262918 error rate: 0.789\n",
      "i: 0 j: 620 nb: 1308 cost: 1.9273973 error rate: 0.789\n",
      "i: 0 j: 640 nb: 1308 cost: 2.137092 error rate: 0.789\n",
      "i: 0 j: 660 nb: 1308 cost: 1.9274015 error rate: 0.789\n",
      "i: 0 j: 680 nb: 1308 cost: 2.0030372 error rate: 0.789\n",
      "i: 0 j: 700 nb: 1308 cost: 1.9281007 error rate: 0.789\n",
      "i: 0 j: 720 nb: 1308 cost: 1.9311236 error rate: 0.793\n",
      "i: 0 j: 740 nb: 1308 cost: 1.930942 error rate: 0.789\n",
      "i: 0 j: 760 nb: 1308 cost: 1.9280195 error rate: 0.789\n",
      "i: 0 j: 780 nb: 1308 cost: 1.9353257 error rate: 0.79\n",
      "i: 0 j: 800 nb: 1308 cost: 1.9263757 error rate: 0.789\n",
      "i: 0 j: 820 nb: 1308 cost: 1.9285902 error rate: 0.789\n",
      "i: 0 j: 840 nb: 1308 cost: 1.9283326 error rate: 0.789\n",
      "i: 0 j: 860 nb: 1308 cost: 1.925663 error rate: 0.789\n",
      "i: 0 j: 880 nb: 1308 cost: 1.928249 error rate: 0.789\n",
      "i: 0 j: 900 nb: 1308 cost: 1.9854661 error rate: 0.789\n",
      "i: 0 j: 920 nb: 1308 cost: 1.9272938 error rate: 0.789\n",
      "i: 0 j: 940 nb: 1308 cost: 1.9293319 error rate: 0.789\n",
      "i: 0 j: 960 nb: 1308 cost: 1.930688 error rate: 0.789\n",
      "i: 0 j: 980 nb: 1308 cost: 3.546058 error rate: 0.79\n",
      "i: 0 j: 1000 nb: 1308 cost: 1.9294019 error rate: 0.79\n",
      "i: 0 j: 1020 nb: 1308 cost: 1.9305552 error rate: 0.789\n",
      "i: 0 j: 1040 nb: 1308 cost: 1.9274943 error rate: 0.789\n",
      "i: 0 j: 1060 nb: 1308 cost: 1.9272165 error rate: 0.789\n",
      "i: 0 j: 1080 nb: 1308 cost: 1.9289182 error rate: 0.789\n",
      "i: 0 j: 1100 nb: 1308 cost: 1.936025 error rate: 0.842\n",
      "i: 0 j: 1120 nb: 1308 cost: 1.929261 error rate: 0.788\n",
      "i: 0 j: 1140 nb: 1308 cost: 1.9257916 error rate: 0.789\n",
      "i: 0 j: 1160 nb: 1308 cost: 1.9244026 error rate: 0.789\n",
      "i: 0 j: 1180 nb: 1308 cost: 1.9233466 error rate: 0.789\n",
      "i: 0 j: 1200 nb: 1308 cost: 1.925046 error rate: 0.789\n",
      "i: 0 j: 1220 nb: 1308 cost: 1.9250493 error rate: 0.789\n",
      "i: 0 j: 1240 nb: 1308 cost: 2.445864 error rate: 0.835\n",
      "i: 0 j: 1260 nb: 1308 cost: 1.9296873 error rate: 0.789\n",
      "i: 0 j: 1280 nb: 1308 cost: 1.9262257 error rate: 0.789\n",
      "i: 0 j: 1300 nb: 1308 cost: 1.928734 error rate: 0.788\n",
      "i: 1 j: 0 nb: 1308 cost: 1.9274153 error rate: 0.789\n",
      "i: 1 j: 20 nb: 1308 cost: 1.9282556 error rate: 0.79\n",
      "i: 1 j: 40 nb: 1308 cost: 2.1854393 error rate: 0.821\n",
      "i: 1 j: 60 nb: 1308 cost: 1.9288077 error rate: 0.79\n",
      "i: 1 j: 80 nb: 1308 cost: 1.9273546 error rate: 0.789\n",
      "i: 1 j: 100 nb: 1308 cost: 1.925882 error rate: 0.789\n",
      "i: 1 j: 120 nb: 1308 cost: 1.9271579 error rate: 0.789\n",
      "i: 1 j: 140 nb: 1308 cost: 1.9276342 error rate: 0.789\n",
      "i: 1 j: 160 nb: 1308 cost: 1.9278979 error rate: 0.788\n",
      "i: 1 j: 180 nb: 1308 cost: 1.9272563 error rate: 0.788\n",
      "i: 1 j: 200 nb: 1308 cost: 1.9246976 error rate: 0.789\n",
      "i: 1 j: 220 nb: 1308 cost: 1.92681 error rate: 0.789\n",
      "i: 1 j: 240 nb: 1308 cost: 1.9263301 error rate: 0.789\n",
      "i: 1 j: 260 nb: 1308 cost: 1.9257641 error rate: 0.79\n",
      "i: 1 j: 280 nb: 1308 cost: 1.925494 error rate: 0.789\n",
      "i: 1 j: 300 nb: 1308 cost: 1.9264581 error rate: 0.789\n",
      "i: 1 j: 320 nb: 1308 cost: 1.9297161 error rate: 0.789\n",
      "i: 1 j: 340 nb: 1308 cost: 1.9280138 error rate: 0.789\n",
      "i: 1 j: 360 nb: 1308 cost: 1.9271472 error rate: 0.789\n",
      "i: 1 j: 380 nb: 1308 cost: 1.9296726 error rate: 0.789\n",
      "i: 1 j: 400 nb: 1308 cost: 1.9273375 error rate: 0.789\n",
      "i: 1 j: 420 nb: 1308 cost: 1.9255503 error rate: 0.789\n",
      "i: 1 j: 440 nb: 1308 cost: 1.9257944 error rate: 0.789\n",
      "i: 1 j: 460 nb: 1308 cost: 1.9263853 error rate: 0.789\n",
      "i: 1 j: 480 nb: 1308 cost: 1.9282466 error rate: 0.789\n",
      "i: 1 j: 500 nb: 1308 cost: 1.9313726 error rate: 0.789\n",
      "i: 1 j: 520 nb: 1308 cost: 1.9340677 error rate: 0.789\n",
      "i: 1 j: 540 nb: 1308 cost: 1.9307196 error rate: 0.789\n",
      "i: 1 j: 560 nb: 1308 cost: 1.9295579 error rate: 0.789\n",
      "i: 1 j: 580 nb: 1308 cost: 1.93036 error rate: 0.789\n",
      "i: 1 j: 600 nb: 1308 cost: 1.929747 error rate: 0.789\n",
      "i: 1 j: 620 nb: 1308 cost: 1.9296526 error rate: 0.789\n",
      "i: 1 j: 640 nb: 1308 cost: 1.928709 error rate: 0.789\n",
      "i: 1 j: 660 nb: 1308 cost: 1.9298744 error rate: 0.789\n",
      "i: 1 j: 680 nb: 1308 cost: 1.937417 error rate: 0.789\n",
      "i: 1 j: 700 nb: 1308 cost: 1.9295555 error rate: 0.789\n",
      "i: 1 j: 720 nb: 1308 cost: 1.9283264 error rate: 0.789\n",
      "i: 1 j: 740 nb: 1308 cost: 1.9269034 error rate: 0.789\n",
      "i: 1 j: 760 nb: 1308 cost: 1.9257163 error rate: 0.789\n",
      "i: 1 j: 780 nb: 1308 cost: 1.924526 error rate: 0.789\n",
      "i: 1 j: 800 nb: 1308 cost: 1.9245574 error rate: 0.789\n",
      "i: 1 j: 820 nb: 1308 cost: 1.924814 error rate: 0.789\n",
      "i: 1 j: 840 nb: 1308 cost: 1.9254591 error rate: 0.789\n",
      "i: 1 j: 860 nb: 1308 cost: 1.9272584 error rate: 0.789\n",
      "i: 1 j: 880 nb: 1308 cost: 1.9297862 error rate: 0.789\n",
      "i: 1 j: 900 nb: 1308 cost: 1.9298795 error rate: 0.789\n",
      "i: 1 j: 920 nb: 1308 cost: 1.9285538 error rate: 0.789\n",
      "i: 1 j: 940 nb: 1308 cost: 1.9279157 error rate: 0.789\n",
      "i: 1 j: 960 nb: 1308 cost: 1.9269477 error rate: 0.789\n",
      "i: 1 j: 980 nb: 1308 cost: 1.9269766 error rate: 0.789\n",
      "i: 1 j: 1000 nb: 1308 cost: 1.927372 error rate: 0.789\n",
      "i: 1 j: 1020 nb: 1308 cost: 1.9264969 error rate: 0.789\n",
      "i: 1 j: 1040 nb: 1308 cost: 1.9264976 error rate: 0.789\n",
      "i: 1 j: 1060 nb: 1308 cost: 1.926073 error rate: 0.789\n",
      "i: 1 j: 1080 nb: 1308 cost: 1.9264979 error rate: 0.789\n",
      "i: 1 j: 1100 nb: 1308 cost: 1.9316719 error rate: 0.798\n",
      "i: 1 j: 1120 nb: 1308 cost: 1.9298369 error rate: 0.789\n",
      "i: 1 j: 1140 nb: 1308 cost: 1.9281722 error rate: 0.789\n",
      "i: 1 j: 1160 nb: 1308 cost: 1.9267855 error rate: 0.79\n",
      "i: 1 j: 1180 nb: 1308 cost: 1.9246607 error rate: 0.788\n",
      "i: 1 j: 1200 nb: 1308 cost: 1.9455155 error rate: 0.788\n",
      "i: 1 j: 1220 nb: 1308 cost: 1.9263973 error rate: 0.789\n",
      "i: 1 j: 1240 nb: 1308 cost: 1.9302982 error rate: 0.79\n",
      "i: 1 j: 1260 nb: 1308 cost: 1.9299201 error rate: 0.789\n",
      "i: 1 j: 1280 nb: 1308 cost: 1.928326 error rate: 0.789\n",
      "i: 1 j: 1300 nb: 1308 cost: 1.9286051 error rate: 0.789\n",
      "i: 2 j: 0 nb: 1308 cost: 1.9289194 error rate: 0.789\n",
      "i: 2 j: 20 nb: 1308 cost: 1.9291619 error rate: 0.789\n",
      "i: 2 j: 40 nb: 1308 cost: 1.926715 error rate: 0.789\n",
      "i: 2 j: 60 nb: 1308 cost: 1.9267011 error rate: 0.789\n",
      "i: 2 j: 80 nb: 1308 cost: 1.925863 error rate: 0.789\n",
      "i: 2 j: 100 nb: 1308 cost: 1.9254769 error rate: 0.789\n",
      "i: 2 j: 120 nb: 1308 cost: 1.9264581 error rate: 0.789\n",
      "i: 2 j: 140 nb: 1308 cost: 1.9275635 error rate: 0.789\n",
      "i: 2 j: 160 nb: 1308 cost: 1.9279593 error rate: 0.789\n",
      "i: 2 j: 180 nb: 1308 cost: 1.9275194 error rate: 0.789\n",
      "i: 2 j: 200 nb: 1308 cost: 1.9278306 error rate: 0.789\n",
      "i: 2 j: 220 nb: 1308 cost: 1.9275943 error rate: 0.789\n",
      "i: 2 j: 240 nb: 1308 cost: 1.9271764 error rate: 0.789\n",
      "i: 2 j: 260 nb: 1308 cost: 1.9255534 error rate: 0.789\n",
      "i: 2 j: 280 nb: 1308 cost: 1.9250325 error rate: 0.789\n",
      "i: 2 j: 300 nb: 1308 cost: 1.9259413 error rate: 0.789\n",
      "i: 2 j: 320 nb: 1308 cost: 1.9280107 error rate: 0.789\n",
      "i: 2 j: 340 nb: 1308 cost: 1.9265481 error rate: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 2 j: 360 nb: 1308 cost: 1.9255563 error rate: 0.788\n",
      "i: 2 j: 380 nb: 1308 cost: 1.9251419 error rate: 0.789\n",
      "i: 2 j: 400 nb: 1308 cost: 1.9275148 error rate: 0.789\n",
      "i: 2 j: 420 nb: 1308 cost: 1.9279933 error rate: 0.789\n",
      "i: 2 j: 440 nb: 1308 cost: 1.9266092 error rate: 0.789\n",
      "i: 2 j: 460 nb: 1308 cost: 1.9248382 error rate: 0.789\n",
      "i: 2 j: 480 nb: 1308 cost: 1.9248874 error rate: 0.788\n",
      "i: 2 j: 500 nb: 1308 cost: 1.9268639 error rate: 0.789\n",
      "i: 2 j: 520 nb: 1308 cost: 1.9260527 error rate: 0.789\n",
      "i: 2 j: 540 nb: 1308 cost: 1.9255645 error rate: 0.789\n",
      "i: 2 j: 560 nb: 1308 cost: 1.9254756 error rate: 0.789\n",
      "i: 2 j: 580 nb: 1308 cost: 1.9265329 error rate: 0.789\n",
      "i: 2 j: 600 nb: 1308 cost: 1.9252056 error rate: 0.789\n",
      "i: 2 j: 620 nb: 1308 cost: 1.925992 error rate: 0.789\n",
      "i: 2 j: 640 nb: 1308 cost: 1.928254 error rate: 0.789\n",
      "i: 2 j: 660 nb: 1308 cost: 1.9281934 error rate: 0.789\n",
      "i: 2 j: 680 nb: 1308 cost: 1.9274412 error rate: 0.789\n",
      "i: 2 j: 700 nb: 1308 cost: 1.9268596 error rate: 0.79\n",
      "i: 2 j: 720 nb: 1308 cost: 1.9267217 error rate: 0.789\n",
      "i: 2 j: 740 nb: 1308 cost: 1.9274473 error rate: 0.79\n",
      "i: 2 j: 760 nb: 1308 cost: 1.9269769 error rate: 0.788\n",
      "i: 2 j: 780 nb: 1308 cost: 1.9280994 error rate: 0.789\n",
      "i: 2 j: 800 nb: 1308 cost: 1.9282677 error rate: 0.79\n",
      "i: 2 j: 820 nb: 1308 cost: 1.9263415 error rate: 0.789\n",
      "i: 2 j: 840 nb: 1308 cost: 1.9301753 error rate: 0.789\n",
      "i: 2 j: 860 nb: 1308 cost: 1.9282435 error rate: 0.789\n",
      "i: 2 j: 880 nb: 1308 cost: 1.9267455 error rate: 0.789\n",
      "i: 2 j: 900 nb: 1308 cost: 1.9263834 error rate: 0.789\n",
      "i: 2 j: 920 nb: 1308 cost: 1.9279094 error rate: 0.789\n",
      "i: 2 j: 940 nb: 1308 cost: 1.928787 error rate: 0.789\n",
      "i: 2 j: 960 nb: 1308 cost: 2.544893 error rate: 0.789\n",
      "i: 2 j: 980 nb: 1308 cost: 1.9269984 error rate: 0.789\n",
      "i: 2 j: 1000 nb: 1308 cost: 1.9258207 error rate: 0.789\n",
      "i: 2 j: 1020 nb: 1308 cost: 1.9254777 error rate: 0.789\n",
      "i: 2 j: 1040 nb: 1308 cost: 1.9268359 error rate: 0.789\n",
      "i: 2 j: 1060 nb: 1308 cost: 1.9274381 error rate: 0.789\n",
      "i: 2 j: 1080 nb: 1308 cost: 1.9291972 error rate: 0.789\n",
      "i: 2 j: 1100 nb: 1308 cost: 1.9282752 error rate: 0.789\n",
      "i: 2 j: 1120 nb: 1308 cost: 1.9305087 error rate: 0.789\n",
      "i: 2 j: 1140 nb: 1308 cost: 1.932946 error rate: 0.789\n",
      "i: 2 j: 1160 nb: 1308 cost: 1.9300828 error rate: 0.789\n",
      "i: 2 j: 1180 nb: 1308 cost: 1.9282584 error rate: 0.789\n",
      "i: 2 j: 1200 nb: 1308 cost: 1.9281523 error rate: 0.789\n",
      "i: 2 j: 1220 nb: 1308 cost: 1.9286903 error rate: 0.789\n",
      "i: 2 j: 1240 nb: 1308 cost: 1.9275593 error rate: 0.789\n",
      "i: 2 j: 1260 nb: 1308 cost: 1.9278893 error rate: 0.789\n",
      "i: 2 j: 1280 nb: 1308 cost: 1.9284737 error rate: 0.789\n",
      "i: 2 j: 1300 nb: 1308 cost: 1.9294668 error rate: 0.789\n",
      "i: 3 j: 0 nb: 1308 cost: 1.9296255 error rate: 0.789\n",
      "i: 3 j: 20 nb: 1308 cost: 1.9296972 error rate: 0.789\n",
      "i: 3 j: 40 nb: 1308 cost: 1.9297032 error rate: 0.789\n",
      "i: 3 j: 60 nb: 1308 cost: 1.9290905 error rate: 0.789\n",
      "i: 3 j: 80 nb: 1308 cost: 1.9293672 error rate: 0.789\n",
      "i: 3 j: 100 nb: 1308 cost: 1.9292774 error rate: 0.789\n",
      "i: 3 j: 120 nb: 1308 cost: 1.9283903 error rate: 0.789\n",
      "i: 3 j: 140 nb: 1308 cost: 1.9283897 error rate: 0.789\n",
      "i: 3 j: 160 nb: 1308 cost: 1.9276152 error rate: 0.789\n",
      "i: 3 j: 180 nb: 1308 cost: 1.928108 error rate: 0.789\n",
      "i: 3 j: 200 nb: 1308 cost: 1.9266742 error rate: 0.789\n",
      "i: 3 j: 220 nb: 1308 cost: 1.9273871 error rate: 0.789\n",
      "i: 3 j: 240 nb: 1308 cost: 1.9272538 error rate: 0.789\n",
      "i: 3 j: 260 nb: 1308 cost: 1.9267428 error rate: 0.789\n",
      "i: 3 j: 280 nb: 1308 cost: 1.9260736 error rate: 0.789\n",
      "i: 3 j: 300 nb: 1308 cost: 1.9260852 error rate: 0.789\n",
      "i: 3 j: 320 nb: 1308 cost: 1.927456 error rate: 0.789\n",
      "i: 3 j: 340 nb: 1308 cost: 1.9280291 error rate: 0.789\n",
      "i: 3 j: 360 nb: 1308 cost: 1.9282405 error rate: 0.789\n",
      "i: 3 j: 380 nb: 1308 cost: 1.9323761 error rate: 0.789\n",
      "i: 3 j: 400 nb: 1308 cost: 1.9244168 error rate: 0.789\n",
      "i: 3 j: 420 nb: 1308 cost: 1.9253566 error rate: 0.789\n",
      "i: 3 j: 440 nb: 1308 cost: 1.9273272 error rate: 0.789\n",
      "i: 3 j: 460 nb: 1308 cost: 1.9253271 error rate: 0.789\n",
      "i: 3 j: 480 nb: 1308 cost: 1.926416 error rate: 0.789\n",
      "i: 3 j: 500 nb: 1308 cost: 1.9285085 error rate: 0.789\n",
      "i: 3 j: 520 nb: 1308 cost: 1.9370718 error rate: 0.789\n",
      "i: 3 j: 540 nb: 1308 cost: 1.9254866 error rate: 0.789\n",
      "i: 3 j: 560 nb: 1308 cost: 1.9265872 error rate: 0.789\n",
      "i: 3 j: 580 nb: 1308 cost: 1.9266706 error rate: 0.789\n",
      "i: 3 j: 600 nb: 1308 cost: 1.9244821 error rate: 0.789\n",
      "i: 3 j: 620 nb: 1308 cost: 1.9249223 error rate: 0.789\n",
      "i: 3 j: 640 nb: 1308 cost: 1.9250251 error rate: 0.789\n",
      "i: 3 j: 660 nb: 1308 cost: 1.9257115 error rate: 0.789\n",
      "i: 3 j: 680 nb: 1308 cost: 1.924992 error rate: 0.789\n",
      "i: 3 j: 700 nb: 1308 cost: 1.9246516 error rate: 0.789\n",
      "i: 3 j: 720 nb: 1308 cost: 1.9249194 error rate: 0.789\n",
      "i: 3 j: 740 nb: 1308 cost: 1.9268018 error rate: 0.789\n",
      "i: 3 j: 760 nb: 1308 cost: 1.9264426 error rate: 0.789\n",
      "i: 3 j: 780 nb: 1308 cost: 1.9258552 error rate: 0.789\n",
      "i: 3 j: 800 nb: 1308 cost: 1.9275829 error rate: 0.789\n",
      "i: 3 j: 820 nb: 1308 cost: 1.9272449 error rate: 0.789\n",
      "i: 3 j: 840 nb: 1308 cost: 1.9266582 error rate: 0.789\n",
      "i: 3 j: 860 nb: 1308 cost: 1.9257396 error rate: 0.789\n",
      "i: 3 j: 880 nb: 1308 cost: 1.9265895 error rate: 0.789\n",
      "i: 3 j: 900 nb: 1308 cost: 1.9250852 error rate: 0.789\n",
      "i: 3 j: 920 nb: 1308 cost: 1.92567 error rate: 0.789\n",
      "i: 3 j: 940 nb: 1308 cost: 1.9256543 error rate: 0.789\n",
      "i: 3 j: 960 nb: 1308 cost: 1.9255366 error rate: 0.789\n",
      "i: 3 j: 980 nb: 1308 cost: 1.9277928 error rate: 0.789\n",
      "i: 3 j: 1000 nb: 1308 cost: 1.928687 error rate: 0.789\n",
      "i: 3 j: 1020 nb: 1308 cost: 1.9286485 error rate: 0.789\n",
      "i: 3 j: 1040 nb: 1308 cost: 1.9283109 error rate: 0.789\n",
      "i: 3 j: 1060 nb: 1308 cost: 1.9272156 error rate: 0.789\n",
      "i: 3 j: 1080 nb: 1308 cost: 1.9265348 error rate: 0.789\n",
      "i: 3 j: 1100 nb: 1308 cost: 1.9279743 error rate: 0.789\n",
      "i: 3 j: 1120 nb: 1308 cost: 1.9280534 error rate: 0.789\n",
      "i: 3 j: 1140 nb: 1308 cost: 1.9278867 error rate: 0.789\n",
      "i: 3 j: 1160 nb: 1308 cost: 1.9278477 error rate: 0.789\n",
      "i: 3 j: 1180 nb: 1308 cost: 1.9280943 error rate: 0.789\n",
      "i: 3 j: 1200 nb: 1308 cost: 1.9292291 error rate: 0.789\n",
      "i: 3 j: 1220 nb: 1308 cost: 1.9271468 error rate: 0.789\n",
      "i: 3 j: 1240 nb: 1308 cost: 1.9278908 error rate: 0.789\n",
      "i: 3 j: 1260 nb: 1308 cost: 1.9276528 error rate: 0.789\n",
      "i: 3 j: 1280 nb: 1308 cost: 1.9280906 error rate: 0.789\n",
      "i: 3 j: 1300 nb: 1308 cost: 1.9259477 error rate: 0.789\n",
      "i: 4 j: 0 nb: 1308 cost: 1.9254365 error rate: 0.789\n",
      "i: 4 j: 20 nb: 1308 cost: 1.9259241 error rate: 0.789\n",
      "i: 4 j: 40 nb: 1308 cost: 1.9269885 error rate: 0.789\n",
      "i: 4 j: 60 nb: 1308 cost: 1.9280071 error rate: 0.788\n",
      "i: 4 j: 80 nb: 1308 cost: 1.9278597 error rate: 0.789\n",
      "i: 4 j: 100 nb: 1308 cost: 1.9273576 error rate: 0.789\n",
      "i: 4 j: 120 nb: 1308 cost: 1.9282101 error rate: 0.789\n",
      "i: 4 j: 140 nb: 1308 cost: 1.9273131 error rate: 0.789\n",
      "i: 4 j: 160 nb: 1308 cost: 1.9259586 error rate: 0.789\n",
      "i: 4 j: 180 nb: 1308 cost: 1.9264922 error rate: 0.789\n",
      "i: 4 j: 200 nb: 1308 cost: 1.9273237 error rate: 0.789\n",
      "i: 4 j: 220 nb: 1308 cost: 1.926893 error rate: 0.789\n",
      "i: 4 j: 240 nb: 1308 cost: 1.9277275 error rate: 0.789\n",
      "i: 4 j: 260 nb: 1308 cost: 1.9273682 error rate: 0.789\n",
      "i: 4 j: 280 nb: 1308 cost: 1.9275875 error rate: 0.789\n",
      "i: 4 j: 300 nb: 1308 cost: 1.926782 error rate: 0.789\n",
      "i: 4 j: 320 nb: 1308 cost: 1.9256463 error rate: 0.789\n",
      "i: 4 j: 340 nb: 1308 cost: 1.9263653 error rate: 0.789\n",
      "i: 4 j: 360 nb: 1308 cost: 1.9257574 error rate: 0.789\n",
      "i: 4 j: 380 nb: 1308 cost: 1.9270061 error rate: 0.789\n",
      "i: 4 j: 400 nb: 1308 cost: 1.9287987 error rate: 0.789\n",
      "i: 4 j: 420 nb: 1308 cost: 1.9294044 error rate: 0.789\n",
      "i: 4 j: 440 nb: 1308 cost: 1.9275277 error rate: 0.789\n",
      "i: 4 j: 460 nb: 1308 cost: 1.9270561 error rate: 0.789\n",
      "i: 4 j: 480 nb: 1308 cost: 1.9267203 error rate: 0.789\n",
      "i: 4 j: 500 nb: 1308 cost: 1.9276965 error rate: 0.789\n",
      "i: 4 j: 520 nb: 1308 cost: 1.92721 error rate: 0.789\n",
      "i: 4 j: 540 nb: 1308 cost: 1.9263842 error rate: 0.789\n",
      "i: 4 j: 560 nb: 1308 cost: 1.9261937 error rate: 0.789\n",
      "i: 4 j: 580 nb: 1308 cost: 1.9273423 error rate: 0.789\n",
      "i: 4 j: 600 nb: 1308 cost: 1.9270396 error rate: 0.789\n",
      "i: 4 j: 620 nb: 1308 cost: 1.9265834 error rate: 0.789\n",
      "i: 4 j: 640 nb: 1308 cost: 1.9271472 error rate: 0.789\n",
      "i: 4 j: 660 nb: 1308 cost: 1.9264166 error rate: 0.789\n",
      "i: 4 j: 680 nb: 1308 cost: 1.9269034 error rate: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 4 j: 700 nb: 1308 cost: 1.927229 error rate: 0.789\n",
      "i: 4 j: 720 nb: 1308 cost: 1.927414 error rate: 0.789\n",
      "i: 4 j: 740 nb: 1308 cost: 1.9263638 error rate: 0.789\n",
      "i: 4 j: 760 nb: 1308 cost: 1.9276469 error rate: 0.789\n",
      "i: 4 j: 780 nb: 1308 cost: 1.9280436 error rate: 0.789\n",
      "i: 4 j: 800 nb: 1308 cost: 1.9268202 error rate: 0.789\n",
      "i: 4 j: 820 nb: 1308 cost: 1.9271905 error rate: 0.789\n",
      "i: 4 j: 840 nb: 1308 cost: 1.9274355 error rate: 0.789\n",
      "i: 4 j: 860 nb: 1308 cost: 1.9283943 error rate: 0.789\n",
      "i: 4 j: 880 nb: 1308 cost: 1.9278791 error rate: 0.789\n",
      "i: 4 j: 900 nb: 1308 cost: 1.9271013 error rate: 0.789\n",
      "i: 4 j: 920 nb: 1308 cost: 1.9276682 error rate: 0.789\n",
      "i: 4 j: 940 nb: 1308 cost: 1.9285917 error rate: 0.789\n",
      "i: 4 j: 960 nb: 1308 cost: 1.9256511 error rate: 0.789\n",
      "i: 4 j: 980 nb: 1308 cost: 1.9256104 error rate: 0.789\n",
      "i: 4 j: 1000 nb: 1308 cost: 1.9260243 error rate: 0.789\n",
      "i: 4 j: 1020 nb: 1308 cost: 1.9254693 error rate: 0.789\n",
      "i: 4 j: 1040 nb: 1308 cost: 1.925481 error rate: 0.789\n",
      "i: 4 j: 1060 nb: 1308 cost: 1.9270082 error rate: 0.789\n",
      "i: 4 j: 1080 nb: 1308 cost: 1.9267545 error rate: 0.789\n",
      "i: 4 j: 1100 nb: 1308 cost: 1.9253944 error rate: 0.789\n",
      "i: 4 j: 1120 nb: 1308 cost: 1.926246 error rate: 0.789\n",
      "i: 4 j: 1140 nb: 1308 cost: 1.9274472 error rate: 0.789\n",
      "i: 4 j: 1160 nb: 1308 cost: 1.9255048 error rate: 0.789\n",
      "i: 4 j: 1180 nb: 1308 cost: 1.9259347 error rate: 0.789\n",
      "i: 4 j: 1200 nb: 1308 cost: 1.9255577 error rate: 0.789\n",
      "i: 4 j: 1220 nb: 1308 cost: 1.943319 error rate: 0.789\n",
      "i: 4 j: 1240 nb: 1308 cost: 1.926425 error rate: 0.789\n",
      "i: 4 j: 1260 nb: 1308 cost: 1.928543 error rate: 0.789\n",
      "i: 4 j: 1280 nb: 1308 cost: 1.9290622 error rate: 0.789\n",
      "i: 4 j: 1300 nb: 1308 cost: 1.928374 error rate: 0.789\n",
      "i: 5 j: 0 nb: 1308 cost: 1.9291532 error rate: 0.789\n",
      "i: 5 j: 20 nb: 1308 cost: 1.9283859 error rate: 0.789\n",
      "i: 5 j: 40 nb: 1308 cost: 1.95613 error rate: 0.789\n",
      "i: 5 j: 60 nb: 1308 cost: 1.9285748 error rate: 0.789\n",
      "i: 5 j: 80 nb: 1308 cost: 1.9285586 error rate: 0.789\n",
      "i: 5 j: 100 nb: 1308 cost: 1.9267668 error rate: 0.789\n",
      "i: 5 j: 120 nb: 1308 cost: 1.926367 error rate: 0.789\n",
      "i: 5 j: 140 nb: 1308 cost: 1.9277219 error rate: 0.789\n",
      "i: 5 j: 160 nb: 1308 cost: 1.9279777 error rate: 0.789\n",
      "i: 5 j: 180 nb: 1308 cost: 1.9296522 error rate: 0.789\n",
      "i: 5 j: 200 nb: 1308 cost: 1.9279768 error rate: 0.789\n",
      "i: 5 j: 220 nb: 1308 cost: 1.9261986 error rate: 0.789\n",
      "i: 5 j: 240 nb: 1308 cost: 1.9265149 error rate: 0.789\n",
      "i: 5 j: 260 nb: 1308 cost: 1.9271082 error rate: 0.789\n",
      "i: 5 j: 280 nb: 1308 cost: 1.9278667 error rate: 0.788\n",
      "i: 5 j: 300 nb: 1308 cost: 1.9276773 error rate: 0.789\n",
      "i: 5 j: 320 nb: 1308 cost: 1.9275625 error rate: 0.789\n",
      "i: 5 j: 340 nb: 1308 cost: 1.9291185 error rate: 0.789\n",
      "i: 5 j: 360 nb: 1308 cost: 1.929701 error rate: 0.789\n",
      "i: 5 j: 380 nb: 1308 cost: 1.9304721 error rate: 0.789\n",
      "i: 5 j: 400 nb: 1308 cost: 1.9268991 error rate: 0.789\n",
      "i: 5 j: 420 nb: 1308 cost: 1.9262868 error rate: 0.789\n",
      "i: 5 j: 440 nb: 1308 cost: 1.9272518 error rate: 0.789\n",
      "i: 5 j: 460 nb: 1308 cost: 1.9268091 error rate: 0.789\n",
      "i: 5 j: 480 nb: 1308 cost: 1.9267397 error rate: 0.789\n",
      "i: 5 j: 500 nb: 1308 cost: 1.9252704 error rate: 0.789\n",
      "i: 5 j: 520 nb: 1308 cost: 1.925359 error rate: 0.789\n",
      "i: 5 j: 540 nb: 1308 cost: 1.9251602 error rate: 0.789\n",
      "i: 5 j: 560 nb: 1308 cost: 1.9246112 error rate: 0.789\n",
      "i: 5 j: 580 nb: 1308 cost: 1.9254616 error rate: 0.789\n",
      "i: 5 j: 600 nb: 1308 cost: 1.9263132 error rate: 0.789\n",
      "i: 5 j: 620 nb: 1308 cost: 1.9272838 error rate: 0.789\n",
      "i: 5 j: 640 nb: 1308 cost: 1.9271624 error rate: 0.789\n",
      "i: 5 j: 660 nb: 1308 cost: 1.927195 error rate: 0.789\n",
      "i: 5 j: 680 nb: 1308 cost: 1.9267722 error rate: 0.789\n",
      "i: 5 j: 700 nb: 1308 cost: 1.9251112 error rate: 0.789\n",
      "i: 5 j: 720 nb: 1308 cost: 1.9254714 error rate: 0.789\n",
      "i: 5 j: 740 nb: 1308 cost: 1.9254963 error rate: 0.788\n",
      "i: 5 j: 760 nb: 1308 cost: 1.9261929 error rate: 0.789\n",
      "i: 5 j: 780 nb: 1308 cost: 1.9258893 error rate: 0.789\n",
      "i: 5 j: 800 nb: 1308 cost: 1.9255108 error rate: 0.789\n",
      "i: 5 j: 820 nb: 1308 cost: 1.9272012 error rate: 0.789\n",
      "i: 5 j: 840 nb: 1308 cost: 1.924837 error rate: 0.787\n",
      "i: 5 j: 860 nb: 1308 cost: 1.9307642 error rate: 0.789\n",
      "i: 5 j: 880 nb: 1308 cost: 1.92994 error rate: 0.789\n",
      "i: 5 j: 900 nb: 1308 cost: 1.9282305 error rate: 0.789\n",
      "i: 5 j: 920 nb: 1308 cost: 1.9285804 error rate: 0.789\n",
      "i: 5 j: 940 nb: 1308 cost: 1.9263719 error rate: 0.788\n",
      "i: 5 j: 960 nb: 1308 cost: 1.9276848 error rate: 0.788\n",
      "i: 5 j: 980 nb: 1308 cost: 1.9320372 error rate: 0.789\n",
      "i: 5 j: 1000 nb: 1308 cost: 1.9278187 error rate: 0.789\n",
      "i: 5 j: 1020 nb: 1308 cost: 1.9269543 error rate: 0.789\n",
      "i: 5 j: 1040 nb: 1308 cost: 1.9267397 error rate: 0.789\n",
      "i: 5 j: 1060 nb: 1308 cost: 1.9288582 error rate: 0.789\n",
      "i: 5 j: 1080 nb: 1308 cost: 1.928426 error rate: 0.789\n",
      "i: 5 j: 1100 nb: 1308 cost: 1.9262706 error rate: 0.789\n",
      "i: 5 j: 1120 nb: 1308 cost: 1.9267523 error rate: 0.789\n",
      "i: 5 j: 1140 nb: 1308 cost: 1.9286425 error rate: 0.789\n",
      "i: 5 j: 1160 nb: 1308 cost: 1.9288459 error rate: 0.789\n",
      "i: 5 j: 1180 nb: 1308 cost: 1.9287055 error rate: 0.789\n",
      "i: 5 j: 1200 nb: 1308 cost: 1.9259472 error rate: 0.789\n",
      "i: 5 j: 1220 nb: 1308 cost: 1.9250637 error rate: 0.789\n",
      "i: 5 j: 1240 nb: 1308 cost: 1.9252652 error rate: 0.789\n",
      "i: 5 j: 1260 nb: 1308 cost: 1.9275125 error rate: 0.789\n",
      "i: 5 j: 1280 nb: 1308 cost: 1.9300429 error rate: 0.789\n",
      "i: 5 j: 1300 nb: 1308 cost: 1.9301184 error rate: 0.789\n",
      "i: 6 j: 0 nb: 1308 cost: 1.9287871 error rate: 0.789\n",
      "i: 6 j: 20 nb: 1308 cost: 1.9271711 error rate: 0.789\n",
      "i: 6 j: 40 nb: 1308 cost: 1.9286389 error rate: 0.789\n",
      "i: 6 j: 60 nb: 1308 cost: 1.9294896 error rate: 0.789\n",
      "i: 6 j: 80 nb: 1308 cost: 1.92929 error rate: 0.789\n",
      "i: 6 j: 100 nb: 1308 cost: 1.9276503 error rate: 0.789\n",
      "i: 6 j: 120 nb: 1308 cost: 1.9281949 error rate: 0.789\n",
      "i: 6 j: 140 nb: 1308 cost: 1.9270607 error rate: 0.789\n",
      "i: 6 j: 160 nb: 1308 cost: 1.9260656 error rate: 0.789\n",
      "i: 6 j: 180 nb: 1308 cost: 1.9254688 error rate: 0.789\n",
      "i: 6 j: 200 nb: 1308 cost: 1.9250313 error rate: 0.789\n",
      "i: 6 j: 220 nb: 1308 cost: 1.9261341 error rate: 0.789\n",
      "i: 6 j: 240 nb: 1308 cost: 1.9276166 error rate: 0.789\n",
      "i: 6 j: 260 nb: 1308 cost: 1.9271061 error rate: 0.789\n",
      "i: 6 j: 280 nb: 1308 cost: 1.9259135 error rate: 0.789\n",
      "i: 6 j: 300 nb: 1308 cost: 1.9260154 error rate: 0.789\n",
      "i: 6 j: 320 nb: 1308 cost: 1.9272188 error rate: 0.789\n",
      "i: 6 j: 340 nb: 1308 cost: 1.9272856 error rate: 0.789\n",
      "i: 6 j: 360 nb: 1308 cost: 1.9294988 error rate: 0.789\n",
      "i: 6 j: 380 nb: 1308 cost: 1.9286851 error rate: 0.789\n",
      "i: 6 j: 400 nb: 1308 cost: 1.9264143 error rate: 0.789\n",
      "i: 6 j: 420 nb: 1308 cost: 1.9260442 error rate: 0.789\n",
      "i: 6 j: 440 nb: 1308 cost: 1.9262575 error rate: 0.789\n",
      "i: 6 j: 460 nb: 1308 cost: 1.9262718 error rate: 0.789\n",
      "i: 6 j: 480 nb: 1308 cost: 1.9295975 error rate: 0.789\n",
      "i: 6 j: 500 nb: 1308 cost: 1.9308441 error rate: 0.789\n",
      "i: 6 j: 520 nb: 1308 cost: 1.930458 error rate: 0.789\n",
      "i: 6 j: 540 nb: 1308 cost: 1.9297601 error rate: 0.789\n",
      "i: 6 j: 560 nb: 1308 cost: 1.9540528 error rate: 0.789\n",
      "i: 6 j: 580 nb: 1308 cost: 1.9269836 error rate: 0.789\n",
      "i: 6 j: 600 nb: 1308 cost: 1.927401 error rate: 0.789\n",
      "i: 6 j: 620 nb: 1308 cost: 1.9270719 error rate: 0.789\n",
      "i: 6 j: 640 nb: 1308 cost: 1.9280387 error rate: 0.789\n",
      "i: 6 j: 660 nb: 1308 cost: 1.9278177 error rate: 0.789\n",
      "i: 6 j: 680 nb: 1308 cost: 1.9266334 error rate: 0.789\n",
      "i: 6 j: 700 nb: 1308 cost: 1.9262813 error rate: 0.789\n",
      "i: 6 j: 720 nb: 1308 cost: 1.9272256 error rate: 0.786\n",
      "i: 6 j: 740 nb: 1308 cost: 1.9274276 error rate: 0.789\n",
      "i: 6 j: 760 nb: 1308 cost: 1.9289789 error rate: 0.789\n",
      "i: 6 j: 780 nb: 1308 cost: 1.9299433 error rate: 0.789\n",
      "i: 6 j: 800 nb: 1308 cost: 1.9297924 error rate: 0.789\n",
      "i: 6 j: 820 nb: 1308 cost: 1.9291669 error rate: 0.789\n",
      "i: 6 j: 840 nb: 1308 cost: 1.9281498 error rate: 0.789\n",
      "i: 6 j: 860 nb: 1308 cost: 1.9286873 error rate: 0.789\n",
      "i: 6 j: 880 nb: 1308 cost: 1.9274993 error rate: 0.789\n",
      "i: 6 j: 900 nb: 1308 cost: 1.9266584 error rate: 0.789\n",
      "i: 6 j: 920 nb: 1308 cost: 1.9276992 error rate: 0.789\n",
      "i: 6 j: 940 nb: 1308 cost: 1.927995 error rate: 0.789\n",
      "i: 6 j: 960 nb: 1308 cost: 1.9272643 error rate: 0.789\n",
      "i: 6 j: 980 nb: 1308 cost: 1.9265876 error rate: 0.789\n",
      "i: 6 j: 1000 nb: 1308 cost: 1.9263295 error rate: 0.789\n",
      "i: 6 j: 1020 nb: 1308 cost: 1.925261 error rate: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 6 j: 1040 nb: 1308 cost: 1.9247782 error rate: 0.789\n",
      "i: 6 j: 1060 nb: 1308 cost: 1.9239725 error rate: 0.788\n",
      "i: 6 j: 1080 nb: 1308 cost: 1.9249032 error rate: 0.789\n",
      "i: 6 j: 1100 nb: 1308 cost: 1.9258085 error rate: 0.789\n",
      "i: 6 j: 1120 nb: 1308 cost: 1.9264895 error rate: 0.789\n",
      "i: 6 j: 1140 nb: 1308 cost: 1.926167 error rate: 0.789\n",
      "i: 6 j: 1160 nb: 1308 cost: 1.9262017 error rate: 0.789\n",
      "i: 6 j: 1180 nb: 1308 cost: 2.0281615 error rate: 0.788\n",
      "i: 6 j: 1200 nb: 1308 cost: 1.9262278 error rate: 0.789\n",
      "i: 6 j: 1220 nb: 1308 cost: 1.9272046 error rate: 0.789\n",
      "i: 6 j: 1240 nb: 1308 cost: 1.9286007 error rate: 0.789\n",
      "i: 6 j: 1260 nb: 1308 cost: 1.9297556 error rate: 0.789\n",
      "i: 6 j: 1280 nb: 1308 cost: 1.9295666 error rate: 0.789\n",
      "i: 6 j: 1300 nb: 1308 cost: 1.9275141 error rate: 0.789\n",
      "i: 7 j: 0 nb: 1308 cost: 1.927106 error rate: 0.789\n",
      "i: 7 j: 20 nb: 1308 cost: 1.9267431 error rate: 0.789\n",
      "i: 7 j: 40 nb: 1308 cost: 1.926187 error rate: 0.789\n",
      "i: 7 j: 60 nb: 1308 cost: 1.9270061 error rate: 0.789\n",
      "i: 7 j: 80 nb: 1308 cost: 1.9280136 error rate: 0.789\n",
      "i: 7 j: 100 nb: 1308 cost: 1.9243602 error rate: 0.787\n",
      "i: 7 j: 120 nb: 1308 cost: 1.9473099 error rate: 0.787\n",
      "i: 7 j: 140 nb: 1308 cost: 1.9297203 error rate: 0.789\n",
      "i: 7 j: 160 nb: 1308 cost: 1.9283054 error rate: 0.789\n",
      "i: 7 j: 180 nb: 1308 cost: 1.928088 error rate: 0.789\n",
      "i: 7 j: 200 nb: 1308 cost: 1.9287455 error rate: 0.789\n",
      "i: 7 j: 220 nb: 1308 cost: 1.931258 error rate: 0.789\n",
      "i: 7 j: 240 nb: 1308 cost: 1.9307598 error rate: 0.789\n",
      "i: 7 j: 260 nb: 1308 cost: 1.9280415 error rate: 0.789\n",
      "i: 7 j: 280 nb: 1308 cost: 1.9284546 error rate: 0.791\n",
      "i: 7 j: 300 nb: 1308 cost: 1.9301025 error rate: 0.79\n",
      "i: 7 j: 320 nb: 1308 cost: 1.9296002 error rate: 0.789\n",
      "i: 7 j: 340 nb: 1308 cost: 1.9274582 error rate: 0.789\n",
      "i: 7 j: 360 nb: 1308 cost: 1.9256883 error rate: 0.789\n",
      "i: 7 j: 380 nb: 1308 cost: 1.9264567 error rate: 0.789\n",
      "i: 7 j: 400 nb: 1308 cost: 1.9268074 error rate: 0.789\n",
      "i: 7 j: 420 nb: 1308 cost: 1.9403182 error rate: 0.787\n",
      "i: 7 j: 440 nb: 1308 cost: 1.92906 error rate: 0.788\n",
      "i: 7 j: 460 nb: 1308 cost: 1.9272082 error rate: 0.789\n",
      "i: 7 j: 480 nb: 1308 cost: 1.9267598 error rate: 0.789\n",
      "i: 7 j: 500 nb: 1308 cost: 1.9263792 error rate: 0.789\n",
      "i: 7 j: 520 nb: 1308 cost: 1.9275001 error rate: 0.789\n",
      "i: 7 j: 540 nb: 1308 cost: 1.9291071 error rate: 0.789\n",
      "i: 7 j: 560 nb: 1308 cost: 1.9305289 error rate: 0.789\n",
      "i: 7 j: 580 nb: 1308 cost: 1.9279408 error rate: 0.789\n",
      "i: 7 j: 600 nb: 1308 cost: 1.9272754 error rate: 0.789\n",
      "i: 7 j: 620 nb: 1308 cost: 1.9272293 error rate: 0.789\n",
      "i: 7 j: 640 nb: 1308 cost: 1.9267324 error rate: 0.789\n",
      "i: 7 j: 660 nb: 1308 cost: 1.9270816 error rate: 0.789\n",
      "i: 7 j: 680 nb: 1308 cost: 1.9269025 error rate: 0.789\n",
      "i: 7 j: 700 nb: 1308 cost: 1.9266468 error rate: 0.789\n",
      "i: 7 j: 720 nb: 1308 cost: 1.9300048 error rate: 0.788\n",
      "i: 7 j: 740 nb: 1308 cost: 1.925838 error rate: 0.788\n",
      "i: 7 j: 760 nb: 1308 cost: 1.9268119 error rate: 0.788\n",
      "i: 7 j: 780 nb: 1308 cost: 1.9275143 error rate: 0.789\n",
      "i: 7 j: 800 nb: 1308 cost: 1.9272768 error rate: 0.789\n",
      "i: 7 j: 820 nb: 1308 cost: 1.9244446 error rate: 0.787\n",
      "i: 7 j: 840 nb: 1308 cost: 1.9229839 error rate: 0.787\n",
      "i: 7 j: 860 nb: 1308 cost: 1.9232488 error rate: 0.787\n",
      "i: 7 j: 880 nb: 1308 cost: 1.9244642 error rate: 0.787\n",
      "i: 7 j: 900 nb: 1308 cost: 1.926354 error rate: 0.788\n",
      "i: 7 j: 920 nb: 1308 cost: 2.0220728 error rate: 0.781\n",
      "i: 7 j: 940 nb: 1308 cost: 1.9267288 error rate: 0.789\n",
      "i: 7 j: 960 nb: 1308 cost: 1.9274623 error rate: 0.789\n",
      "i: 7 j: 980 nb: 1308 cost: 1.9277651 error rate: 0.789\n",
      "i: 7 j: 1000 nb: 1308 cost: 1.9270552 error rate: 0.789\n",
      "i: 7 j: 1020 nb: 1308 cost: 1.9268036 error rate: 0.789\n",
      "i: 7 j: 1040 nb: 1308 cost: 1.9267982 error rate: 0.789\n",
      "i: 7 j: 1060 nb: 1308 cost: 1.9266899 error rate: 0.789\n",
      "i: 7 j: 1080 nb: 1308 cost: 1.9275892 error rate: 0.789\n",
      "i: 7 j: 1100 nb: 1308 cost: 1.9271672 error rate: 0.789\n",
      "i: 7 j: 1120 nb: 1308 cost: 1.9269547 error rate: 0.789\n",
      "i: 7 j: 1140 nb: 1308 cost: 1.9268359 error rate: 0.789\n",
      "i: 7 j: 1160 nb: 1308 cost: 1.9263027 error rate: 0.789\n",
      "i: 7 j: 1180 nb: 1308 cost: 1.9260188 error rate: 0.789\n",
      "i: 7 j: 1200 nb: 1308 cost: 1.9254451 error rate: 0.789\n",
      "i: 7 j: 1220 nb: 1308 cost: 1.9253874 error rate: 0.789\n",
      "i: 7 j: 1240 nb: 1308 cost: 1.9269611 error rate: 0.789\n",
      "i: 7 j: 1260 nb: 1308 cost: 1.9274193 error rate: 0.789\n",
      "i: 7 j: 1280 nb: 1308 cost: 1.9268352 error rate: 0.789\n",
      "i: 7 j: 1300 nb: 1308 cost: 1.9262422 error rate: 0.789\n",
      "i: 8 j: 0 nb: 1308 cost: 1.926283 error rate: 0.789\n",
      "i: 8 j: 20 nb: 1308 cost: 1.9258375 error rate: 0.789\n",
      "i: 8 j: 40 nb: 1308 cost: 1.9274607 error rate: 0.789\n",
      "i: 8 j: 60 nb: 1308 cost: 1.9286085 error rate: 0.789\n",
      "i: 8 j: 80 nb: 1308 cost: 1.9277244 error rate: 0.789\n",
      "i: 8 j: 100 nb: 1308 cost: 1.9281131 error rate: 0.789\n",
      "i: 8 j: 120 nb: 1308 cost: 1.931482 error rate: 0.789\n",
      "i: 8 j: 140 nb: 1308 cost: 1.9293425 error rate: 0.789\n",
      "i: 8 j: 160 nb: 1308 cost: 1.927576 error rate: 0.789\n",
      "i: 8 j: 180 nb: 1308 cost: 1.9265491 error rate: 0.789\n",
      "i: 8 j: 200 nb: 1308 cost: 1.9264163 error rate: 0.789\n",
      "i: 8 j: 220 nb: 1308 cost: 1.9260199 error rate: 0.789\n",
      "i: 8 j: 240 nb: 1308 cost: 1.926394 error rate: 0.789\n",
      "i: 8 j: 260 nb: 1308 cost: 1.9274399 error rate: 0.789\n",
      "i: 8 j: 280 nb: 1308 cost: 1.9265127 error rate: 0.789\n",
      "i: 8 j: 300 nb: 1308 cost: 1.9245628 error rate: 0.789\n",
      "i: 8 j: 320 nb: 1308 cost: 1.9244691 error rate: 0.789\n",
      "i: 8 j: 340 nb: 1308 cost: 1.926514 error rate: 0.789\n",
      "i: 8 j: 360 nb: 1308 cost: 1.9271004 error rate: 0.789\n",
      "i: 8 j: 380 nb: 1308 cost: 1.9274329 error rate: 0.789\n",
      "i: 8 j: 400 nb: 1308 cost: 1.9269518 error rate: 0.789\n",
      "i: 8 j: 420 nb: 1308 cost: 1.9260254 error rate: 0.789\n",
      "i: 8 j: 440 nb: 1308 cost: 1.9264946 error rate: 0.789\n",
      "i: 8 j: 460 nb: 1308 cost: 1.9256228 error rate: 0.789\n",
      "i: 8 j: 480 nb: 1308 cost: 1.9255605 error rate: 0.789\n",
      "i: 8 j: 500 nb: 1308 cost: 1.9255868 error rate: 0.789\n",
      "i: 8 j: 520 nb: 1308 cost: 1.925647 error rate: 0.789\n",
      "i: 8 j: 540 nb: 1308 cost: 1.9262055 error rate: 0.789\n",
      "i: 8 j: 560 nb: 1308 cost: 1.9268785 error rate: 0.789\n",
      "i: 8 j: 580 nb: 1308 cost: 1.9243112 error rate: 0.788\n",
      "i: 8 j: 600 nb: 1308 cost: 1.923573 error rate: 0.788\n",
      "i: 8 j: 620 nb: 1308 cost: 1.9240916 error rate: 0.788\n",
      "i: 8 j: 640 nb: 1308 cost: 1.9275495 error rate: 0.786\n",
      "i: 8 j: 660 nb: 1308 cost: 1.9276748 error rate: 0.789\n",
      "i: 8 j: 680 nb: 1308 cost: 1.927098 error rate: 0.789\n",
      "i: 8 j: 700 nb: 1308 cost: 1.9277457 error rate: 0.789\n",
      "i: 8 j: 720 nb: 1308 cost: 1.9268639 error rate: 0.789\n",
      "i: 8 j: 740 nb: 1308 cost: 1.9262494 error rate: 0.789\n",
      "i: 8 j: 760 nb: 1308 cost: 1.9264706 error rate: 0.789\n",
      "i: 8 j: 780 nb: 1308 cost: 1.924718 error rate: 0.789\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X, Y = getData()\n",
    "    model = ANN([2000, 1000])\n",
    "    model.fit(X, Y, show_fig=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
